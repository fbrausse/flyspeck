% Revisions to the Kepler Conjecture.
% Author: Hales, Harrison, McLaughlin, Nipkow, Obua, Zumkeller

% email: hales@pitt.edu
%
% latex format

% History:
% Branched from dcg_errata.tex on Oct 8, 2008


\documentclass[11pt]{amsart}
%\documentclass{llncs}
\usepackage{graphicx}
\usepackage{url}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{alltt}
%\usepackage{amsmath,amsthm}

% automatically generate revision number by
% svn propset svn:keywords "LastChangedRevision" revision.tex
\def\svninfo{{\tt
  filename: revision.tex\hfill\break
  LaTeX generation date: \today; \hfill\break
  Repository Root: https://flyspeck.googlecode.com/svn \hfill\break
  SVN $LastChangedRevision$
  }
  }
%-%


% Math notation.
\def\op#1{{\text{#1}}}
\newcommand{\mc}[1]{{\mathcal{#1}}}
\newcommand{\ring}[1]{\mathbb{#1}}
%\def\to{{\ensuremath{\quad\Longrightarrow\quad}}}
\def\lto{\ensuremath{\,\leadsto\,}}
\def\line{$\ell$}
\def\text{\hbox}

% Typesetting:
\def\coloneq{\mathrel{\mathop:}=}
\def\sfrac#1#2{{\textstyle \frac {#1} {#2}}}
\def\eqref#1{(\ref{#1})}

% Flyspeck:
\def\pt{\mathrm{pt}}
\def\doct{\delta_{\mathrm{oct}}}
\def\asolid{\mathrm{a}}
\def\sqroot{\mathrm{sqrt}}
\def\rcp{\mathrm{rcp}}
\def\bstein{\mathrm{B}}

% Harrison's macros:
\newcommand{\nat}{\mbox{$\protect\mathbb N$}}
\newcommand{\num}{\mbox{$\protect\mathbb Z$}}
\newcommand{\rat}{\mbox{$\protect\mathbb Q$}}
\newcommand{\real}{\mbox{$\protect\mathbb R$}}
\newcommand{\complex}{\mbox{$\protect\mathbb C$}}
\newcommand{\xxx}{\mbox{$\protect\mathbb X$}}
\newcommand{\bool}{\mbox{$\protect\mathbb B$}}

\newcommand{\lamb}[1]{\lambda #1.\:}
\newcommand{\eps}[1]{\varepsilon #1.\:}
\newcommand{\all}[1]{\forall #1.\:}
\newcommand{\ex}[1]{\exists #1.\:}
\newcommand{\exu}[1]{\exists! #1.\:}

\newcommand{\True}{\top}
\newcommand{\False}{\bot}
\newcommand{\Not}{\neg}
% \newcommand{\And}{\wedge}
\let\And=\wedge                    % Another definition in amsmath

\newcommand{\Or}{\vee}
\newcommand{\Imp}{\Rightarrow}
\newcommand{\Iff}{\Leftrightarrow}

\newcommand{\entails}{\vdash}
\newcommand{\sequent}{\to}
\newcommand{\proves}{\vdash}

\newcommand{\Ands}{\bigwedge}
\newcommand{\Ors}{\bigvee}

\newcommand{\BQ}{\mbox{$\ulcorner$}}
\newcommand{\BEQ}{\mbox{\raise4pt\hbox{$\ulcorner$}}}
\newcommand{\EQ}{\mbox{$\urcorner$}}
\newcommand{\EEQ}{\mbox{\raise4pt\hbox{$\urcorner$}}}

\let\psubset=\subset                    % Pure TeX: thanks to MAJ %
\let\subset=\subseteq
\let\psupset=\supset
\let\supset=\supseteq

\newcommand{\powerset}{\wp}             % This is pretty dire...  %

\newcommand{\Union}{\cup}
\newcommand{\Inter}{\cap}
\newcommand{\Unions}{\bigcup}
\newcommand{\Inters}{\bigcap}

\newcommand{\binom}[2]{{{#1} \choose {#2}}}

\newcommand{\proof}{{\bf \noindent Proof:\ }}
\newcommand{\qed}{Q.E.D.}

\newcommand{\Rule}{\infer}

\newcommand{\restrict}{\upharpoonright} % This is lousy and must be fixed! %

\newcommand{\bigsqcap}{\mbox{\Large{$\sqcap$}}}

\newcommand\leb{\lbrack\!\lbrack}
\newcommand\reb{\rbrack\!\rbrack}
\newcommand{\sem}[1]{\leb #1 \reb}

\newcommand{\BA}{\begin{array}[t]{l}}
\newcommand{\EA}{\end{array}}

\newcommand{\sqle}{\sqsubseteq}
\newcommand{\sqlt}{\sqsubset}

\newcommand{\from}{\leftarrow}
\newcommand{\too}{\twoheadrightarrow}

\newcommand{\Los}{{\L}o{\'s}}
\newcommand{\del}{\partial}

\newcommand{\QUOTE}[1]{\mbox{$\BQ #1 \EQ$}}
\newcommand{\floor}[1]{\mbox{$\left\lfloor #1 \right\rfloor$}}
\newcommand{\ceil}[1]{\mbox{$\left\lceil #1 \right\rceil$}}
\newcommand{\bracket}[1]{\mbox{$\left\langle #1 \right\rangle$}}

\newcommand{\until}{\;\mbox{\tt U}\;}

\newcommand{\tm}{$^{\mbox{{\tiny TM}}}$}
\newcommand{\registeredtrademark}{\Pisymbol{psy}{210}}

\newcommand{\itanium}{Intel{\registeredtrademark} Itanium{\registeredtrademark}}
\newcommand{\pentium}{Intel{\registeredtrademark} Pentium{\registeredtrademark}}

% This is an infix modulus operator

\newcommand{\mod}{\mbox{\tt{ mod }}}

% This (\imod) is for congruences mod.

\makeatletter
\def\imod#1{\allowbreak\mkern10mu({\operator@font mod}\,\,#1)}
\makeatother

% This is stuff for boxes, stolen from HOL documentation

\newcommand\HOLSpacing{12pt}

\newlength{\hsbw}
\setlength{\hsbw}{\textwidth}
\addtolength{\hsbw}{-\arrayrulewidth}
\addtolength{\hsbw}{-\tabcolsep}

\newenvironment{boxed}{\begin{flushleft}
 \begin{tabular}{@{}|c@{}|@{}}\hline
 \begin{minipage}[b]{\hsbw}
 \vspace*{.06in}
 \begingroup\small\baselineskip\HOLSpacing}{\endgroup\end{minipage}\\ \hline
 \end{tabular}
 \end{flushleft}}

% Some additional "log-like" functions

\newcommand{\asn}{\mathop{\mathrm{asn}}\nolimits}
\newcommand{\acs}{\mathop{\mathrm{acs}}\nolimits}
\newcommand{\atn}{\mathop{\mathrm{atn}}\nolimits}
\newcommand{\atan}{\mathop{\mathrm{atan}}\nolimits}
\newcommand{\asin}{\mathop{\mathrm{asin}}\nolimits}
\newcommand{\acos}{\mathop{\mathrm{acos}}\nolimits}

% Handy for multivariate stuff

\newcommand{\xs}{\overline{x}}

% Hyphenation stuff

\hyphenation{dijk-stra}

\newcommand{\citeN}{\cite}



\newtheorem{definition}[subsubsection]{Definition}
\newtheorem{thm}[subsubsection]{Theorem}
\newtheorem{lemma}[subsubsection]{Lemma}
\newtheorem{assumption}[subsubsection]{Assumption}
\newtheorem{corollary}[subsubsection]{Corollary}
\newtheorem{remark}[subsubsection]{Remark}


\parindent=0pt
\parskip=\baselineskip

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Revising the proof of the Kepler Conjecture}

\author[Hales]{Thomas C. Hales}
\address[T.~Hales]{Math Department, University of Pittsburgh}
\email{hales@pitt.edu}

\author[Harrison]{John Harrison}
\address[J.~Harrison]{Intel JF1-13, Hillsboro OR}
\email{johnh@ichips.intel.com}

\author[McLaughlin]{Sean McLaughlin}
\address[S.~McLaughlin]{CMU}

\author[Nipkow]{Tobias Nipkow}
\address[T.~Nipkow]{TUM}

\author[Obua]{Steven Obua}
\address[S.~Obua]{TUM}

\author[Zumkeller]{Roland Zumkeller}
\address[R.~Zumkeller]{Palaiseau}

\begin{abstract}
The Kepler conjecture asserts that no packing of congruent balls in three-dimensional Euclidean space can
have density greater than that of the face-centered cubic packing.  The original proof,
announced in 1998 and published in 2006, is long and complex. The process of  revision and review did not end with the publication of the proof.
This article summarizes the current status of several long-term iniatiatives to
reorganize the original proof into a more transparent form and to provide a greater
level of certification of the correctness of the computer code and other details of the proof.
\end{abstract}


\maketitle

\section{Introduction}

In 2006, {\it Discrete and Computational Geometry} devoted a special issue to the
proof of the Kepler Conjecture on sphere packings, which asserts that no
packing of congruent balls in three-dimensional Euclidean space can
have density greater than that of the face-centered cubic packing \cite{Hales:2006:DCG}.
This density is $\pi/\sqrt{18}$.

The proof is long and complex.  The editors' forward to that issue remarks
that ``the reviewing of these papers ws a particularly enormous and daunting task.''
``The main portion of the reviewing took place in a seminar run at E\"otvos University
over a 3 year period.  Some computer experiments were done in a detailed check.
The nature of this proof, consisting in part of a large number of inequalities having
little internal structure, and a complicated proof tree, makes it hard for humans
to check every step reliably.  Detailed checking of specific assertions found them to
be essentially correct in every case tested.  The reviewing process produced in the
reviewers a strong degree of conviction of the essential correctness of the proof
approach, and that the reduction method led to nonlinear programming problems of
tractable size.''

The process of review and revision did not end when the proof was published.
This article summarizes the current status of several long-term iniatiatives to
reorganize the original proof into a more transparent form and to provide a greater
level of certification of the correctness of the computer code and other details of the proof.

\section{The Flyspeck Project}

\subsection{Formal Proof}


\subsection{Formal Proof of the Kepler Conjecture}


\subsection{An outline of the initiatives}

This section gives a summary of several different initiatives related to the Kepler conjecture.

\subsubsection{Standard ML reimplementation of the code}

S. McLaughlin has reimplemented the computer code that is used in the proof
of the Kepler conjecture.  Section~\ref{sec:code} describes the difficulties
in verifying the computer code in its original form.  Section~\ref{sec:sean}
describes the reimplementation.

\subsubsection{Verifying nonlinear inequalities with Bernstein bases}

\subsubsection{Graph generation}

\subsubsection{Linear programming}

\subsubsection{Formal Blueprint}

\subsubsection{Text formalization}

\subsubsection{Automated Translation}

\subsubsection{Extensions to HOL Light}




\section{Formalizing the ordinary mathematics}

The computer code in Flyspeck has so far received the lion's share of the
formal effort. This is entirely reasonable since there are, or at least were,
real questions about the feasibility of reproducing these results in a formal
way. However, the Flyspeck proof includes a large amount of `ordinary'
mathematics, which also needs to be formalized. Here we are on fairly safe
ground in principle, because by now we understand the formalization of
mainstream mathematics quite well \cite{wiedijk-17}. It is safe to predict that
this formalization can be done, and we can even hope for a reasonably accurate
estimate of the effort involved. Nevertheless, the formalization is certainly
non-trivial and will require considerable work. Here we describe some of the
work on formalizing Euclidean space and measure theory, and the development of
further proof automation, which should be useful in this endeavor.

\subsection*{Formalizing Euclidean space}

Much of our work has been devoted to developing a solid general theory of
Euclidean space $\real^N$ \cite{harrison-euclidean}. For Flyspeck, we
invariably just need the special case $\real^3$. While some concepts, e.g.
vector cross products, are specific to $\real^3$, most of the theory has been
developed for general $\real^N$ so as to be more widely applicable. Our theorem
prover HOL Light \cite{harrison-demo} is based on a logic without dependent
types, but we can still encode the index $N$ as a type (roughly, an arbitrary
indexing type of size $N$). This means that theorems about specific sizes like
$3$ really are just type instantiations of theorems for general $N$ stated with
polymorphic type variables. The theory contains the following:

\begin{itemize}

\item Basic properties of vectors in $\real^N$, linear operators and matrices,
dimensions of vector subspaces and other bits of linear algebra. For example,
the following is a formal statement of the theorem that a square matrix $A'$ is
a left inverse to another one $A$ iff it is a right inverse. Note that the
double use of the same type variable $N$ constrains the theorem to square
matrices:

\begin{scriptsize}\begin{alltt}
|- \(\forall\)A:real^N^N A':real^N^N. (A ** A' = mat 1) \(\Iff\) (A' ** A = mat 1)
\end{alltt}\end{scriptsize}

\item Metric and topological notions like distances, open sets, closure,
compactness and paths. Some of these are very general, others are more specific
to Euclidean space. Some results include the Heine-Borel theorem, the Banach
fixed-point theorem and Brouwer's fixed-point theorem. The following is a
formal statement that continuous functions preserve connectedness.

\begin{scriptsize}\begin{alltt}
|-  \(\forall\)f:real^M->real^N s. f continuous_on s \(\And\) connected s \(\Imp\) connected(IMAGE f s)
\end{alltt}\end{scriptsize}

\item Properties of convex sets, convex hulls, cones etc. Results include
Helly's theorem, Carath\'eodory's theorem, and various classic results about
separating and supporting hyperplanes. The following states a simpler but not
entirely trivial result that convex hulls preserve compactness.

\begin{scriptsize}\begin{alltt}
|- \(\forall\)s:real^N->bool. compact s \(\Imp\) compact(convex hull s)
\end{alltt}\end{scriptsize}

\item Sequences and series of vectors and uniform convergence, Fr\'echet
derivatives and their properties, up to various forms of the inverse function
theorem, as well as specific 1-dimensional theorems like Rolle's theorem and
the Mean Value Theorem. Here is the formal statement of the chain rule for
Fr\'echet derivatives.

\begin{scriptsize}\begin{alltt}
|- \(\forall\)f:real^M->real^N g:real^N->real^P f' g'.
        (f has_derivative f') (at x) \(\And\)
        (g has_derivative g') (at (f x))
        \(\Imp\) ((g o f) has_derivative (g' o f')) (at x)
\end{alltt}\end{scriptsize}

\end{itemize}

\subsection*{Formalizing measure theory}

Although the basic Euclidean theory is an important foundation, and many of the
concepts like `convex hull' are used extensively in the Flyspeck mathematics,
perhaps the most important thing to formalize is the concept of volume.

We define integrals of general vector-valued functions over subsets of
$\real^N$, using the Kurzweil-Henstock gauge integral definition. We develop
all the usual properties such as additivity and the key monotone and dominated
convergence theorems. We also develop a theory of {\em absolutely} integrable
functions, where both $f$ and $|f|$ are gauge integrable; this is known to
coincide with the Lebesgue integral. Here is a formal statement of the simple
theorem that integration preserves linear scaling:

\begin{scriptsize}\begin{alltt}
|- \(\forall\)f:real^M->real^N y s h:real^N->real^P.
        (f has_integral y) s \(\And\) linear h \(\Imp\) ((h o f) has_integral h(y)) s
\end{alltt}\end{scriptsize}

Using this integral applied to characteristic functions, we develop a theory of
(Lebesgue) measure, which of course gives volume in the 3-dimensional case. The
specific notion `measure zero' is formalized as {\tt negligible}, and we also
have a general notion of a set having a finite measure, and a function {\tt
measure} to return that measure when it exists. For example, this is the basic
additivity theorem:

\begin{scriptsize}\begin{alltt}
|- \(\forall\)s t. measurable s \(\And\) measurable t \(\And\) DISJOINT s t
         \(\Imp\) measure(s UNION t) = measure s + measure t
\end{alltt}\end{scriptsize}

We have proved that various `well-behaved' sets such as bounded convex ones and
compact ones, or more generally those with negligible frontier (boundary) are
measurable, e.g.

\begin{scriptsize}\begin{alltt}
|- \(\forall\)s:real^N->bool. bounded s \(\And\) negligible(frontier s) \(\Imp\) measurable s
\end{alltt}\end{scriptsize}

The main lack at the moment is a set of results for actually computing the
measures of specific sets, as needed for Flyspeck. We can evaluate most basic
1-dimensional integrals by appealing to the Fundamental Theorem of Calculus,
but we need to enhance the theory of integration with stronger Fubini-type
results so that we can evaluate multiple integrals by iterated one-dimensional
integrals. This work is in progress at the time of writing.

\subsection*{Enhanced automation}

Using coordinates, many non-trivial geometric statements in $\real^3$, or other
Euclidean spaces of specific finite dimension, can be reduced purely to the
elementary theory of reals. This is known to be decidable using quantifier
elimination \cite{tarski-decision,collins,hormander-pdo2}. However, in practice
this is often problematic because quantifier elimination for nonlinear formulas
is inefficient. The problem is particularly severe if we want to have any kind
of {\em formal} proof, as we do in Flyspeck, since producing such a proof
induces further slowdowns \cite{mahboubi-hormander,mclaughlin-harrison}. With
this in mind, we have explored a different approach to the case of purely
universally quantified formulas \cite{harrison-sos}, based on ideas of Parrilo
\cite{parrilo-semidefinite}. This involves reducing the initial problem to
semidefinite programming, solving the SDP problem using an external tool and
reconstructing a `sum-of-squares' (SOS) certificate that can easily be formally
checked.

For example, suppose we wish to verify that if a quadratic equation $a x^2 + b
x + c = 0$ has a real root, then $b^2 \geq 4 a c$. Using the SDP solver we find
an algebraic certificate $b^2 - 4 a c = (2 a x + b)^2 - 4 a (a x^2 + b x + c)$,
from which the required fact follows easily: $(2 a x + b)^2 \geq 0$ because it
is a square, and $4 a (a x^2 + b x + c) = 0$ because $x$ is a root, and so we
deduce $b^2 - 4 a c \geq 0$. This method seems very useful for automating
routine nonlinear reasoning in a way that is easy and quick to formally verify,
so that we don't have to rely on the correctness of a complicated program. It
is even capable of solving the coordinate forms of some of the simpler Flyspeck
inequalities directly, though it seems unlikely to be competitive with
customized nonlinear optimization methods as implemented by Zumkeller. For
example, one simple Flyspeck inequality is the following, which after being
reduced to a real problem with 9 variables (three coordinates for each point)
is solved by SOS in a second:
$$ \|u - v\| \geq 2 \And \|u - w\| \geq 2 \And \|v - w\| \geq 2 \And
   \|u - v\| < \sqrt{8}
   \Imp \|w - (u + v)/2\| > \|u - v\|/2
$$

A quite different approach to geometric theorem proving is to work in the
setting of a general real vector space or normed real vector space. In this
case, other decision methods are available \cite{solovay-jointpaper}. In
particular, one of these decision procedures that we have implemented in HOL
Light can sometimes handle simple forms of spatial reasoning in a purely
`linear' way, and so be much more efficient than the direct reduction to
coordinates, even if we do in fact have a specific dimension in mind. One
real example from formalizing complex analysis is the following in $\real^2$:
$$ |\|w - z\| - r| = d \And \|u - w\| < d/2 \And \|x - z\| = r
   \Imp d/2 \leq \|x - u\|
$$




\section{Standard ML Reimplementation of Code}

\subsection{Code}
\label{sec:code}



The Hales and Ferguson proof of the Kepler
Conjecture~\cite{Hales:2005:Annals} relies significantly
on computation. Computer code is used extensively, and is
central both to the correctness of the result and to a thorough
understanding of the proof. 

  There are 4 major difficulties with understanding and verifying
this particular code base. The first and most glaring difficulty is
simply the amount of code. At Hales's website~\cite{website:Hales:1998:Code}
there are well over 50,000 lines of programs in Java, C++, and
Mathematica. This represents only the calculations Hales did himself.
Sam Ferguson also completed many of the calculations with a completely
different code base of 137,000\footnote{There is a large amount of
code copying in Ferguson's code, resulting in a much larger code base.
The number of distinct lines is difficult to measure} lines of C. By
contrast, the proof of the Four Color Theorem by Robinson \textit{et.
al.}~\cite{Robertson:1997:JCTB} is less than 3,000 lines of C.

Another difficulty is in the organization of the code. The
calculations were done over the space of four years and involved % 10->4, tch, Oct 11, 2008
thousands of executions of the various programs. The variety of
computational tasks include, among others, estimating global bounds on
inequalities using nonlinear methods, proving inequalities rigorously
using interval arithmetic, generating graphs satisfying certain
properties, generating linear programs from graphs, and bounding
linear programs. Despite a labeling scheme used by Hales to uniquely
identify his calculations, many computations relied upon by the proof
are difficult to find in the original source code. To locate, for
instance, computation ``CALC-821707685'' from Hales's paper in
\emph{Discrete and Computational Geometry}~\cite{Hales:2006:DCG},
p.159, you can search on the website~\cite{website:Hales:1998:Code} in
vain. Ferguson has no such naming convention. While we believe records
of the computations do exist, it is not always an easy matter
to find them without the authors' guidance.

The third difficulty lies in the complexity of the implementation. For instance,
the software Hales developed to prove the inequalities upon which his
paper rests is relatively complicated. Processing power at the time
(1985-1995) was just barely capable of completing the computations he
requested. To keep the length of execution to days or weeks instead of
months or years, Hales extensively optimized his code. The
optimizations were often implemented without comment in the source,
and in some cases were difficult for us to understand.  

Finally, Hales uses C++ and Ferguson uses C to carry out
\emph{interval arithmetic} calculations. In the process, they
explicitly set the IEEE 754~\cite{IEEE:1985:IEE754} rounding modes on
the floating point unit. There are a number of difficulties with using
floating point numbers and rounding modes directly. The first is that
reasoning about floating point instructions requires a relatively deep
understanding of the machine architecture~\cite{Monniaux:2008:TOPLAS}.
For instance, setting the rounding mode\footnote{In Hales C++ code
base the rounding modes are explicitly changed at least 400 times.}
changes the state of the processor itself. Such an instruction has a
global effect on all subsequent floating point computations. Moreover,
compilers, libraries, and even processors are notorious for unsound
implementations of the 754 standard. A GCC mailing list entry from
1998\footnote{\url{http://gcc.gnu.org/ml/gcc/1998-02/msg00998.html}},
3 years after the Kepler project was completed, contains the following
comment (after describing fixes for a number of other 754-related
bugs): ``When you use fpu/cpu to do rounding, you have to mark the
variable volatile. Otherwise, the compiler may do some thing [sic] you don't
want.'' On the other hand, the volatile keyword is not well defined by
the C language definition~\cite{Kernighan:1988:C}, meaning that a C
program using the volatile keyword is undefined by the
language definition. This is an unsatisfactory state of affairs.

% ------------------------------------------------------------------------------
%  Motivation                                                                   
% ------------------------------------------------------------------------------



\subsection{Motivation and Current Status}
\label{sec:sean}

Our motivation in this work is to bring the computational aspects of
the Kepler conjecture closer to the level of simplicity and clarity
necessary for formalization by a proof assistant. Our goals are:

\begin{enumerate}
\item The code should be rewritten in a functional language with a
  rigorous definition whose semantics resembles those supported by
  proof assistants such as HOL Light and Coq.
\item The implementation should be written in a subset of the language
  that is easily translatable to the object languages of Coq and HOL
  Light. This means it should not, for instance, use features such as state
  (references) that do not have obvious analogues in proof assistants.
\item The implementation should be able to avoid the use of floating
  point numbers and rounding modes.
\item The code should be organized in such a way that any
  computation upon which the proof relies can be evaluated from a single
  interface.  This will allow Flyspeck developers to easily find and
  check the paper results during the formalization process.
\end{enumerate} 

\noindent This
work is in the spirit of Robinson, \textit{et.
al.}~\cite{Robertson:1997:JCTB}, which simplified the original code of
Appel and Haken~\cite{Appel:1986:FourColor}, and was used by Gonthier to
construct the fully formal proof~\cite{Gonthier:2005:FourColor} in the
Coq proof assistant. 

We chose Standard ML for the reimplementation for a number of reasons.
It has a formal definition~\cite{Milner:1990:SML}, and thus programs
have a meaning apart from the particular compiler used. It has a
foreign function interface that allows us to use external libraries
with relative ease. It has an expressive module system, and thus it is
possible to write the code with respect to an abstract libraries and
arithmetics. This allows us, for instance, to use multiple independent
implementations of interval arithmetic such as
MPFI~\cite{Revol:2005:MPFI}, as well as our own implementation using
floating point numbers. SML also has a very efficient
compiler~\cite{website:MLton}\footnote{Our SML implementation runs at
worst twice as slow as Hales C++ implementation compiled with GCC.}.

We have made significant progress toward our goals. Almost all the
code has been completely rewritten~\cite{McLaughlin:2008:KeplerCode} in
SML. The graph generator
from~\cite{Nipkow:2005:Tame} has been incorporated into the user
interface. Using the module system, we have completed a generic linear
programming interface to check the linear programs. We have
incorporated the GLPK and CPLEX libraries for solving linear programs
via the foreign function interface. The dual variables of the linear
programming solutions are rigorously checked in SML using interval
arithmetic, similar to Obua's work in~\cite{Obua:2005:Thesis}. We have
also incorporated the nonlinear optimization packages CFSQP and Knitro
into our interface to allow (non-rigorous) testing of inequalities.
This has allowed us to catch a number of transcription errors in
revisions of the Kepler proof. Thus, we have a fairly complete suite
of software support for the Flyspeck project. We are now in the
process of organizing and reevaluating the thousands of computations
found in the proof.



\section{Tame Graph Enumeration}

\label{sec:graph}

This section sketches the machine-checked proof of Hales's Claim~3.13 and
Theorem 19.1~\cite{Hales:2006:DCG}:
\begin{thm}\label{Archive:complete}
Any tame plane graph is isomorphic to a graph in the Archive.
\end{thm}
Tame graphs are particular plane graphs that represent potential
counterexamples. The \emph{Archive} is a list of over 5000 plane
graphs. Hales generated the Archive with the help of a Java program that
enumerates all plane graphs. Tameness is defined
in Section~18 and the enumeration is sketched in Section~19 of
\cite{Hales:2006:DCG}. There are two potential sources of errors:
Hales's publications only sketch the details of the enumeration, and the
referees only had a ``diagonal look'' at the implementation, more than 2000
lines of Java.

We recast Hales's Java program for the enumeration of all tame graphs in
logic, proved its completeness with the help of an interactive theorem prover,
ran it, and compared the output to Hales's
Archive.  It turns out that Hales was right, the Archive is complete,
although redundant (there are at most 2771 tame graphs).
Doing all this inside a logic and a theorem prover requires two things:
\begin{itemize}
\item The logic must contain a programming language.
We used Church's \emph{higher-order logic} (HOL) based on $\lambda$-calculus,
the foundation of functional programming. Programs in HOL are simply
sets of recursion equations, i.e.\ pure logic.
\item The programming language contained in the logic must be efficiently
executable and such executions must count as proofs. The theorem prover that we used, Isabelle/HOL~\cite{LNCS2283} fulfills this criterion. If all functions
that appear in a term are either data, e.g.\ numbers, or functions defined
by recursion equations, Isabelle/HOL offers the possibility to evaluate this term $t$ in one big and relatively efficient step to a value $v$, giving rise to the theorem $t = v$.
\end{itemize}
The enumeration of all tame graphs generates 23 million plane graphs ---
hence the need to perform massive computations in reasonable time.

Now we give a top-level overview of the formalization and proof of
completeness of the enumeration of tame graphs in HOL. For details
see~\cite{NipkowBS-IJCAR06}. The the complete machine-checked proof, over
17000 lines, is available online in the Archive of Formal Proofs at
\url{afp.sf.net}~\cite{BauerN-AFP06}.

\subsection{Plane Graphs}

Following Hales we represent finite, undirected, plane graphs as lists
(= finite sets) of faces and faces as lists of vertices. Note that by
representing faces as lists they have an orientation. The enumeration of
plane graphs requires an additional distinction between \emph{final}
and \emph{non-final} faces. Hence a face is really a pair of a list
of vertices and a Boolean.
A plane graph is \emph{final} iff each of its faces is.
In final graphs we can ignore the Boolean component of the faces.

\subsection{Enumeration of Plane Graphs}

Hales characterises plane graphs by an executable enumeration and sketches a
proof of completeness of this enumeration. We have followed Hales and taken
this enumeration as the definition of planarity.  Hales's enumeration of plane
graphs proceeds inductively: you start with a seed graph with two faces, the
final outer one and the (reverse) non-final inner one. If a graph contains a
non-final face, it can be subdivided into a final face and any number of
non-final ones.  Because a face can be subdivided in many ways, this process
defines a tree of graphs. By construction the leaves must be final graphs,
and they are the plane graphs we are interested in: any plane graph of $n$
faces can be generated in $n-1$ steps by this process, adding one (final)
face at a time. For details see~\cite{Hales:2006:DCG} or
\cite{NipkowBS-IJCAR06}.

The enumeration is parameterized by a natural number $p$ which controls the
maximal size of final faces in the generated graphs. The seed graph
\textit{Seed$_p$} contains two $(p+3)$-gons and the final
face created in each step may at most be a $(p+3)$-gon. As a result,
different parameters lead to disjoint sets of graphs.

The HOL formalization defines an executable function
\textit{next-plane$_p$} that maps a
graph to a list of graphs, the successor graphs reachable by subdividing one
non-final face.  The plane graphs are the final graphs reachable from
\textit{Seed$_p$} via \textit{next-plane$_p$} for some $p$.

\subsection{Enumeration of Tame Graphs}

Hales's definition of tameness is already quite close to a direct logical
formulation. Hence the HOL formalization is very close to his. Of course
pictures of graphs had to be translated into formulae, taking implicit
symmetries in pictures into account. We found one simplification: in the
definition of an admissible weight assignment one can drop condition 3
without changing the set of tame graphs. What facilitated our work
considerably was that a number of Hales's 8 tameness conditions are directly
executable. The details are described elsewhere~\cite{NipkowBS-IJCAR06}.

The enumeration of tame graphs is a modified enumeration of plane graphs
where we remove final graphs that are definitely not tame, and prune the
search tree at non-final graphs that cannot lead to tame graphs anymore.  It
is not obvious from the definition that there are only finitely many tame
graphs; this follows because the enumeration terminates. Hales
description~\cite{Hales:2006:DCG} is deliberately sketchy and we had to study
his Java programs for the precise formulation of the pruning criteria.  This
is the most delicate part of the proof because we need to balance
effectiveness of pruning with simplicity of the completeness proof: weak
pruning criteria are easy to justify but lead to unacceptable run times of
the enumeration, sophisticated pruning techniques are difficult to justify
formally. Since computer-assisted proofs are still very laborious,
simplifying those proofs was of prime importance. In the end, the HOL
formalization defines a function \textit{next-tame$_p$}
from a graph to a graph list. It computes the list of plane successor graphs
\textit{next-plane$_p$ g} and post-processes it as follows:
\begin{enumerate}
\item Remove all graphs from the list that cannot lead to tame graphs
because of lower bound estimates for the total admissible weight of the final
graph.
\item Finalize all triangles in all of the graphs in the list
(because every 3-cycle in a tame graph must be a face).
\item Remove final graphs that are not tame from the list.
\end{enumerate}
A necessary but possibly not sufficient check for tameness is used in the last
step. Hence the enumeration may actually produce non-tame graphs. This is
unproblematic: in the worst case a fake counterexample to the Kepler
conjecture is produced, but we do not miss any real ones.

Although we have roughly followed Hales procedure, we have simplified it in
many places. In particular we removed his special treatment of
\textit{Seed$_0$} and \textit{Seed$_1$}, which is a fairly intricate
optimization that turned out to be unnecessary.

The following completeness theorem is the key result:
\begin{thm}\label{thm:TameEnum_comp}
If a tame and final graph $g$ is reachable from
\textit{Seed$_p$} via \textit{next-plane$_p$}
then $g$ is also reachable from \textit{Seed$_p$} via \textit{next-tame$_p$}.
\end{thm}

Each step \textit{next-tame$_p$} is executable and an exhaustive enumeration
of all graphs reachable from a seed graph is easily defined on top of it. We
call this function \textit{tameEnum$_p$}. By definition, tame graphs may
contain only triangles up to octagons, which corresponds to the parameters $p
= 0,\dots,5$.

\subsection{The Archive}

In order to build on the above enumeration of all tame graphs without having
to rerun the enumeration, the results of running \textit{tameEnum$_p$} with
$p = 0,\dots,5$ are put into an Archive and isomorphic graphs are
eliminated. The result are 2771 graphs, as opposed to Hales's 5128. The
reasons are twofold: there are many isomorphic copies of graphs in Hales's
Archive and it contains a number of non-tame graphs, partly because, for
efficiency reasons, he did not enforce all tameness conditions in his Java
program. The new reduced Archive is also available
online~\cite{BauerN-AFP06}.

Finally we can prove Theorem~\ref{Archive:complete}: if $g$ is tame plane
graph, Theorem~\ref{thm:TameEnum_comp} and the definition of
\textit{tameEnum} tell us that $g$ must be contained in \textit{tameEnum$_p$}
for some $p=0,\dots,5$. Hence it suffices to enumerate \textit{tameEnum$_p$},
$p=0,\dots,5$, and check that, modulo graph isomorphism, the result is the
same as the Archive. This is a proposition that can be proved by executing it
(because the HOL formalization also includes a verified executable test for
graph isomorphism which we do not discuss).




\section{Verifying Linear Programming}
\label{sec:lp}

(to be supplied by Obua)

\section{Proving Nonlinear Inequalities with Bernstein bases}
\label{sec:zumkeller}

The list of about thousand non-linear inequalities forms the computationally
hardest part of the proof. This section presents a technique aimed at proving
these, based on polynomial approximation and the Bernstein bases. We feel that
our approach better fits the requirements of formal proof, as outlined in
section \ref{sean} and hope to refine it to eventually cover all Flyspeck
inequalities.

The proof contains the following definitions \cite{sp1}:
\begin{align*}
  \pt &\coloneq - \frac \pi 3 + 4 \arctan \frac{\sqrt 2}5 \\
\doct &\coloneq \frac {\pi - 4 \arctan \frac{\sqrt 2}5}{2 \sqrt 2}\\
\Delta(y) &\coloneq \frac 1 2
  \left|
  \begin{array}{ccccc}
0 & 1 & 1 & 1 & 1 \\
   1 & 0 & y_3^2 & y_2^2 & y_1^2 \\
   1 & y_3^2 & 0 & y_4^2 & y_5^2 \\
   1 & y_2^2 & y_4^2 & 0 & y_6^2 \\
   1 & y_1^2 & y_5^2 & y_6^2 & 0
  \end{array}
  \right|\\
\asolid_0(y) &\coloneq y_1 y_2 y_3 + \sfrac 1 2 (
y_1^2 y_2 + y_1 y_2^2 + y_1^2 y_3 + y_2^2 y_3 + y_1 y_3^2 \\
&\qquad\qquad\qquad {} + y_2 y_3^2 - y_1 y_4^2 - y_2 y_5^2 - y_3 y_6^2)\\
\asolid_1(y) &\coloneq \asolid_0 (y_1, y_5, y_6, y_4, y_2, y_3)\\
\asolid_2(y) &\coloneq \asolid_0 (y_2, y_4, y_6, y_5, y_1, y_3)\\
\asolid_3(y) &\coloneq \asolid_0 (y_4, y_5, y_3, y_1, y_2, y_6)\\
\gamma(y) &\coloneq 
- \frac \doct 6 \sqrt {\Delta(y)} + \frac 2 3 \sum_{i=0}^3 \arctan \frac
{\sqrt{\Delta(y)}} {\asolid_i(y)}
\end{align*}

Our guiding example will be inequality 586468779. Its statement is:
\begin{equation}
\forall y \in [2,2.51]^6.\; \gamma(y) \le \pt \label{gamma-pt}
\end{equation}
Interval arithmetic, used to prove the inequalities in the original proof,
suffers from the \emph{dependency problem}: the minimum and maximum of the
formula $x-x$ are overestimated because $[a,b] - [a,b] = [a-b,b-a]$, although
$x-x$ is clearly $0$. Subdividing $[a,b]$ into $[a,\sfrac{a+b}2]$ and
$[\sfrac{a+b}2,b]$, and then re-evaluating the formula yields an improved
result. However, depending on the problem, the number of required subdivisions
can be very large. This is why checking some inequalities takes a very long
time.

\subsection{From Geometrical Functions to Polynomials}
Fortunately, better methods than interval arithmetic are available, if the
function under consideration is polynomial. A quick look at $\gamma$ tells us
that \eqref{gamma-pt} is not polynomial, since it has occurrences of
$\sqrt\cdot$, $1/\cdot$, and $\arctan$. Can it nevertheless be reduced
to a polynomial problem? Two strategies come to mind:

First, algebraic laws such as $\sqrt a \le b \Leftrightarrow a \le b^2$ (if $b
\ge 0$) and $\frac a b \le c \Leftrightarrow a \le bc$ (if $b>0$) can often be
used to eliminate occurrences of $\sqrt\cdot$ and $1/\cdot$. The list of
trigonometric identities is endless. For our example, Vega's rule $\arctan a +
\arctan b = \arctan \frac {a + b} {1 - ab}$ seems useful. Unfortunately, this
technique quite often yields huge expressions that are difficult to deal with by
there sheer size. Also, an algebraic transformation to a polynomial problem may
simply be impossible (we suspect that this is the case for \eqref{gamma-pt}).

A second technique is based on replacing $\gamma$ with a polynomial $g$ that
fits above it, but is sill smaller than $\pt$. Clearly, if there exists a $g$
such that
\begin{equation}
\forall y \in [2,2.51]^6.\; \gamma(y) \le g(y) \label{gamma-g}
\end{equation}
and
\begin{equation}
\forall y \in [2,2.51]^6.\; g(y) \le \pt, \label{g-pt}
\end{equation}
then by transitivity \eqref{gamma-pt} holds.

Such a polynomial $g$ can be obtained by replacing $\sqrt{\cdot}$, $1/\cdot$ and
$\arctan$ with polynomial approximations. We only need to ensure that we use
upper approximations for positive occurrences and lower approximations for
negative ones. Only occurrences whose arguments contain variables need to be
replaced, since e.g. $\sqrt 2$ is a (constant) polynomial itself.

In the definition of $\gamma$ the function $\arctan{}$ occurs positively, so it
is replaced by an upper approximation $\overline{\arctan}$. The term
$\frac{\sqrt{\Delta(y)}} {\asolid_i(y)}$ is first unfolded to $
{\sqrt{\Delta(y)}} \cdot \frac 1 {\asolid_i(y)}$. Both the square root and
reciprocal occur positively again here, so they can be replaced by upper
approximations $\overline\sqroot$ and $\overline\rcp$, respectively. This yields
$\overline{\arctan} (\overline\sqroot(\Delta(y)) \cdot
\overline\rcp(\asolid_i(y)))$ in all four summands. It remains only the
$\sqrt\cdot$ occurring negatively after $-\frac \doct 6$, which is to be
replaced by a lower approximation $\underline\sqroot$.

We choose the following approximations:
\begin{align*}
\overline{\arctan}(t) &\coloneq \arctan \frac{\sqrt 2}5 + \frac {25} {27}\left(t - \frac{\sqrt 2}5\right)\\
\overline\rcp(t) &\coloneq \frac 1 4 - \frac {37 t} {1600} + \frac {t^2} {1000}- \frac{13 t^3} {640000} + \frac {t^4} {6400000}\\
\underline\sqroot(t) &\coloneq 8 \sqrt 2 + \frac 3 {64 (\pi - 4 \arctan \frac{\sqrt 2}5)} (t - 128)\\
\overline\sqroot(t) &\coloneq 8 \sqrt 2 + \frac 1 {16 \sqrt 2}(t -128)
\end{align*}

These approximations are valid w.r.t. \eqref{gamma-pt}. For example,
$$\forall t \in\Delta ([2,2.51]^6).\; \sqrt{t} \le \overline\sqroot(t).$$ This
can be established by elementary means, knowing that $\Delta ([2,2.51]^6)
\subseteq [128;501]$. The latter can be shown automatically by the method
outlined in subsection \ref{bernstein}.

Our construction of $g$ therefore ensures \eqref{gamma-g}. Moreover, the
approximations were chosen (using polynomial interpolation) in a way such that
$$\gamma (2,2,2,2,2,2) = g (2,2,2,2,2,2) = \pt. \label{eq-gamma-g-pt}$$
This is important, because otherwise \eqref{g-pt} cannot hold.

% TODO note on potential pitfall when imbricating approximations


\subsection{Bounding Polynomials}
\label{bernstein}
In order to prove \eqref{gamma-pt}, it remains to be shown that $g(y) \le \pt$.
This can be done with the help of Bernstein polynomials. We briefly outline the
case of a single variable $x$ here.

The $i$th Bernstein basis polynomial of order $k$ is defined as
$$\bstein^k_i(x) \coloneq {k \choose i}x^i(1-x)^{k-i}.$$
For a polynomial $p$ and a vector $b \in \mathbb R^k$, if
$$p(x) = \sum_{i=0}^k b_i \cdot \bstein^k_i(x)$$
then $b$ is called the \emph{Bernstein representation} of $p$. In this case
$$\forall x \in [0;1].\; p(x) \le \max_i b_i.$$

This property is tremendously useful: it gives us an upper bound on $p$, namely
the largest coefficient of $p$'s Bernstein representation. By a change of
variable we can reduce any domain to $[0,1]$. The generalization to the
multivariate case is straightforward \cite{garloff, roland-thesis}.

In order to bound a polynomial it thus suffices to convert it into Bernstein
representation. This can be done by a matrix multiplication (the Bernstein basis
of order $k$ forms a basis of the vector space of all polynomials of degree up
to $k$). For practical purposes it is however crucial to use a more efficient
algorithm (cf. \cite{garloff, roland-thesis}).

Note that $g$ contains irrational coefficients. This is a consequence of
requirement \eqref{eq-gamma-g-pt} and cannot be avoided. However, we were able
to choose the approximation polynomials in a way such that the transcendental
parts can be factored out (hence the occurrence of $\pi - 4 \arctan \frac{\sqrt
  2}5$ in the definition of $\underline\sqroot$). As can be easily checked with
symbolic algebra software, the polynomial $p(y) \coloneq \sqrt 2 (g(y) - \pt)$
has rational coefficients! It can thus be converted to a Bernstein
representation without rounding, using the algorithm presented in
\cite{roland-thesis}. With this method the only divisions are by powers of 2,
which can be efficiently represented using dyadic numbers.

The polynomial $p$ consists of 12945 monomials and has total degree 18. A
prototype implementation in Haskell returns $0$ as the maximum for $p$ in about
ten minutes. Thus $\sqrt 2 (g(y) - \pt) \le 0$ and $g(y) \le \pt$.





\section{Deformation Argument, page 131}

[p.131]
In Section~12.7, the argument in the first two
paragraphs about reducing to a polygon is incomplete, because it doesn't show that the deformation can be done in such a way that the distances remain at least $2$.  The Remark~12.7 about this issue only applies to the subsequent 
deformations.  This is a serious issue that requires an extended explanation.



%\twocolumn
\parskip=0.2\baselineskip
\section{Errata}


\subsection{Relation between the Abridged and Unabridged Versions}

The abridged version of the Kepler conjecture
in the Annals \cite{Hales:2005:Annals}
was generated by the same tex
files as the unabridged version in \cite{Hales:2006:DCG}.

Because of the way these documents were produced
from the same tex files,
it seems that nearly every correction to
the abridged version will also be a correction to the unabridged version.
We list the errata for the
unabridged version. The same list applies to corresponding 
passages in the abridged version.  


\subsection{Format}

Each correction gives its location in \cite{Hales:2006:DCG}.
The location
\line+n counts down from the top of the page, or
if a section or lemma number is provided, it
counts from the top of that organizational unit.
The location \line-n counts up from the bottom
of the page. Footnotes are not included in the
count from the bottom.  Every line containing
text of any sort is included in the count,
including displayed equations, section headings,
and so forth.  The material to the left of $\lto$ 
indicates original text, and material to the right of the
arrow gives replacement text.  Text in italic provides
comments about the corrections.


\subsection{Code}

In addition to the corrections to the text mentioned below, 
there have been some corrections to the computer code.
This includes code in Java, Mathematica, Objective CAML, and C++.
They are described in detail in~\cite{Hales:2008:Errata}.


\subsection{Listing}

[p.47,Lemma~5.16] $Q\lto F$

[p.49,\line+2] supposed \lto suppose
	
[p.63,Lemma~7.10]
	${\mathcal S}$-system \lto $Q$-system
	
[p.73][p.124] {\it Some applications of Theorem~8.4 rely on
the proof of the theorem, which is more general than
the statement of the theorem.}

[p.75,Remark~8.11]
	show\lto shows

[p.78,\line-7] constraints \lto constraint

[p.86,\line+14] Let $\{0,v\}$ be 
          the diagonal of an upright quarter in the $Q$-system
        \lto
       Let $v$ be a vertex with $2t_0<|v|<\sqrt8$.
	
          Remark: Section~9 assumes that the diagonal belongs to
          a quarter in the $Q$-system, but Lemma~10.14 uses these
          results when $\{0,v\}$ has $0$ or $1$ anchors.  To make
          this coherent, we should assume throughout Section~9 that
          we have the weaker condition that whenever $\{0,v\}$ has
          two or more anchors it belongs to a quarter in the $Q$-system.
          The proofs of Section~9 all go through in this context.
          (Lemma~9.7 is all that is relevant here.)

[p.87,Definition~9.3]
	In definition of $\Delta(v,W^e)$, we
	can have some $Q$ (as in Fig~9.1)
	with negative orientation.
	In this case, $E_v\cap E_i$ can clip
	the other side.  We want the object
	without clipping, so the definition must
	be modified slightly to reflect this.
	
[p.88,Definition~9.6]
	The definition is poorly worded.  First of
	all, it requires that the subscript to
	$\epsilon$ should be a vertex, but then in
	the displayed equation, it makes $w/2$ the
	subscript, which needn't be a vertex.  To
	define $\epsilon'$, move from $w/2$ along
	the ray through $x'$ until an edge of the
	Voronoi cell is encountered.  If $v,w,u$
	are the three vertices defining that edge,
	then set $\epsilon'_v(\Lambda,x)=u$.
	Degenerate cases, such as when two different
	edges are encountered at the same time,
	can be resolved in any reasonable fashion.
	
[p.88,Lemma~9.7,\line+2] 
	$w$\text{ and } $v$\lto $w$ \text{ and } $u$

	
[p.88,L.~9.7,Claim~1]
	\text{ with } $|w - w'|\le 2t_0$, \text{ and }
	\lto \text{ with }

	
[p.88,L.~9.7,\line+5]
         Then: $\lto$ Let
          $
          R'_w = \{x\in R_w \cap(0,\{u,w\})\mid 
          \epsilon_0(x,\{u,w\}) = u.
          $
          Assume that $R'_w$ is not empty. Then:
         %{\it (This hypothesis is satisfied
        %in every application of Lemma~9.7.
        %We note that this forces the orientation of $\{0,v,w'\}$ to
        %be negative in $Q=\{0,v,w',u\}$, which in turn forces $Q$
        %to be a quarter.)}

[p88,L.~9.7,Claim~3]
        $R_w \lto R'_w$

[p.89,\line+2]
	$
	\{w,v\}\lto\{w,u\}
	$

[p.92,\line+16,\line+21]
   $     \max_j u_j \lto \max_j |u_j|$
	
[p.93,\line-4]
	$
	\text{obstructed from }w \lto
	\text{obstructed from }w'
	$
		
[p.93,\line-2]
	$
	\text{from some} \lto \text{for some}
	$

[p.99,\line+1]
        $
        \text{start} \lto \text{star}
        $

[p.105,Lemma~10.14]  In the proof of the cases involving
   $0$ or $1$ anchor, a combination of the decompositions from
   Section~8.4 and Section~9 are used.  These decompositions haven't
   been shown to be compatible.  
   FIX: It is better to combine
   $\Delta(v,W)$ with $t_0$-truncation on the rest of the quad-cluster.
   With a $t_0$ truncation, we no longer have the non-positivity results
   from Section~8.  (The quoins give a positive contribution.) However,
   I have checked that
   the estimate on $\Delta(v,W)$ is sufficiently small that we still
   obtain a constant less than $-1.04\,\op{pt}$.
   

[p.116][p.121] Definition~11.7 allows masked
flat in definition of $3$-unconfined.
Definition~11.24 requires no masked flats
in the same definition.  FIX: Use Definition~11.24 (no masked flats).  Where masked flats occur,
treat them with Lemma~11.23, parts (1) and (2).

[p.116,\line+1] 
	$
	\text{Lemma}~4.16 \lto \text{Lemma}~4.17
	$

[p.117,before Lemma~11.9]
	$
	\text{two others} \lto \text{three others}
	$
	
[p.117,Def~11.8]
    $
    y1 \lto y_1
    $
    
 
[p.119,Definition~11.5]  By definition, we require a masked flat quarter to
be a strict quarter. 
	
[p.121] See p.116.

[p.121,\line-5]
	$
	0.2274 \lto 0.02274
	$
	
[p.123. flat case (2)]  It is missing
isolated quarters cut from the side.
To fix this, in condition 2(f), 
	$
	\eta_{456}\ge\sqrt2 \lto
	\eta_{456}\ge\sqrt2 \text{ or } \eta_{234}\ge\sqrt2.
	$
	
[p.124] See p.73.
	
[p.126]  Theorem~12.1 needs to be stated in
a form that allows the application in pp.251-252
and Lemma~13.5.  In these applications, the
regions are smaller than standard regions.
Yet in the statement of the theorem, the regions
are standard regions.  This is not a problem
in practice, because the proof is at a much
finer level of decomposition than standard regions.
However, the wording needs to be changed so
that the theorem applies precisely.

[p.126] 
Theorem~12.1 should include $\sigma_R(D)\le s_n$
with $s_3 = 1\,\op{pt}$ and $s_4=0$, and
$\tau_R(D) \ge t_3 = 0$.

[p.131] There is a long note in a separate section below about
the deformation arguments on this page.


[p.139,Lemma~12.18,proof,\line+3] 
	$C_0(|v|,\pi) \lto
	C_0^u(|v|,\pi)
	$
	
[p.139,Lemma~12.18] 
	$
	\tau_0(C_0^u(2t_0,\pi))-\pi_{\text{max}}\lto
	\tau_0(C_0^u(2.2,\pi))-\pi_{\text{max}}
	$

[p.144,\line+11,\line+17]
	$2t_0^2 \lto (2t_0)^2
	$

%c
[p.146]
		$S_n^\pm$ \lto
	of 3-crowded, 3-undefined, and
	4-crowded combinations

%c	
[p.148,Sect. 13.6]  This entire
section is misplaced.  It belongs with
Sections 25.5 and 25.6.

%c
[p.149,before 13.7]
the diagrams\lto
	Figs~25.1--25.4

%c	
[p.149,p.156] $\delta_{loop}$ is not defined.

[p.156,Lemma~13.5,\line+4]
	$$
	\begin{array}{lll}
	\text{respectively for }\tau_R(D)\lto\\
	\text{respectively, for }\sigma_R(D) \text{ and }
	\tau_R(D),  
	\end{array}
	$$

%c
[p.164,\line-1] 
	This shows$\ldots$ occur.
	\lto This completes the proof.


%c
[p.173,\line+4] {\it Insert the subscript on $b$,
as in Proposition~15.5, starting on page 173:}
   $b$ \lto $b_q$.



	
[p.182,Lemma~16.7]  I do not understand why
the bound holds on each half.  It seems that
the decomposition into the halves might not be
compatible with the geometry: cone or quoin
terms might ``cross over'' into the other half.
At any rate, it is not a direct consequence
of Theorem~8.4.

Proposed fix: Show by an interval arithmetic
calculation that each side separately satisfies
that bound $0$ if each vertex has height at most $2.3$ (HYPOTHESIS I\_5127197465).
If any vertex has height greater than $2.3$ show that the $\op{vor}_0$-scored quad cluster scores
less than $-1.04\,\op{pt}$.  For this, we may use the deformations of Lemmas~12.10 and 13.1.  We may also use calculation I\_474496219, which shows that if the diagonal reaches $2\sqrt2$, each half is at most $0.009$.  By these deformations and this calculation, the result now follows from calculation (HYPOTHESIS I\_7710172071).
%%XX
% Check in Mathematica NMaximize, so this is at least reasonable.

[p.241]  {\it `Mixed' is defined so as to include
the pure analytic case.  In earlier papers,
`mixed' excludes the pure analytic.  }
	$$
	\text{mixed}\lto\text{mixed or pure}
	$$
	
[p.243,\line+13,\line+14,\line+15]
	{\it Delete three sentences:}
	`Let $v_{12}$ be $\ldots$  We let $\ldots$
	 Break the pentagon $\ldots$'
	
[p.248,last displayed formula]  
	$=$ \lto $+$
{\it so that it reads}
	$$
	\sum_i f_{R_i}(D) \le \hat\sigma(Q_i) +
	\op{vor}_{R',0}(D) + \pi_R
	$$

[p.252,Sec.~25.7,Cases~2 and 3]  {\it `The flat quarter'
is mentioned, but there are no flat quarters
that have been introduced into the context.  
This passage
has been moved by a cut-and-paste edit to a
place it does not belong.}

[p.254,\line+7]
to branch combine \lto to combine

Please report further errors to
Thomas C. Hales.

\bibliographystyle{abbrv}
\bibliography{all}

\bigskip
\svninfo






\end{document}
