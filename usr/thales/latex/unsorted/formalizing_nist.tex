% Template for an AMS article style
\documentclass[11pt]{amsart} %{llncs} % [11pt]{amsart}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{alltt}
\usepackage{url}

%tikz graphics
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{chains,shapes,arrows,trees,matrix,positioning,decorations,backgrounds,fit}

\newtheorem{definition}[section]{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{assumption}[section]{Assumption}
\newtheorem{corollary}[section]{Corollary}
\newtheorem{remark}{Remark}


\parindent=0pt
\parskip=\baselineskip

% Math notation.
\def\op#1{{\operatorname{#1}}}
\newcommand{\ring}[1]{\mathbb{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Formalizing NIST cryptographic standards}
\author{Thomas C. Hales} 
%\institute{University of Pittsburgh\\
%\email{hales@pitt.edu}
\maketitle

% Date: October 25, 2013

\begin{abstract} Based on Snowden documents, the New York Times reported that 
the NSA worked to subvert NIST cryptographic standards~\cite{NYT-backdoor}.  
This article discusses general weaknesses in the NIST standard 800-90A for pseudo-random number
generation.
Among other defects, this NIST standard is deficient because of a pervasive sloppiness in the use of
mathematics.
This, in turn, prevents serious mathematical analysis of the standard and promotes careless implementation in code.
We propose formal verification methods as a remedy.
\end{abstract}

\section{Levels of Mathematical Rigor}

We may categorize mathematical argument as 
 informal,  rigorous, or formal.


Informal mathematics is the vulgar product of the popular press.
Informally, a function is continuous if it can be sketched without lifting a pencil from a sheet of paper;
chaos is the unpredictable
effect of a butterfly flapping its wings in Brazil; and 
a pseudo-random number is one that is paradoxically deterministic yet effectively random.
Informal mathematics is not wrong so much as it is unsuitable for careful science
and engineering.

 A more rigorous approach
to mathematics became necessary in the final decades of the nineteenth century  
to resolve 
paradoxes in Cantor's set theory and other troubles.
For example, disputes about continuity were resolved by clarifying definitions,
eventually refining a single intuitive notion of continuity into a family of related notions: 
continuity, uniform continuity, 
and so forth.  Most professional mathematical publications now adhere to widely accepted 
standards of mathematical
rigor,  enforced through the  diligence of human referees.

Formal mathematics is yet a higher standard~\cite{Hales:2008:formal}.   English and other natural languages are abandoned and 
replaced with languages designed for mathematical
precision. The syntax and the semantics of language are precisely formulated.
The system specifies
every admissible rule of logic and  every mathematical axiom.  
Quality is enforced by a computer, which exhaustively checks every logical step of a proof.

Formal verification becomes appropriate in proofs whose complexity surpasses the capabilities of checking by hand.
(A wiki page catalogues numerous long mathematical theorems that might benefit
from formalization~\cite{WikiLong}.)  
Formal methods are well-suited for many computer-assisted mathematical proofs.  
In fact, at the formal level the line is erased between algorithms implemented as computer code
and mathematical reasoning.  A single software system
 handles  the formal verification of both.  

Formal methods have been under development for decades, and in recent years the verification
of complex software systems, hardware, and intricate theorems has become a reality~\cite{FVI}.  
Already in 1989, it was possible to formally specify and verify a simple computer system from high-level language
to microprocessor~\cite{BHMY}.  As recent examples, 
we mention the full
verification of a C compiler~\cite{CC-Web}
and complex mathematical theorems such as the Feit-Thompson theorem~\cite{OOT-13} and the Four-Color theorem~\cite{gonthier:2008:formal}.


\section{Formal Verification in Cryptography}

Formal verification of computer code can be advised when
human life or large economic interests are at stake:   
aircraft control systems, widely adopted cryptographic standards, or nuclear reactor controllers.
Formal verification reduces the software defect rate to a level that is
scarcely attainable by other means.

For several reasons,  cryptography calls out for formal verification.
The field is highly mathematical.  Many key algorithms can be implemented as
small blocks of code.  A tiny defect can potentially defeat the entire algorithm.
Adversaries actively seek out bugs to exploit.
Cryptography safeguards
large financial interests and fundamental human freedoms.

Various formal tools have been constructed especially for application to cryptography~\cite{MPRI-Notes}.
The $\pi$-calculus 
% R. Milner. Communicating and Mobile Systems: the Pi-calculus. Cambridge University Press, 1999.
has been adapted to cryptographic protocols~\cite{AF},~\cite{SPI},~\cite{RM99}.
% M. Abadi and C. Fournet, Mobile values, new names, and secure communication. In POPL '01: Proceedings of the 28th ACM SIGPLAN-SIGACT
% symposium on the principles of programming languages, pages 104-115, New York, NY, USA 2001.
% M. Abadi and A. D. Gordon, A calculus for cryptographic protocols: The spi calculus. Information and Communication, 148(1):1--70, January 1999.
Projects in the Isabelle proof assistant
include protocol verification through inductive definitions \cite{Pau-ind} or game analysis \cite{Berg-thesis}. In the Coq proof assistant,
there have been successful 
formal verifications of cryptographic primitives \cite{Nowak} and code-based cryptographic proofs \cite{Barthe-2009}.
Significantly, formal methods have started to enter the standardization process~\cite{Meadows}.

The working group on the {\it Theoretical Foundations of Security
Analysis and Design}
~\cite{TFSAD}
% http://www.dsi.unive.it/IFIPWG1_7/
%\cite{FOSAD}
% Foundations of Security Analysis and Design.
%% The volumes of FOCAD.
and the Computer Security Foundations Symposium of the IEEE~\cite{CSF2013} promote formal methods in cryptography.
%http://csf2013.seas.harvard.edu/index.html
%http://www.ieee-security.org/CSFWweb/

In truth, our imperfect knowledge prevents the
comprehensive verification of cryptographic systems.
We are stymied by notorious problems like
P versus NP  and the existence of one-way functions.  
We lack definitive lower bounds on the computational complexity of concrete problems such as
factoring of integers.
Research into security reductions is ongoing.  
There is no comprehensive security model.  
For example, the Dolev-Yao model works at a high level of abstraction, assuming that cryptographic primitives function perfectly~\cite{DY},
% D. Dolev and A. C. Yao, On the security of public-key protocols. IEEE Transaction on Information Theory, 2(29):198--208, March 1983.
while other models operate at various levels of detail.

Nevertheless, we can work with these limitations, implementing a collection of interrelated formal proofs
grounded in current technological capabilities, and move forward from there.


\section{NIST standards}

Earlier critiques of the NIST standard 800-90A for pseudo-random number
generation have focused on specific
defects~\cite{Green-EC},\cite{2006-190}.  Here, we argue that 
mathematical weaknesses run throughout the standard.  Amid the accusations that the NSA has undermined NIST
cryptographic standards, we remind NIST that one of the most effective ways to subvert a cryptographic standard is to 
muddle the math.

The first requirement of a standard is to set the tone and level of discourse that reflects the current
technological capabilities of the matter at hand.  By choosing to present an informal standard, 
avoiding both rigor and formal mathematics,
NIST has produced a standard that is out of step with the mathematical technology.

Some definitions in the NIST standard are merely informal.
For example,  the NIST standard defines\footnote{Here is the full definition from NIST: 
``A process (or data produced by a process) is said to be pseudorandom when the
outcome is deterministic, yet also effectively random, as long as the internal action of the process
is hidden from observation.  For cryptographic purposes, `effectively' means `within the limits
of intended cryptographic strength.'"  Speaking of the data, we may ask with Knuth,  ``is 2 a random number?'' } 
pseudo-randomness as ``deterministic yet also effectively random'' \cite[page 7]{NIST}.
A mathematically rigorous definition of pseudo-random generators
requires much more care, referencing rigorous notions of measure, probability,
and complexity theory.  Properly formulated definitions are given in \cite{Luby1996}, \cite{Yao82}, \cite{BM84}.
As it is manifestly impossible to base rigorous or formal mathematical proofs
on something so vague as ``deterministic yet effectively random,'' the early pages of the NIST standard 
effectively ward off careful mathematical analysis.

The lack of rigor continues throughout the document.  Algorithms are described with English-text
pseudo-code.  With more care,  NIST might
have provided a formal specification and a reference implementation in executable code in a language
with documented semantics.
Overall, the standard gives few mathematical arguments,
and these do not inspire confidence.   The document 
slips into convenient inaccuracies:  
heuristics rather than proofs,  fixed-point arithmetic, 
and Shannon entropy rather than min-entropy.  (See \cite[Appendix C.2]{NIST}.)
In fact, the standard is imprecise to such a degree that competing definitions of entropy are largely irrelevant.

\section{An example of NIST reasoning}

This section goes into detail about a particular mathematical argument that appears in the
NIST standard.\footnote{We pick the most extensive mathematical argument in the document.  It is telling
that this argument is used to justify weakening the standard for the sake of efficiency.} 
For our purposes, the topic of discussion matters less than the nature of the NIST committee's mathematical
thought.  Do they reason as a mathematician in an unbroken chain of logic, or is the committee
satisfied by a lesser standard? 


The context is the following.  Let $E$ be an elliptic curve defined over a finite field $\ring{F}_p$,
defined in affine coordinates by a polynomial equation
$y^2 = f(x)$.
The pseudo-random generator extracts bits from the
 $x$ coordinates of a sequence of points $P_1, P_2,\ldots $ 
on the elliptic curve.  The construction of the sequence of points $P_i$ does not concern us here.
The issue is this: if points are sampled uniformly at random from $E(\ring{F}_p)$, then their $x$ coordinates are 
not uniformly distributed in $\ring{F}_p$; in fact, the $x$ coordinates obviously belong to the subset of $\ring{F}_p$ on which $f(x)$ is a square.  
Research estimates are needed to
determine how big an issue this is.   Aggressive truncation of bits from the binary representation of $x$ might improve pseudo-randomness
but would make the algorithm less efficient.%
\footnote{In light of the much discussed back door to the elliptic curve algorithm, NSA had a secret interest
in persuading users not to discard many bits from  $x$; aggressive truncation would
make the back door more difficult to use.}

NIST quotes the research \cite{MS2000} as ``an additional reason that argues against increasing truncation.''
There are numerous gaps in NIST reasoning.
\begin{enumerate}
\item A bound on discrepancy is not the same as uniform distribution~\cite{SDA1651}.
% See http://www.encyclopediaofmath.org/index.php/Discrepancy
\item Uniform distribution is not the same as cryptographically secure pseudo-randomness.
\item The sets $\{P_i\}$ of points  used in real-world implementations have cardinalities far too small to
be relevant to the given asymptotic estimates.
\item The research does not support the specific NIST rule that ``the recommended number of bits discarded
from each $x$-coordinate will be $16$ or $17$'' and does not convincingly ``argue against increasing truncation.''
\end{enumerate}
Nevertheless, NIST uses the research to make the inflated claim that ``certain
guarantees can be made about the uniform distribution of the resulting truncated quantities'' \cite{NIST}.
This is proof by intimidation.

\section{Assurances}

The NIST standard 800-90A states that ``a user of a DRBG for cryptographic purposes requires assurance
that the generator actually produces (pseudo) random and unpredictable bits.  The user needs assurance
that the design of the generator, its implementation and its use to support cryptographic services are
adequate to protect the user's information'' \cite{NIST}. We agree.

What assurances does NIST actually provide about the generator?  
The document contains no mathematical proofs of pseudo-randomness and no supporting citations.  
Indeed, careful proofs would be impossible,
because as we have noted,
definitions are more informal than rigorous.  Instead, the user of the standard must
rely on NIST's authoritative claim that ``the design of each DRBG mechanism in this Recommendation has
received an evaluation of its security properties prior to its selection for inclusion in this Recommendation.''
That one sentence is the extent of NIST assurance of design.  That's it!  It seems that for NIST,
assurance means to comfort with soothing words. To a mathematician, this attitude is exasperating.
 There is no mathematical statement of what
those security properties are, and no discussion of the methods that were used to reach the conclusion.
We are not told who made the evaluation or what the results of the evaluation were.

Based on the {\it Memorandum of Understanding} between NIST and NSA from 1989 \cite[p.601]{ScAC}, we might
wonder whether NIST's part in the evaluation was limited.
According to the memo, ``The NIST will $\ldots$ recognize the NSA-certified rating of evaluated trusted
systems under the Trusted Computer Security Evaluation Criteria Program {\it without requiring additional
evaluation}'' (italics added). 

Here is the NIST discussion of {\tt HMAC\_DRBG}
(deterministic random bit generation based on hash message authentication codes).  It states, ``In general, even
relatively weak hash functions seem to be quite strong when used in the HMAC construction.
On the other hand, there is not a reduction proof from the hash function's collision resistance
properties to the security of the DRBG'' \cite[Appendix E.2]{NIST}. 
Note the informal tone of the discussion, the  reassurance
that a weakness is strength, the brevity, and absence of mathematical theorems.  

Cryptographic standards derive their security through the
underlying mathematics.   We can place our trust in mathematics but not in assurances such as these.

\section{Conclusions}


According to NIST aspirations,
``NIST works to publish the strongest cryptographic standards possible.'' \cite{NIST-Supp}.
% http://csrc.nist.gov/publications/nistbul/itlbul2013_09_supplemental.pdf
Our analysis shows that judged by professional mathematical standards, NIST is 
very far from its goal. Indeed, the current NIST standard was written in a pre-Snowden era of unverified assurances.

NIST sets the standard both
by its choice of algorithms and by its attitude towards rigor.
Overall,  its general careless tone will 
facilitate vulnerable implementations of the standard.  

Better approaches to standardization are available.  In fact,
a number of formal verification projects have been completed (such as a formal verification of a C compiler
mentioned above) that dwarf what we specifically ask NIST  to do.  Please
adopt verification technologies in widespread use!
Improvement in the formal specification of NIST standards is the first
critical step in a larger process of formal verification along the entire chain, including
the
underlying mathematical concepts, cryptographic primitives, protocols and algorithms, 
and end implementations
in computer code.


 

\bibliographystyle{amsplain}
\bibliography{../bibliography/all}


\end{document}


