\newcommand{\R}{\mathbb{R}}
\newcommand{\IR}{\mathbb{IR}}
\newcommand{\Arctan}{\mathop{\rm Arctan}}
\newcommand{\abss}[1]{\lvert#1\rvert}
\newcommand{\iabs}{\mathop{\rm iabs}}
\newcommand{\bx}{{\bf x}}
\newcommand{\GT}{G_{\hbox{\tiny Taylor}}}


\begin{abstract}
  This article describes a formal proof of the Kepler conjecture in a
  combination of the HOL Light and Isabelle proof assistants.  This
  paper constitutes the official published account of the now
  completed Flyspeck project.
\end{abstract}


\section{Introduction}


In 1611, Kepler published the booklet {\it Six-Cornered Snowflake},
which contains a statement now known as the Kepler conjecture: no
packing of congruent balls in Euclidean three-space has density
greater than that of the face-centered cubic packing~\cite{XX}.  It is
the oldest problem in discrete geometry.  In 1900, Hilbert included
the Kepler conjecture in his influential list of mathematical
problems.  Starting in the 1950s, L.\ Fejes T\'oth gave a coherent
proof strategy and eventually suggested that computers might be used
to study the problem~\cite{XX}.  The truth of the Kepler conjecture
was established by Ferguson and Hales in 1998, but their proof was not
published in full until 2006~\cite{DCG}.

The delay in publication was caused by the difficulties that the
referees had in verifying a complex computer proof.     Lagarias has
described the review process~\cite{LagXX}.
He writes, ``The nature of this proof $\ldots$ makes it hard for humans
to check every step reliably. $\ldots$ [D]etailed checking of many
specific assertions found them to be essentially correct in every
case.  The result of the reviewing process produced in these reviewers
a strong degree of conviction of the essential correctness of this
proof approach, and that the reduction method led to nonlinear
programming problems of tractable size.''  
In the end, the proof
was published without complete certification from the
referees.

At the Joint Math meetings in Baltimore in 2003(?), Hales announced a
project to give a formal proof of the Kepler conjecture, and later he
published a project description \cite{MAPS XX}.
%  A published announcement of the project later appeared in
%in [MAPS]. 
% {Introduction to the Flyspeck Project, MAPS 05021. http://drops.dagstuhl.de/opus/volltexte/2006/432}.
%That announcement states ``In truth, my motivations for the project are far more complex than a simple hope of 
%removing residual doubt from the minds of few referees. 
%Indeed, I see formal methods as fundamental to the long-term growth of mathematics.''
The project is called {\it Flyspeck}, an expansion of the the acronym
FPK, for the Formal Proof of the Kepler conjecture.  The project has
formalized both the traditional text parts of the proof as well as the
parts of the proof that are implemented in computer code as
calculations.  This paper constitutes the official published account
of the now completed Flyspeck project.

The first definite contribution to the project was the formal
verification of a major piece of computer code that was used in the
proof, in work at TU Munich by Bauer under the direction of
Nipkow. (See section XX.)  Major work on the project started when NSF
funded the project in 200(XX?).  An international conference on the
project and formal proofs sponsored by the NSF and the Hanoi Math
Institute in 200(XX?) transformed the project into a large
international collaboration.  There have been numerous research papers
and theses directly related to this project ~\cite{XX}.  The roles of
the individual projet members have been spelled out in the
announcement of the proof~\cite{XX}.  The book ``Dense Sphere
Packings'' describes the mathematical details of the proof that was
formalized.  This article focuses on the formalization itself.

The Flyspeck project has roughly similar size and complexity as other
major formalization projects such as the Feit-Thompson
theorem~\cite{XX}, the Compcert project giving a verified C compiler
\cite{XX}, and the XX project giving a verified operating system
microkernel.~\cite{XX}.  Although our project may set a new record in
terms of lines of code in a verification project, this may have more
to do with some inefficiencies in our approach than with the true
complexity of the project.

The code and documentation for the Flyspeck project are available at
a Google code repository devoted to the project~\cite{XX}.  The
parts of the project that have been carried out in Isabelle are
available from the Isabelle code repository.  Other software tools are
needed for the execution of the code, such as subversion (for
interactions with the code repository), HOL Light, OCaml (the
implementation language of HOL Light), caml P5 parsing (for syntax
extension to OCaml for parsing of mathematical terms), glpk (for
linear programming), and Isabelle/HOL. [XX many citations.]

The main statement in the formal proof of the Kepler conjecture can be
verified in about 5 hours (??XX) on an ordinary laptop computer.  We
encourage readers of this article to make an independent
verification of this main statement on their computers.  This main
statement reduces the Kepler conjecture to three computationally
intensive subclaims that are described in Section~\ref{sec:statement}.  Two of
these subclaims can be checked in less than a day each.  The third and
most difficult of these subclaims takes about 5000 hours of CPU hours
to verify.



\section{The HOL Light proof assistant}

The formal verifications have been carried out in the HOL Light and
Isabelle proof assistants.

[XX Add about a page on HOL Light.]

This article displays various terms that are represented in HOL syntax.  For
the convenience of the reader, we note some of the syntactic
conventions of HOL Light.  In particular, the universal quantifier
$\forall$ is written as (!), the existential quantifier $\exists$ is
written (?), the embedding of natural numbers into the real numbers is
denoted (\&).


\section{The statement}\label{sec:statement}

As mentioned in the introduction, the Kepler conjecture asserts that
no packing of congruent balls in Euclidean three-space can have
density exceeding that of the face-centered cubic packing.  That
density is $\pi/\sqrt{18}$, or approximately $0.74$.  The
face-centered cubic packing is not the only packing that realizes the
bound.  The hexagonal-close packing and various packings combining
layers from the hexagaon-close packing and the face-centered cubic
packing all achieve this bound.  The theorem that has been formalized
does not make any uniqueness claims.

The density of a packing is defined as a limit of the density obtained
within finite containers, as the size of the container tends to
infinity.  To make the statement in the formal proof as simple as
possible, we formalize a statement about the density of a packing
inside a finite spherical container.  This statement contains an error
term.  The ratio of the error term to the volume of the container
tends to zero as the volume of the container tends to infinity.  Thus
in the limit, we obtain the Kepler conjecture in its traditional form.

As a ratio of volumes, the density of a packing is scale invariant.
There is no loss of generality in assuming that the balls in the
packing are normalized to have unit length.  We identify a packing of
balls in $\ring{R}^3$ with the set $V$ of centers of the balls, so
that the distance between distinct elements of $V$ is at least $2$,
the diameter of a ball.

More formally, we have the following definition.

\begin{obeylines}

\begin{verbatim}

`(packing V <=> 
  (!u v. u IN V /\ v IN V /\ dist(u,v) < &2 ==> u = v))`

\end{verbatim}
\end{obeylines}
This definition states that $V$ is a packing if and only if for every
$\u, \v \in V$, if the distance from $\u$ to $\v$ is less than $2$,
then $\u=\v$.

We define the constant {\tt the\_kepler\_conjecture} to be the term

\begin{obeylines}

\begin{verbatim}
`the_kepler_conjecture <=>
  (!V. packing V
    ==> (?c. !r. &1 <= r
        ==> &(CARD(V INTER ball(vec 0,r))) <=
            pi * r pow 3 / sqrt(&18) + c * r pow 2))`
\end{verbatim}
\end{obeylines}

In words, we define the Kepler conjecture to be the following claim:
for every packing $V$, there exists a real number $c$ such that for
every real number $r\ge 1$, the number of elements $V$ contained in an
open spherical container of radius $r$ centered a the origin is at
most
\[
  \frac{\pi\, r^3}{\sqrt{18}} + c\, r^2.
\]
An analysis of our proof of the theorem shows that there exists a
small computable constant $c$ that works uniformly for all packings
$V$, but we only formalize the weaker statement that allows $c$ to
depend on $V$.  The restriction $r\ge 1$, which bounds $r$ away from
$0$, is needed because there can be arbitrarily small containers whose
intersection with $V$ is nonempty.

The proof of the Kepler conjecture relies on a combination of
traditional mathematical argument and three separate bodies of
computer calculation.  The results of the computer calculations have
been expressed in precise mathematical terms and specified formally in
HOL Light.  The computer calculations are as follows.
\begin{enumerate}
\item The proof of the Kepler conjecture relies on nearly a thousand
  nonlinear inequalities.  These inequalities can be expressed in
  terms of trigonometric and inverse trigonometric functions, the
  square root function, and arithmetic over the real numbers.  The
  term \verb!the_nonlinear_inequalities! in HOL Light is the
  conjunction of these nonlinear inequalities.  See Section~XX.
\item The combinatorial structure of each possible counterexample to
  the Kepler conjecture is encoded as a planar graph satisfying a
  number of restrictive conditions.  Any graph satisfying these
  conditions is said to be {\it tame}.  A list of all tame graphs up
  to isomorphism has been generated by an exhaustive computer search.
  The formal statement that every tame graph is isomorphic to one of
  these cases can be expressed in HOL Light as
  \verb!import_tame_classification!.  See Section~XX.
\item The final body of computer code is a large collection of linear
  programs.  The results have been formally specified as
  \verb!linear_programming_results! in HOL Light.  See Section~XX.
\end{enumerate}

It is then natural to break the formal proof of the Kepler conjecture
into four parts: the formalization of the text part (that is, the
traditional non-computer portions of the proof), and the three
separate bodies of computer calculations.  Because of the size of the
formal proof, the full proof of the Kepler conjecture has not been
obtained in a single session of HOL Light.  What we formalize in a
single session is a theorem

\begin{obeylines}

\begin{verbatim}
|-  the_nonlinear_inequalities /\
    import_tame_classification
    ==> the_kepler_conjecture
\end{verbatim}

\end{obeylines}

This theorem represents the formalization of two of the four parts of
the proof: the text part of the proof and the linear programming.  It
leaves the other two parts (nonlinear inequalities and tame
classification) as assumptions.  The formal proof of the assumption
\verb!the_nonlinear_inequalities! is
described in Section~XX.  The formal proof of
\verb!import_tame_classification! in Isabelle is
described in Section~XX.
%was obtained in the Isabelle proof
%assisant and translated into a term in HOL Light, as described in
%Section~XX.  
Thus, combining all these results from various
sessions of HOL Light and Isabelle, we have obtained a formalization
of every part of the proof of the Kepler conjecture.



\section{Text formalization}


The next sections of this article turn to each of the four parts of
the proof of the Kepler conjecture, starting wih the text part of the
formalization in this section.  In the remainder of this article, we
will call the proof of the Kepler conjecture as it appears in
\cite{DCG} the {\it original proof}.
%The formalization of the text part of the proof of the Kepler conjecture follows the plan presented
%in the book {\it Dense Sphere Packings: A Blueprint for Formal Proofs} \cite{XX}.  
The proof that appears in \cite{XX} will be called the {\it blueprint
  proof}.  The formalization closely follows the blueprint proof.  In
fact, the book and the formalization were developed together, with
numerous revisions of the text in response to issues that arose in the
formalization.

Since the blueprint and formal proof were developed at the same time,
the numbering of lemmas and theorems continued to change as project
took shape.  The blueprint and formal proof are cross-linked by a
system of identifiers that persist through project revisions, as
described in \cite{Urban XX}.



\subsection{differences between the original proof and the blueprint proof}

The blueprint proof follows the same general outline as the original
proof.  However, many changes have been made to make it more suitable
for formalization.  We list some of the primary differences between
the two proofs

\begin{enumerate}
\item In the blueprint proof, topological results concerning planar
  graphs are replaced with purely combinatorial results about
  hypermaps.  
\item The blueprint proof is based on a different geometric partition
  of space than that originally used.  Marchal introduced this
  partition and first observed its relevance to the Kepler conjecture.
 Marchal's partition is described by 
   rules that are better adapted to formalization than the original.
 \item In a formal proof, every new concept comes at a cost: libraries
   of lemma must be developed to support each concept.  We have
   organized the blueprint proof around a small number of major
   concepts such as spherical trigonometry, volume, hypermap, fan,
   polyhedra, and Voronoi partitions.
 \item The statements of the blueprint proof are more precise, and
   make all hypotheses explicit.
 \item To facilitate a large collaboration, the chapters of the
   blueprint have been made as independent from one another as
   possible, and long proofs have been broken up into a series of
   shorter lemmas.
\item In
  the original, computer calculations were a last resort after
  as much was done by hand as feasible.
  In the
  blueprint, the use of computer has been fully embraced.  As a
  result, many laborious lemmas of the original proof can be
  eliminated.
\end{enumerate}


Because the original proof was not used for the formalization, we
cannot assert that the original proof has been formally verified to be
error free.  Similarly, we cannot assert that the computer code for
the original proof is free of bugs.  The detection and correction of
small errors is a routine part of any formalization project.  Overall,
hundreds of small errors in the proof of the Kepler conjecture were
corrected during formalization.  

\subsection{appendices to the blueprint}

As part of the Flyspeck project, the blueprint proof has been
supplemented with $84$ page unpublished appendix filled with
additional details about the proof.   We briefly describe the
appendix.

The first part of the appendix give details about how to formalize the
main estimate~\cite[Sec~7.4]{DSP}.  The main estimate is the most
technically challenging part of the original text, and its
formalization was the most technically challenging part of the
blueprint text.  In fact, the original proof of the main estimate
contained an error that was detected in the original text is described
and corrected in \cite{XX Revision}.

A second major part of the appendix is devoted to the proof of the
Cell Cluter Inequality~\cite[Thm~6.93]{DSP}.  (The theorem is stated
below, in Section~\ref{sec:rt}.)  In the blueprint text, the proof is
skipped, with the following comment: ``The proof of this cell cluster
inequality is a computer calculation, which is the most delicate
computer estimate in the book.  It reduces the cell cluster inequality
to hundreds of nonlinear inequalities in at most six variables.''

A final part of the appendix deals with the final integration of the
various libraries in the project.  As a large collaborative effort
that used two different proof assistants there were many small
differences between libraries that had to be reconciled to obtain a
clean final theorem.  The most significant difference to reconcile was
the notion of planarity as used in classification of tame graphs and
the notion of planarity that appears in the hypermap library.  Tame
graph planarity is defined by an algorithm that starts with a polygon
and successively adds loops to it in a way that intuitively preserves
planarity.  (The algorithm is an implementation in code of a process
of drawing a sequence of loops on a sheet of paper, each connected to
the previous without crossings.)  By contrast, in the hypermap
library, the Euler formula is used as the basis of the definition of
hypermap planarity.  The appendix describes how to relate the two
notions.  This appendex can be viewed as an expanded version of
Section~4.7.4 of \cite{DSP}.


\section{Nonlinear inequalities}

All but a few nonlinear inequalities in the Flyspeck project have the
following general form
\begin{equation}\label{eqn:D}
\forall \bx,\ \bx \in D \implies f_1(\bx) < 0 \vee \ldots \vee f_k(\bx) < 0.
\end{equation}
where $D = [a_1,b_1] \times \ldots \times [a_n, b_n]$ is a rectangular
domain inside $\R^n$ and $\bx = (x_1,\ldots,x_n)$. In the remaining
few inequalities, the form is simililar, except that $k=1$ and the
inequality is not strict.  In every case, the number of variables $n$
is at most $6$. The following functions and operations appear in
inequalities: basic arithmetic operations, square root, sine, cosine,
arctangent, arcsine, arccosine, and the analytic continuation of
$\arctan(\sqrt{x})/\sqrt{x}$ to the region $x > -1$.  For every point
$\bx\in D$, at least one of the functions $f_i$ is analytic in a
neighborhood of $\bx$ (and takes a negative value), but situations
arise in which not every function is analytic or even defined at every
point in the domain.

Formal verification of inequalities is based on interval
arithmetic~\cite{Moore:1966:IntervalAnalysis}. 
 For example, $[3.14, 3.15]$ is an
interval approximation of $\pi$ since $3.14 \le \pi \le 3.15$. 
In order to work with interval approximations,
arithmetic operations are defined over intervals. Denote the set of
all intervals over $\R$ as $\IR$. A function (operation) $F:\IR \to
\IR$ is called an interval extension of $f:\R \to \R$ if the following
condition holds
\begin{equation*}
\forall I \in \IR,\ \{ f(x)\,:\,x \in I \} \subset F(I).
\end{equation*}
This definition can be easily extended to functions on $\R^k$. It is
easy to construct interval extensions of basic arithmetic operations
and elementary functions. For instance,
\begin{equation*}
\begin{split}
[a_1,b_1] \oplus [a_2,b_2] &= [a, b] \text { for some $a \le a_1 + a_2$ and $b \ge b_1 + b_2$},\\
[a_1,b_1] \ominus [a_2,b_2] &= [a, b] \text { for some $a \le a_1 - b_2$ and $b \ge b_1 - a_2$}.
\end{split}
\end{equation*}
Here, $\oplus$ ($\ominus$) denotes an interval extension of $+$
($-$). We do not define the result of $\oplus$ as $[a_1 + a_2, b_1 +
b_2]$ since we may want to represent all intervals with limited
precision numbers (for example, decimal numbers with at most 3
significant figures). With basic interval operations, it is possible
to construct an interval extension of an arbitrary arithmetic
expression by replacing all elementary operations with corresponding
interval extensions. Such an interval extension is called the natural
interval extension. Natural interval extensions could be imprecise and
there are several ways to improve them.

One simple way to improve an interval enclosure of a function is to
subdivide the original interval into subintervals and evaluate an
interval extension of the function on all subintervals.  Using basic
interval arithmetic and subdivisions, it would theoretically be
possible to prove all nonlinear inequalities that arise in the
project. Unfortunately, this method will not work well in practice
since the number of subdivisions required to establish some
inequalities could be enormous, especially for multivariate
inequalities.

Both the  C++ informal verification code (from
the original proof of the Kepler conjecture) and our formal
verification procedure implemented in OCaml and HOL Light use improved
interval extensions based on Taylor approximations.

Suppose that a function $g(x)$ is twice differentiable. Fix $y \in
[a,b]$. Then we have the following formula for all $x \in [a,b]$:
\begin{equation*}
g(x) = g(y) + g'(y)(x - y) + \frac{1}{2}g''(\xi)(x - y)^2
\end{equation*}
for some value $\xi = \xi(x) \in [a,b]$.  Choose $w$ such that $w \ge
\max\{y - a, b - y\}$ and define $r(x) = \abss{g'(y)}w +
\frac{1}{2}\abss{g''(\xi(x))}w^2$. We get the following inequalities
\begin{equation*}
\forall x \in [a,b],\ g(y) - r(x) \le g(x) \le g(y) + r(x).
\end{equation*}
Let $G$, $G_1$, and $G_2$ be any interval extensions of $g$, $g'$, and
$g''$. Find $e$ such that $e \ge \iabs\bigl(G_1([y,y])\bigr)w +
\frac{w^2}{2}\iabs\bigl(G_2([a,b])\bigr)$, where
$\iabs\bigl([c,d]\bigr) = \max\{\abss{c}, \abss{d}\}$.  Assume that
$G([y,y]) = [y_l,y_u]$. Then the following function defines a (second
order) Taylor interval approximation of $g(x)$:
\begin{equation*}
\GT([a,b]) = [l, u] \text{ where $l \le y_l - e$ and $u \ge y_u + e$}.
\end{equation*}
That is,
\begin{equation*}
\forall x \in [a,b], \quad g(x) \in \GT([a,b]).
\end{equation*}
In our verification procedure, we always take $y$ close to the
midpoint $(a+b)/2$ of the interval in order to minimize the value of
$w$. There is an analogous Taylor approximation for multivariate
functions based on the multivariate Taylor theorem.

As a small example, we compute a Taylor interval approximation of
$g(x)$ on $[1,2]$. We have $g'(x) = 1 - {1}/{(1 + x^2)}$ and $g''(x)
= {-2x}/{(1 + x^2)^2}$. Take $y = 1.5$ and natural interval
extensions $G_1$ and $G_2$ of $g'(x)$ and $g''(x)$. Then $w = 0.5$,
$G([1.5,1.5]) = [0.517,0.518]$, $G_1([1.5,1.5]) = [0.692,0.693]$, and
$G_2([1,2]) = [-0.5, -0.16]$. We get $e = 0.409$ and hence $\GT([1,2])
= [0.108, 0.927]$. This result is much better than the result obtained
with the natural extension $G([1,2]) = [-0.11, 1.22]$. 
%Moreover, we
%can immediately prove \eqref{ex-ineq} since $g([1,2]) \subset
%\GT([1,2])$. Note also that the verification of \eqref{ex-ineq} with
%$\GT([1,2])$ is computationally more efficient than the verification
%with $G([1,1.5])$ and $G([1.5, 2])$. 
We note that in the calculation of $\GT([1,2])$, we evaluate the
expensive interval extension of $\arctan$ only once.  To obtain
similar accuracy with subdivision, more than one evaluation of $\arctan$
is needed.  In general, it is necessary to subdivide the original
interval into subintervals even when Taylor interval approximation are
used. But in most cases, Taylor interval approximations lead to fewer
subdivisions than natural interval extensions.

Taylor interval approximations may also be used to
prove the monotonicity of functions.  By expanding $g'(x)$ in a 
Taylor series, we obtain a Taylor interval approximation
\begin{equation*}
\forall x \in [a,b], \quad g'(x) \in \GT'([a,b]).
\end{equation*}
for some interval $\GT'([a,b])$.  If $0$ is not in this interval, then the
derivative has fixed sign, and
the function $g$ is monotonic, so that the maximum value of $g$ occurs
at the appropriate endpoint.  More generally, in multivariate inequalities,
a partial derivative of fixed sign may be used to reduce the verification on a
rectangle of dimension $k$ to a rectangle of dimension $k-1$.

A few of the inequalities are sharp.  That is, the inequalities to be
proved have the form $f \le 0$, where $f(\bx_0)=0$ at some point $\bx_0$
in the domain $D$ of the inequality.  In each case that occurs, $\bx_0$ lies at a
corner of the rectangular domain.  We are able to prove these
inequalities by showing that (1) $f(\bx_0) = 0$ by making a direct
computation using exact arithmetic; (2) $f < 0$ on the complement of
some small neighborhood $U$ of $\bx_0$ in the domain; and (3) every
partial derivative of $f$ on $U$ has the appropriate sign to make the
maximum of $f$ on $U$ to occur at $\bx_0$.  The final two steps are
carried out using our standard tools of Taylor intervals.

All the ideas presented in the discusssion above have been
formalized in HOL Light and a special automatic verification procedure
has been written in the combination of OCaml and HOL Light for
verification of general multivariate nonlinear inequalities. This
procedure consists of two parts. The first part is an informal search
procedure which finds an appropriate subdivision of the original
inequality domain and other information which can help in the formal
verification step (such as whether or not to apply the monotonicity
argument, which function from a disjunction should be verified,
etc.). The second part is a formal verification procedure which takes
the result of the informal search procedure as input and produces a
final HOL Light theorem. A detailed description of the search and
verification procedures can be found
in~\cite{Solovyev-thesis,Solovyev:NFM2013}.

All formal numerical computations are done with special finite
precision floating-point numbers formalized in HOL Light. It is
possible to change the precision of all computations dynamically and
the informal search procedure tries to find minimal precision
necessary for the formal verification. At the lowest level, all
computations are done with natural numbers. We improved basic HOL
Light procedures for natural numbers by representing natural numerals
over an arbitrary base (the base 2 is the standard base for HOL Light
natural numerals) and by providing arithmetic procedures for computing
with such numerals. Note that all computations required in the
nonlinear inequality verification procedure are done entirely inside
HOL Light, and the results of all arithmetic operations are HOL Light
theorems. As a consequence, the formal verification of nonlinear
inequalities in HOL Light is much slower than the original informal
C++ code.

The term \verb!the_nonlinear_inequalities! is defined as a conjunction
of several hundred nonlinear inequalities. The domains of these
inequalities have been partitioned to create more than 23,000
inequalities. The verification of all nonlinear inequalities in HOL
Light on the Microsoft Azure cloud took approximately 5000
processor-hours. Almost all verifications were made in parallel with
32 cores using GNU parallel \cite{Tange2011a}.  Hence the real time
was less than a week ($5000 < 32\times 168$). These verifications
were made in July and August, 2014.  Nonlinear inequalities
were verified with compiled versions of HOL Light and the verification
tool developed in Solovyev's 2012 thesis.

The verifications were rechecked at Radboud University on a server
with 120 Xeon 2.3GHz CPUs and 3TB RAM, in October 2014.  This
reverification required about 9370 processor-hours over a period of 6
days.  Identical results were obtained through rechecking.

%Almost all verifications were made in parallel with 32 cores using GNU 
%parallel \cite{Tange2011a}...


\section{Combining HOL Light sessions}


The nonlinear inequalities were obtained in a number of separate sessions
of HOL Light that were run in parallel.  By the design of HOL Light, it
is not possible to pass a theorem from one session to another without
fully reconstructing the proof in each session.  

To combine the results into a single session of HOL Light, we used a
specially modified version of HOL Light that accepts a theorem from
another session without proof.  We briefly describe this modified
version of HOL Light.

Each theorem is expressed in terms of a collection of constants, and
those constants are defined by other constants, recursively extending
back to the primitive constants in the HOL Light kernel.  Similarly,
the theorem and constants have types, and those types also extend
recursively back through other constants and types to the primitive
types and constants of the kernel.  A theorem relies on a list of
axioms, which also have histories of constants and types.
%We must also consider the list of axioms in the system and their entire
%history of constants and types as part of the history of each theorem.

The semantics of a theorem is determined by this entire history of
constants, types, and axioms, reaching back to the kernel.  The
modified version of HOL Light is designed in such a way that a theorem
can be imported from another session, provided the theorem is proved
in another session, and the entire histories of constants, types, and
axioms for that theorem are exactly the same in the two sessions.

To implement this in code, each theorem is transformed into canonical
form.  To export a theorem, the canonical form of the theorem and its
entire history are converted faithfully to a large string, and the MD5
hash of the string is saved to disk.  The modified version of HOL
Light then allows the import of a theorem if the appropriate MD5 is
found.  The import mechanism is prevented from being used in ways
other than the intended use, through the scoping rules of the OCaml
language.  

To check that no pieces were overlooked in the distribution of
inequalities to various cores, the pieces have been reassembled in the
specially modified version of HOL Light.  In that version, we obtain
a formal proof of the theorem

\begin{verbatim}
|- the_nonlinear_inequalities
\end{verbatim}

This theorem is exactly the assumption made in the formal proof
of the Kepler conjecture, as stated in Section~\ref{sec:statement}.
We remark that the modified version of HOL Light is not used during
the proof of any other of the main results of the Kepler conjecture.
It is only used to assemble the small theorems from parallel sessions
to produce this one master theorem.

\section{Tame Classification}

[XX insert about 2 pages of summary of Bauer-Nipkow project.]

The first major success of the Flyspeck project was the formalization
of the classification of tame plane graphs.  In the original proof of
the Kepler conjecture, this classification was done by computer, using
custom software to generate planar graphs satisfying given
properties.  This formalization project thus involved the verification
of computer code.  Work on the formal verification of the code was
started by Gertrud Bauer, and became the subject of her PhD thesis.
The work was completed by Bauer and Nipkow in \cite{XX}.


\section{Importing results from Isabelle}


The tame graph classification was done in the Isabelle/HOL proof
assistant, while all the rest of the project has been carried out in
HOL Light.  It seems that it would be feasible to translate the
Isabelle code to HOL Light to have the entire project under the same
roof, but this lies beyond the scope of the Flyspeck project.

Current tools do not readily allow the automatic import of this result
from Isabelle to HOL Light.  A tool that automates the import from
Isabelle to HOL Light was written by S. McLaughlin with precisely this
application in mind~\cite{XX}, this tool has not been maintained.  A
more serious issue is that the proof in Isabelle uses computational
reflection: the code is verified then translated into ML code, which
is executed, and its output is then used as a theorem in Isabelle.
The HOL Light kernel does not permit reflection.  Thus, the reflected
portions of the formal proof would have to be modified as part of the
import.

Instead, we leave the formalization of the Kepler conjecture
distributed between two different proof assistants.  In HOL Light, the
Isabelle work appears as an assumption

\verb!import_tame_classification!.  This constant is 

\begin{obeylines}

\begin{verbatim}
|- import_tame_classification <=>
     (!g. g IN PlaneGraphs /\ tame g ==> fgraph g IN_simeq archive)
\end{verbatim}

\end{obeylines}

The right-hand side is the translation into HOL Light of the following
completeness theorem in Isabelle:

%\begin{obeylines}

%\begin{verbatim}
%|- "g \<in> PlaneGraphs" and "tame g" 
%       shows "fgraph g \<in>\<^isub>\<simeq> Archive"
%\end{verbatim}

%\end{obeylines}

%\begin{verbatim}
\begin{lstlisting}[keepspaces=true,stringstyle=\tt,basicstyle=\small,%
frame=none,framesep=8pt,mathescape,morekeywords={and,shows},columns=flexible]
|- "g $\in$ PlaneGraphs" and "tame g" shows "fgraph g $\in_{\simeq}$ Archive"
\end{lstlisting}
%\end{verbatim}
% http://afp.sourceforge.net/browser_info/current/AFP/Flyspeck-Tame/Completeness.html

In informal terms, the assumption asserts that every tame plane graph
is isomorphic to a graph appearing in a certain long explict list (the
archive) of graphs.  All of the HOL terms \verb!PlaneGraphs!,
\verb!tame!, \verb!archive!, \verb!IN_simeq!, \verb!fgraph! are
verbatim translations of the corresponding definitions in Isabelle
(extended recursively to the constants appearing in the definitions).
The types are similarly translated between proof assistants (lists to
lists, natural numbers to natural numbers, and so forth).  These
definitions and types were translated by hand.  The archive of graphs
is generated from the same ML file for both the HOL Light and the
Isabelle statements.


Since the formal proof is distributed between two different systems
with two different logics, we briefly indicate why this theorem in
Isabelle must also be a proof in HOL (assuming the consistency of
Isabelle).  Briefly, this particular statement could be expressed as a
SAT problem in first-order propositional logic.  SAT problems pass
directly between systems and are satisfiable in one system if and only
if they are satisfiable in the other (assuming the consistency of both
systems).  In expressing the classification theorem as a SAT problem,
the point is that all quantifiers in the theorem are actually bounded
and can thus be expanded as finitely many cases in propositional
logic.  We also note that the logics of HOL Light and Isabelle/HOL are
very closely related to one another.
%Each of the finitely many possible graphs can be
%specified by a finite number of boolean conditions.  Similarly, graph
%isomorphism can be replaced by an enumeration of finitely many
%possible bijections of vertices.  Tameness involves some conditions in
%integer arithmetic, but again everything is bounded, and a bounded arithmetic
%table can be drawn up in boolean gates using the usual tricks.
%Thus, at a theoretical level, there is no difficulty in sharing this
%theorem between systems.

\section{Linear programs}

The blueprint proof reduces infinite packings that are potential
counterexamples to the Kepler
conjecture to finite packings $V$ with at most $15$ points and with
other properties. These packings are called contravening packings. It
is also shown that the combinatorial structure of a contravening
packing can be encoded with a tame hypermap (graph). The tame
graph classification theorem  shows that there are finitely many
tame hypermaps. For each tame hypermap it is possible to generate a
list of inequalities which must be satisfied by a potential
counterexample (associated with the given tame hypermap). Most of
these inequalities are nonlinear. 

To rule out a potential counterexample, it is enough to show that the
system of nonlinear inequalities has no solution.  A linear relaxation
of these inequalities is obtained by replacing nonlinear quantities
with new variables. For instance, the dihedral angles of a simplex are
nonlinear functions of the edge lengths fo the simplex. New variables
are introduced for each angle and linear combinations of angles become
linear forms. The next step is to show that the linear program is not
feasible. This implies that the original system of nonlinear
inequalities is inconsistent, and hence there is no contravening
packing associated with the given tame hypermap. The process of
construction and solution of linear programs is repeated for all tame
hypermaps and it is shown that no contravening packings (and hence
counterexamples to the Kepler conjecture) exist.

There are two parts in the formal verification of linear
programs. First, linear programs are generated from formal theorems,
nonlinear inequalities, and tame hypermaps. Second, a general linear
program verification procedure verifies all generated linear programs.

Formal generation of linear programs follow the procedure outlined
above. The first step is generation of linear inequalities from
properties of contravening packings. Many such properties directly
follow from nonlinear inequalities. As described above, nonlinear
inequalities are transformed into linear inequalities by introducing
new variables for nonlinear expressions. In fact, we do not change
original nonlinear inequalities but simply introduce new HOL Light
constants for nonlinear functions and reformulate nonlinear inequalities
in a linear form.

For example, suppose we have the following inequalities: $x +
x^2 \le 3$ and $x \ge 2$. Define $y = x^2$ to obtain the following
linear system of inequalities: $x + y \le 3$, $x \ge 2$, and $y \ge 4$
(the last inequality follows from $x \ge 2$).  We
ignore the nonlinear dependency of $y$ on $x$, and obtain a system of linear
inequalities which can be easily shown to be inconsistent.

The generation of linear inequalities from properties of contravening packings is
a semi-automatic procedure: many such inequalities are derived with a special
automatic procedure but some of them need manual formal proofs.

The next step is the relaxation of linear inequalities with irrational
coefficients. We do not compute with
irrational numbers directly.  Consider the example, $x - \sqrt{2} y \le
\pi$, $x + y \le \sqrt{35}$, $x \ge 5$, $y \ge 0$. We have
\begin{equation*}
\begin{split}
x - \sqrt{2} y \le \pi &\implies x - 1.42 y \le \pi \implies x - 1.42 y \le 3.15,\\
x + y \le \sqrt{35} &\implies x + y \le 6.
\end{split}
\end{equation*}
Note that the implication $x - \sqrt{2} y \le \pi \implies x - 1.42 y
\le \pi$ is valid because $y \ge 0$. All variables used in our linear
programs  are nonnegative and hence we can apply similar
transformations in the actual linear programs our project. The system
$x - 1.42 y \le 3.15$, $x + y \le 6$, $x \ge 5$, $y \ge 0$ is
inconsistent and thus the original system is also inconsistent.

The relaxation of irrational coefficients is done completely
automatically. In fact, inequalities with integer coefficients are
produced by multiplying each inequality by a sufficiently large power
of $10$ (decimal numerals are used in the verification of linear
programs).

The last step of the formal generation of linear programs is the
instantiation of free variables of linear inequalities with special
values computed from an associated tame hypermap. In this way, each
tame hypermap produces a linear program which is formally checked for
feasibility. Not all linear programs obtained in this way are
infeasible.  For about half of thel tame hypermaps, linear relaxations
are insufficiently precise and do not yield infeasible linear
programs.  In that situation, a feasible linear program is split into
several cases where new linear inequalities are introduced. New cases
are produced by considering alternatives in the form $x \le a \vee a
\le x$ where $x$ is some variable and $a$ is a constant (other forms
of additional inequalities are possible; it is only important that all
alternatives are considered). The main idea of introducing new
inequalities is that some other inequalities derived from nonlinear
properties may be replaced with more precise linear relaxations.

Case splitting for verification of linear programs in the project is
automatic. In fact, a special informal procedure finds all required
cases first and the formal verification procedure of linear program is
applied to these cases only.

Formal verification of general linear programs is relatively easy. We
demonstrate our verification procedure with an example. A detailed
description of this procedure can be found in~\cite{Solovyev:LP}. All
variables in each verified linear program must be nonnegative and
bounded. Our goal is to verify that the following system is
infeasible:
\begin{equation*}
\begin{split}
x - 1.42 y \le 3.15,\ x + y \le 6,\ 
5 \le x \le 10,\ 0 \le y \le 10.
\end{split}
\end{equation*}
Introduce slack variables $s_1, s_2$ for inequalities which do not
define bounds of variables and construct the following linear program
\begin{equation}\label{eqn:lp}
\begin{split}
&\text{miminimize $s_1 + s_2$ subject to}\\
&x - 1.42 y \le 3.15 + s_1,\ x + y \le 6 + s_2,\ 
5 \le x \le 10, 0 \le y \le 10,\ 
0 \le s_1,\ 0 \le  s_2.
\end{split}
\end{equation}
If the objective value of this linear program is positive then the
original system is infeasible. We are interested in a dual solution of
this linear program. Dual variables correspond to constraints of the
primal linear program. We use an external linear programming tool for
finding dual solutions. Solutions returned by an external tool may be
imprecise. One possible  dual solution is the following: 
\[
(0.704, 1,
-1.705, 0, 0, 0, 0.296, 0).\]
The eight entries of this vector correspond to the eight constraints
in the problem~\ref{eqn:lp}.
Multiply all
constraints by nonzero coefficients from the dual solution with
slack variables set to zero:
\begin{equation*}
\begin{split}
0.704 (x - 1.42 y) &\le 0.704 ( 3.15),\\
x + y &\le 6,\\
-1.705 x &\le -1.705 (5).
\end{split}
\end{equation*}
The sum of these inequalities is
\begin{equation*}
-0.001 x + 0.00032 y \le -0.3074.
\end{equation*}
Examining this inequality, next we modify the coefficients of the dual
solution so that the left-hand side of the inequality is precisely
zero.  We get the following modified dual solution:
\[
 (0.704, 1, -1.705, 0.001, -0.00032, 0, 0.296, 0).
\]
With the modified
dual solution, the sum of the constraints, weighted by the
coefficients of the dual vector, is $0 \le -0.2974$. Clearly, it is a
contradiction, and hence our original system of inequalities is not
consistent.

If we know a modified dual solution, then the formal verification
reduces to the process of summation of inequalities with coefficients
taken from the modified dual solution and checking that the final
result is inconsistent. A modified dual solution can be found with
informal methods. We use GLPK for solving linear
programs~\cite{website:GLPK} and a special C\# program for finding
required dual solutions for linear programs. All dual solutions are
also converted to integer solutions by multiplying all coefficients
by a sufficiently large power of $10$. Hence, the formal verification
procedure works with formal integer arithmetic only. There are about
50,000 linear programs (after considering all possible cases). All
these linear programs can be verified in about $15$ hours on a 2.4GHz
computer. The verification time does not include time for generating
required dual solutions. These solutions need only be computed once,
and results are saved in files which are then loaded by the formal
verification procedure.


\section{Auditing a distributed formal proof}

A proof assistant largely cuts the mathematical referees out of the
verification process.  This is not to say that human oversight is no
longer needed.  Rather, the nature of the oversight is such that
specialized mathematical expertise is only needed for a small
part of the process.  The rest of the audit of a formal proof can be
performed by any trained user of the HOL Light and Isabelle proof
assistants.

The article \cite{XX-Adams} describes the steps involved in the
auditing of the formal proof.  The proofs scripts must be executed to
see that they produce the claimed theorem as output.  The definitions
must be examined to see that the meaning of the final theorem (the
Kepler conjecture) agrees with the common understanding of the
theorem.  In other words, did the right theorem get formalized?  Were
any unapproved axioms added to the system?  The formal proof system
itself should be audited to make sure there is no foul play in the
syntax, visual display, and underlying internals.  A fradulent user of
a proof assistant might ``exploit a flaw to get the project completed
on time or on budget.  In their review, the auditor must assume
malicious intent, rather than use arguments about the improbability of
innocent error.''

This particular formal proof has several special features that call for 
careful auditing.  The most serious issue is that the full formal
proof was not obtained in a single session of HOL Light.  An audit
should check that the statement of the tame classification theorem in
Isabelle has been faithfully translated into HOL Light.  (It seems to
us that our greatest vulnerability to error lies in the hand
translation of this statement from Isabelle to HOL Light.)  In
particular, an audit should check that the long list of tame graphs
that is used in Isabelle is identical to the list that is used in HOL
Light.  (Both systems generated their list from the same master file.)

%The nonlinear inequalities were also combined from various sessions of
%HOL Light.  A specially modified version of HOL Light combines these
%inequalities into a single theorem.  
The auditor should also check the
design of the special modification of HOL Light that was used to combine
nonlinear inequalities into a single session.



\section{Related work and acknowledgements}

Numerous other research projects in the formal proofs have made use of
the Flyspeck project in some way or have been inspired by the needs of
Flyspeck.  These projects include the automated translation of proofs
between formal proof systems [Obua-S], [McLaughlin], [Adams]; the
refactoring of formal proofs [Adams]; machine learning applied to
proofs and proof automation [Urban-Kaliszyk]; a mechanism to execute
trusted external arithmetic from Isabelle/HOL [Obua]; the visual
presentation of proofs [X]; the verification of linear programs [X];
and the verification of nonlinear inequalities [X].


\section{Acknowledgements}

The various roles of project members appears in the statement
announcing the completion of the project.

We wish to acknowledge the help, support, influence, and various
contributions of the following individuals:

%Dang Tat Dat, 
%Hoang Le Truong,
Nguyen Duc Thinh,  
Nguyen Duc Tam, 
%Nguyen Tat Thang,
%Nguyen Quang Truong, 
%Ta Thi Hoai An, 
%Tran Nam Trung, 
%Trieu Thi Diep, 
%Vu Khac Ky, 
Vu Quang Thanh,
%Vuong Anh Quyen,
% 
%Mark Adams,
Catalin Anghel, 
Jeremy Avigad, 
Henk Barendregt,
%Gertrud Bauer, 
%
Herman Geuvers,
Georges Gonthier,
Daron Green,
Mary Johnston,
%Thomas Hales,
%John Harrison, 
%Cezary Kaliszyk,
%Victor Magron,
Christian Marchal,
Laurel %Beth 
Martin, 
%Sean McLaughlin, 
%Tobias Nipkow, 
%Steven Obua, 
%Joe Pleso, 
%
%Jason Rute,
Robert Solovay,
%Alexey Solovyev,
Erin Susick,
Dan Synek,
%Josef Urban,
Nicholas Volker, 
Matthew Wampler-Doty, 
Benjamin Werner,
Freek Wiedijk, 
Carl Witty, and
Wenming Ye.
%and Roland Zumkeller.

We wish to thank the following sources of institutional support: NSF
grant 0503447 on the "Formal Foundations of Discrete Geometry" and NSF
grant 0804189 on the "Formal Proof of the Kepler Conjecture",
Microsoft Azure Research, William Benter Foundation, University of
Pittsburgh, Radboud Research Facilities, Institute of Math (VAST), and
VIASM.


\newpage
\section{Appendix on definitions}

The following theorem provides evidence that key definitions in the
statement of the Kepler conjecture are the expected ones.

\begin{obeylines}

\begin{verbatim}
|-  
// real absolute value:
   (&0 <= x ==> abs x = x) /\ (x < &0 ==> abs x = -- x) /\   

// powers:
    x pow 0 = &1 /\ x pow SUC n = x * x pow n /\

// square root:
   (&0 <= x ==> &0 <= sqrt x /\ sqrt x pow 2 = x) /\ 

// finite sums:
   sum (0..0) f = f 0 /\ sum (0..SUC n) f =  
     sum (0..n) f + f (SUC n) /\ 

// pi:
   abs (pi / &4 - sum (0..n) (\i. (-- &1) pow i / &(2 * i + 1))) 
     <= &1 / &(2 * n + 3) /\

// finite sets and their cardinalities:
   (A HAS_SIZE n <=> FINITE A /\ CARD A = n) /\
   {} HAS_SIZE 0 /\ {a} HAS_SIZE 1 /\ 
   (A HAS_SIZE m /\ B HAS_SIZE n /\ (A INTER B) HAS_SIZE p 
     ==> (A UNION B) HAS_SIZE (m+n - p)) /\

// bijection between R^3 and ordered triples of reals:
   triple_of_real3 o r3 = (\w:real#real#real. w) /\
   r3 o triple_of_real3 = (\v:real^3. v) /\ 

// the origin:
   vec 0 = r3(&0,&0,&0) /\

// the metric on R^3:
   dist(r3(x,y,z),r3(x',y',z')) = 
     sqrt((x - x') pow 2 + (y - y') pow 2 + (z - z') pow 2) /\

// a packing:
   (packing V <=> 
     (!u v. u IN V /\ v IN V /\ dist(u,v) < &2 ==> u = v))
\end{verbatim}

\end{obeylines}



References.

@incollection{Solovyev:NFM2013,
  author={Solovyev, Alexey and Hales, Thomas C.},
  title={{Formal Verification of Nonlinear Inequalities with Taylor Interval 	Approximations}},
  year={2013},
  booktitle={NFM},
  volume={7871},
  series={LNCS},
  pages={383-397}
}

@article{Tange2011a,
  title = {GNU Parallel - The Command-Line Power Tool},
  author = {O. Tange},
  address = {Frederiksberg, Denmark},
  journal = {;login: The USENIX Magazine},
  month = {Feb},
  number = {1},
  volume = {36},
  url = {http://www.gnu.org/s/parallel},
  year = {2011},
  pages = {42-47}
}