%\chapter{Essays}

This is a warm-up chapter that covers key ideas of the sphere packing problem.
The chapter is called ``Essays'' because it contains background material
that I would want to give an audience an firm intuitive understanding of what is really going on in the proof.   How might we think about the sphere packing problem?  How might we think about the computations that enter into the solution?


\section{Face Centered Cubic}

% all sorts of packing and tiling problems.
% Hilbert 18th.  Error-correcting codes.
% conjectures.


The following facts about packings are well-known.  However, there
is a popular and persistent misconception in the popular press
that the face-centered cubic packing is the only packing with
density $\pi/\sqrt{18}$. The comments that follow correct that
misconception.

In the face-centered cubic packing, each ball is tangent to twelve
others.  For each ball in the packing, this arrangement of twelve
tangent balls is the same.  We call it the fcc pattern. In the
hexagonal-close packing, each ball is tangent to twelve others.
For each ball in the packing, the arrangement of twelve tangent
balls is again the same.  We call it the hcp pattern.  The fcc
pattern is different from the hcp pattern.  In the fcc pattern,
there are four different planes through the center of the central
ball that contain the centers of six other balls at the vertices
of a regular hexagon.  In the hcp pattern, there is only one such
plane.  We call the arrangement of balls tangent to a given ball
the {\it local tangent arrangement} of the ball.

There are uncountably many packings of density $\pi/\sqrt{18}$
that have the property that every ball is tangent to twelve others
and such that the tangent arrangement around each ball is either
the fcc pattern or the hcp pattern.

By {\it hexagonal layer}, we mean a translate of the two-dimensional
lattice of points $M$ in the $A_2$ arrangement. That is, $M$ is a
translate of the planar lattice generated by two vectors of length
$2$ and angle $2\pi/3$.  The face-centered cubic packing is an
example of a packing built from hexagonal layers.

If $M$ is a hexagonal layer, a second hexagonal layer $M'$ can be
placed parallel to the first so that each lattice point of $M'$ has
distance $2$ from three different vertices of $M$.  When the second
layer is placed in the manner, it is as close to the first layer as
possible. Fix $M$ and a unit normal to the plane of $M$. The normal
allows us to speak of the second layer $M'$ as being ``above'' or
``below'' the layer $M$. There are two different positions in which
$M'$ can be placed closely above $M$ and two different positions in
which $M'$ can be placed closely below $M$. As we build a packing,
layer by layer, ($M$, $M'$, $M''$, and so forth), there are two
choices at each stage of the close placement of the layer above the
previous layer. Running through different sequences of choices gives
uncountably many packings.  In each of these packings the tangent
arrangement around each ball is that of the twelve spheres in the
face-centered cubic or the twelve spheres in the hexagonal-close
packing.

Let $\Lambda$ be a packing built as a sequence of close-packed
hexagonal layers in this fashion.  If $P$ is any plane parallel to
the hexagonal layers, then there are at most three different
orthogonal projections of the layers $M$ to $P$.  Call these
projections $A$, $B$, $C$.  Each hexagonal layer has a different
projection than the layers immediately above and below it.  In the
fcc packing, the successive layers are $A,B,C,A,B,C,\ldots$.  In
the hcp packing, the successive layers are $A,B,A,B,\ldots$.  If
we represent $A$, $B$, and $C$ as the vertices of a triangle, then
the succession of hexagonal layers can be described by a walk
along the vertices of the triangle. Different walks through the
triangle describe different packings.

% In the face-centered
%cubic packing, every local tangent arrangement of $12$ tangent
%balls is the same.  We call this local tangent arrangement the
%fcc-pattern. Similarly, the local tangent arrangement of the
%hexagonal-close packing will be called the hcp-pattern.  These
%patterns are uniquely determined up to rigid motion.

In fact, the different walks through a triangle give all packings of
infinitely many equal balls in which the tangent arrangement around
every ball is either the fcc pattern of twelve balls or the hcp
pattern of twelve balls.

\bigskip


{

\narrower

\font\ninerm=cmr9 \ninerm

%\def\=#1{\accent"16 #1}


We justify the fact that different walks through a triangle give all
such packings. Assume first that a packing $\Lambda$ contains a ball
(centered at $v_0$) in the hcp pattern. The hcp pattern contains a
uniquely determined plane of symmetry. This plane contains $v_0$ and
the centers of six others arranged in a regular hexagonal. If $v$ is
the center of one of the six others in the plane of symmetry, its
local tangent arrangement of twelve balls must include $v_0$ and an
additional four of the twelve balls around $v_0$. These five centers
around $v$ are not a subset of the fcc pattern. They can be uniquely
extended to twelve centers arranged in the hcp pattern. This hcp
pattern has the same plane of symmetry as the hcp pattern around
$v_0$. In this way, as soon as there is a single center with the hcp
pattern, the pattern propagates along the plane of symmetry to
create a hexagonal layer $M$.

Once a packing $\Lambda$ contains a single hexagonal layer, the
condition that each ball be tangent to twelve others forces a
hexagonal layer $M'$ above $M$ and another hexagonal layer below
$M$.  Thus, a single hexagonal layer forces a sequence of
close-packed hexagonal layers in both directions.

We have justified the claim under the hypothesis that $\Lambda$
contains at least one ball with the hcp pattern.

Assume that $\Lambda$ does not contain any balls whose local
tangent arrangement is the hcp pattern.  Then every local tangent
arrangement is the fcc pattern, and $\Lambda$ itself is then the
face-centered cubic packing.  This completes the proof.


}

\clearpage
\section{Thirteen Spheres}


The Newton-Gregory problem  was mentioned in the chapter on
history.  Is it possible to place thirteen congruent balls so that they
all touch a fixed (fourteenth) ball congruent to the others.
Gregory suspected so, Newton suspected not.  Ultimately Newton turnout
out to be correct.

This essay presents a proof of the problem that is based on a Leech's proof
of 1956.  For decades Leech's short proof stood as a model of elegance and clarity.
However,  when Leech's proof was abruptly dropped from the second
edition of {\it Proofs from the Book}, people started to ask questions \cite{AZ98}.  
B. Casselman, in a note on
the history of this problem, describes Leech's proof as ``cryptic'' and adds 
that ``although his reasoning has been accepted as correct, there are gaps in his
exposition $\ldots$  It would be valuable if someone were to publish an account
of Leech's proof that made it accessible to an elementary undergraduate course'' \cite{BC04}.
Maehara takes up the challenge in his proof for undergraduates \cite{Mae07}.

The general outline of Leech's proof is similar to the general outline of the solution
of the sphere packing problem.  By studying Leech's proof, we can see in a few pages
the structure of this entire book.  For that reason, the ideas in this small essay
are particularly relevant. 

\begin{theorem}  It is impossible to arrange $13$ points on a unit
sphere in such a way that the arc length between each pair is at
least $\pi/3$.
\end{theorem}

We explain the strategy of the proof.  Let $V$ be the maximum
number such that it is possible to arrange $V$ points on the unit
sphere $S^2$ with separation at least $\pi/3$.  Let $C\subset S^2$ be
such an arrangement. Take the Delaunay triangulation (explained below) 
of the sphere with vertices in $C$. Since we chose, $V$ to be as large as
possible, each triangle has circumradius at most $\pi/3$.  By the
Euler formula relating the number of vertices, edges and faces,
we have
    $$V - E + F = 2,\quad 3 F = 2 E,\quad \Rightarrow F =
    (2V-4).$$
Assuming $V\ge 13$, we have $F\ge 22$. 
We will reach a contradiction by showing that
the total area of the triangles is greater than $4\pi$, thus
showing that they cannot triangulate a sphere of area $4\pi$. In
the proof all lengths are arc lengths on the sphere and all
triangles are composed of geodesic arcs.

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/thirteenA.eps}
  \caption{As we move $A$ away from a critical
point, keeping the distance to $C$ fixed, we move to Lexell
circles of smaller area.}
  \label{fig:13:A}
\end{figure}
%\figsz{SCAN/thirteenA.eps}{}{fig:13:A}{0.7}


%% XX Lexell circle.

\begin{lemma} Fix the length of two sides of a spherical triangle.
The area of the triangle, viewed as a function of the length of
the third side, does not have a local minimum.
\end{lemma}

\begin{proof}  Fix both endpoints $BC$ of one of sides of the triangle.
Assume that the length of $AB$ is also fixed at $c$, which $BC$
varies. A critical point of the area function can be viewed
geometrically as a point of tangency between the Lexell circle 
and the circle $S$ of radius $c$ centered at $B$.
We see geometrically that any such point of tangency is a local
maximum for the area rather than a local minimum, because as we
move away from the point of tangency along $S$ we sweep into
Lexell circles representing smaller areas.
\end{proof}

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/thirteenB.eps}
  \caption{As we move $A$ away from a critical
point, keeping the circumradius of $ABC$ fixed, we move to Lexell
circles of smaller area.}
  \label{fig:13:B}
\end{figure}
%\figsz{SCAN/thirteenB.eps}{}{fig:13:B}{0.7}

\begin{lemma} Fix two vertices of a spherical triangle
and its circumscribing circle.  The area of the triangle, viewed
as a function of the position of the third vertex on the
circumscribing circle, does not have a local minimum.
\end{lemma}

\begin{proof}  Again, a critical point is represented
geometrically as a point of tangency between the Lexell circle and
the circumscribing circle.  As in the previous lemma, this is a
local maximum.
\end{proof}

It is then an easy task to find a lower area bound on collections
of triangles subject to edge length constraints or to circumradius
constraints, because there are no interior point minima.  It is
enough to check all endpoints to find the minimum.  We summarize
our findings in a table listing upper and lower bounds on each
side and an upper bound $M$ on the circumradius.  A dash indicates
no constraint.  Let
    $$
    b = \pi/3, \quad c = 1.42, \quad d = 1.7,\quad
    \Delta = 3\arccos(1/3)-\pi,\quad \epsilon = 0.4381.
    $$
We then have
$$
    \begin{array}{lllllllll}
 &\min_1&\max_1&\min_2&\max_2&\min_3&\max_3&M    &\min_{\op{area}}\\
 1:&c &- &c &- &c &- &b &\Delta+\epsilon\\
 2:&b &c &c &- &d &- &b &\Delta+\epsilon\\
 3:&b &c &c &d &c &d &- &\Delta+\epsilon/2\\
 4:&b &c &b &c &c &d &- &\Delta+\epsilon/4\\
 5:&b &c &b &c &d &- &- &-\\
 6:&b &c &b &c &b &c &- &\Delta
    \end{array}
$$
For example, the first row of the table 
asserts that any spherical triangle whose three sides are at least
$c$ and whose circumradius is at most $b$ has area at least $\Delta+\epsilon$.

We draw out some elementary consequences of this table.

\begin{itemize}
  \item $V=13$ and $F=(2V-4)=22$.  In fact, if $V\ge14$ then $F= (2V-4)\ge 24$ and
  the total area of the triangles is at least $24\Delta > 4\pi$.
  \item The total area of the triangles is $4\pi$, which is less than
  $22\Delta+\epsilon$.  Thus, the excesses of the areas over
  $\Delta$ in the table must sum to a constant strictly less than
  $\epsilon$.  Thus, triangles in the first two rows of the table
  never occur.  In particular, a triangle with two edges longer
  than $c$ has edges shorter than $d$.
  \item A triangle in the third row never appears, because it
  would be flanked on both sides by triangles from the first four rows,
  giving excesses $\epsilon/2 + \epsilon/4 + \epsilon/4$ of at
  least $\epsilon$. In particular, a triangle never has two edges
  of length at least $c$.
  \item Triangles with one edge longer than $c$ appear in pairs
  along the longer edge, forming quadrilaterals.  Changing the
  triangulation by switching to the opposite diagonal if
  necessary, we may assume that the shorter diagonal of the
  quadrilateral is an edge of the triangulation.  (We no longer
  have a circumradius bound, because the resulting triangles might not be Delaunay
  triangles.)
  \item The table omits a bound for triangles in the fifth row,
  corresponding to quadrilaterals with a diagonal (in fact both diagonals) of length at
  least $d$. By the spherical law of cosines (Lemma~\ref{lemma:sloc}), the angles of the quadrilateral are
  at least $\pi/2$.  Thus, we may deform the quadrilateral, moving
  vertices internally to shrink the area, until a diagonal has length $d$, at which
  point we use the fourth row to get the bound.  We cannot deform all the way
  to a
  rhombus of side $\pi/3$, because such a rhombus has a diagonal
  of length at most $d$.
  \item There cannot be two edges longer than $c$.  Otherwise we
  get two quadrilaterals, each with area at least $2\Delta +
  \epsilon/2$, for total triangulation area of at least $22\Delta
  + \epsilon$.
  \item In summary, the triangulation has $13$ vertices, $22$
  faces, and at most one edge longer than $c$.  (Call this the long edge.)
  If there is an
  edge of length greater than $c$, the opposite diagonal of the
  corresponding quadrilateral also has length at least $c$.
\end{itemize}



The angles of every triangle, except those along a long edge, are
greater than $\pi/6$. (Again the angle function has no local
minimum so this fact is easy to check by using the spherical law of cosines (Lemma~\ref{lemma:sloc}).) So
the degrees at such vertices are at most $5$.  The degrees of the
vertices meeting a long edge are at most $6$.



There must be a vertex of degree six.  In particular, a long edge
exists.  In fact, we can count the number of oriented edges in the
triangulation two ways.  The oriented edges can be arranged three
to each triangle, or they can be arranged by originating vertex.
Assuming no degree six vertex, we get the contradiction.
    $$66 = 3 F \le 5 V = 65.$$

Let us call a triangle {\it oblong}, if it has a long edge.

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/thirteenC.eps}
  \caption{Vertices of degree four have total
excess of at least $\epsilon/2$.}
  \label{fig:13:C}
\end{figure}
%\figsz{SCAN/thirteenC.eps}{fig:13:C}{1.0}

\begin{lemma}  There is no vertex of degree four, except possibly
when one of the triangles is oblong.
\end{lemma}

\begin{proof}  We calculate an upper bound on the angle $\gamma_{\max}$, and a
lower bound on the area $D_{\min}$, given bounds $[b,c]$ on the
first edge, $[b,c]$ on the second edge, and $[\min_3,\max_3]$ on
the edge opposite the angle $\gamma$.  The upper bound on the
angle $\gamma_{\max}$ occurs when the opposite side is as long as
possible, and each adjacent side is extremal in length or
terminates at a right angle.  We calculate each possibility and
take the maximum.  Let $b'=1.15$ and $b''=1.251$.

$$
    \begin{array}{llllll}
      &\min_3&\max_3   &\gamma_{\max} - \pi/2&   D_{\min}\\
    1:&b     &b'       &-0.21              &\Delta\\
    2:&b'    &b''      &-0.08              &\Delta + \epsilon/12\\
    3:&b''   &c        &0.14               &\Delta + \epsilon/6\\
    \end{array}
$$
We make the following observations about the table.  We can
discard any combination of four rows that gives total angle less
than $2\pi$  because the four angles around a a degree four vertex
sum to $2\pi$.  We can discard any combination of four rows that
gives excess area at least $\epsilon/2$, because the two oblong
triangles also have combined excesses $\epsilon/2$, for a total of
at least $\epsilon$.
\begin{itemize}
    \item There cannot be three or more triangles from the third row,
    because this would give excess area $\epsilon/2$.
    \item  There cannot be fewer than two triangles from the third
    row, because the angle sum would be less than $2\pi$. So there
    are exactly two triangles from the third row.
    \item There cannot be two triangles from the second row,
    because combined with the two in the third row, the
    excess too great.
    \item If there are two from the third row, and at most one
    from the second, then the angle sum is less than $2\pi$.  So
    degree four vertices cannot exist (if none of the triangles at
    that vertex has a long edge).
\end{itemize}
\end{proof}

%%  We could have done this as a linear program.
%%

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/thirteenD.eps}
  \caption{A quadrilateral of degrees $4665$ can
be transformed to degrees $6555$ by swapping
diagonals.}
  \label{fig:13:D}
\end{figure}
%\figsz{SCAN/thirteenD.eps}{}{fig:13:D}{0.7}

The final bits of the proof are purely combinatorial. If there is
one degree six vertex, then counting oriented edges, we get
    $$66 = 3 F \le 6 + 5(V-1) = 66,$$
so equality holds and every other vertex has degree $5$.  If there
are two degree six vertices (both endpoints of the long edge),
then the number of oriented edges is
    $$
    66 = 3 F = 6 (2) + 5 (V-3) + 4 (1) = 66,
    $$
so there must also be a degree $4$.  By the lemma, this vertex of
degree four lies on the quadrilateral formed by the two long-edged
triangles.  Changing the diagonal of this quadrilateral, we get a
triangulation with one vertex of degree six and all other vertices
of degree five.

Remove the vertex of degree six and all its edges from the graph.
We are left with a planar graph with twelve vertices giving the
triangulation of a hexagon.  The six vertices of the hexagon have
degree four and the six internal vertices have degree five.  Such
a triangulation does not exist by Lemma~\ref{lemma:trihex}.
%hexagon}.

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/thirteenE.eps}
  \caption{Removing a vertex of degree $6$ leaves a
triangulation of a hexagon (with $6$ interior vertices of degree
$5$).}
  \label{fig:13:E}
\end{figure}
%\fig{SCAN/thirteenE.eps}{}{fig:13:E}

\subsection{Triangulating the Hexagon}

\begin{lemma}\label{lemma:trihex}  
There does not exist a topological triangulation of the hexagon such that
    \begin{itemize}
     \item   The six vertices on the hexagon have degree four,
     \item   There are six internal vertices, each of degree five.
    \end{itemize}
\end{lemma}

By a topological triangulation, we mean that the sides of the
triangles may be arbitrary simple paths in the plane; they do not
need to be straight line segments.

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/trihexA.eps}
  \caption{Mapping a triangulation of the hexagon to
the icosahedron}
  \label{fig:th:A}
\end{figure}
%\fig{SCAN/trihexA.eps}}{fig:th:A}

\begin{proof}  Assume for a contradiction, that it exists.  Map
the triangulated hexagon to the icosahedron as follows.  Fix an
initial triangle $T$ in the hexagon.  Map it to any triangle $T'$
on the icosahedron.  For any other triangle $S$, pick a path of
triangles from $T$ to $S$.  Follow the corresponding path on the
icosahedron.  Map $S$ to the final triangle $S'$ along the path on
the icosahedron.

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/trihexB.eps}
  \caption{The image of $S$ does not depend on the
path from $T$ to $S$.}
  \label{fig:th:B}
\end{figure}
%\fig{SCAN/trihexB.eps}{.}{fig:th:B}

The triangle $S'$ does not depend on the chosen path from $T$ to
$S$.  In fact any path from $T$ to $S$ can be deformed to any
other path, by a succession of two elementary changes. The first
elementary change replaces the path $\ldots ABA\ldots $ with
$\ldots A\ldots$, or vice versa.  Such a path contraction does not
affect the terminal triangle $S'$ on the path.  The second
elementary change replaces a loop around a vertex (of degree five)
$ABCDEA$ with $A$, or vice versa.  This too, does not alter the
terminal triangle.  Since we have path independence, if $T=S$,
then $T'=S'$.

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/trihexC.eps}
  \caption{On path can be transformed to another by
passing through edges and vertices.}
  \label{fig:th:C}
\end{figure}
%\fig{SCAN/trihexC.eps}{}{fig:th:C}

Now consider the path $T=T_1,\ldots,T_{13}=S$ which runs around
the edge of the hexagon.  It is mapped to a path on the
icosahedron $T'=T'_1,\ldots,T'_{13}=S$.  Note that $T=S$ but
$T'\ne S'$, a contradiction.
\end{proof}

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/trihexD.eps}
  \caption{The triangle $T_1=T_{13}$ must map to both $T'_1$ and $T'_{13}$.}
  \label{fig:th:D}
\end{figure}
%\fig{SCAN/trihexD.eps}{}{fig:th:D}


\subsection{discussion}

\FIXX{Discuss Sch.vdWaeren, differences with Leech, relation to this book}


There were some proofs offered in the nineteenth century that were debunked in \cite{Hal94}.

A number of more recent proofs have appeared.  For example, O. Musin solves it
with Delsarte's method (Legendre polynomials) \cite{Mus06}.  C. Bachoc and F. Vallentin use semidefinite
programming to give a solution \cite{BV06}.  Other proofs appear in 
\cite{Mae01}, \cite{Ans02},  \cite{Bor03}, \cite{Mae07}.   K. B\"or\"oczky and L. Szab\'o have calculated
bounds on how much larger the central sphere must be than the others, before a thirteen sphere
arrangement results \cite{BoSz03}.  
% The I have not read the argument in \cite{Hsi02}, because
%his treatment of thirteen spheres in earlier papers lacked care.

\clearpage
\section{Interval Analysis}%DCG 8.3, p75
\label{sec:bounds-simplex}

Interval analysis is a method to obtain trustworthy results from a computer.
This essay gives a basic introduction to this method.

\subsection{deliberate error}

Interval arithmetic can trace its origins to the method of deliberate error.
This is an ancient method of navigation with imperfect instruments.  When William
the Conqueror
crossed the English Channel in 1066, he deliberately steered to the north of Hastings.

\begin{quote}
%``There is no direct evidence of how a twelfth-century pilot found his way across
%the English Channel $\ldots$
``Any pilot even now who had to make that crossing without a chart or compass,
as William's pilots did, would use the ancient method of Deliberate Error: he would
not steer directly towards his objective but to one side of it, so that when he
saw the coast he would know which way to turn.'' \cite[p81]{How81}
% 1066: The Year of the Conquest, Penguin paperback edition 1981. page 148.
\end{quote}

Deliberate error implements ``better safe than sorry.''
To find an address on a one-way street,  a prudent driver enters the
street at an intersection that comes well before the actual address.  
By entering the one-way street too early,
the driver points in the right direction.  When searching for a familiar landmark
on a two-way street, it is more efficient  
to start the search safely to one side of the landmark, 
rather than search in ever
expanding zigzags in both directions. 


The method of deliberate error cushions the adverse effects of imperfect technology.
The method does not aim to minimize the imperfections.  It works with a faulty
chart and compass.
%The method of deliberate error seeks not to minimize the error of imperfect technolo

\subsection{arithmetic}

The method of deliberate error, implemented to control for round-off errors on a computer,
leads to interval arithmetic.  

Fixed precision 
floating point numbers on a computer exactly represent a finite
number of rational numbers.  The remaining uncountable set of real numbers cannot
be precisely represented.  For example, 64-bit floating point numbers can encode at most
$2^{64}$ different real numbers.    This finite set of representable numbers is not
closed under addition, multiplication, subtraction, or division.  For example,
the input $2.0 + 2.0$ returns $4.0$, because all the numbers involved
are precisely representable.  However, through round-off error, my computer returns 
\begin{verbatim}0.0
\end{verbatim} 
in response to
\begin{verbatim}
  -1.0 + (1.0 + 1.0e-16)
\end{verbatim}
and 
\begin{verbatim}1.0e-16
\end{verbatim} 
in response to 
\begin{verbatim}
  (-1.0 + 1.0) + 1.0e-16.
\end{verbatim}
As this example shows, machine addition is not even associative.  We would quickly be led to absurd
conclusions, if we were to try to reason about floating point operations as if
they formed a group, a ring, or a field.  To reason correctly about floating point, we
must accept these operations for what they are.  To distinguish the imprecise
machine (floating point) operations from standard operations on the field of real numbers, in the rest of this section (except in computer code listings), 
we place a dot over the floating point operations $(\dot +)$, $(\dot -)$, and so forth.

The precise behavior of floating point operations on a computer is governed by 
the IEEE-754 floating point standard \cite{Gol}.  
By giving a precise specification of properties
of floating point arithmetic, the standard makes it possible to reason about the
behavior of floating point.  It is possible to prove theorems about the behavior
of IEEE floating point.  For example, it is possible to prove that a computer that
correctly implements the standard should return the values in reponse to the inputs
given above (in the nearest rounding mode).  

Let $F$ be the set of machine-representable floating point numbers.  We may assume that $F$ contains two 
 special symbols $\pm\infty$.  The total order on the real numbers is extended to $\ring{R}\cup\{\pm\infty\}$, with $-\infty < x < \infty$ for all $x\in\ring{R}$.
Because of these two special symbols, the floating point floor $x\to\floor{x}_F\in F$ and ceiling $x\mapsto\ceil{x}_F\in F$ functions, with domain $\ring{R}$ can be defined.   In this section, we drop the subscript $F$ on the
floor and ceiling functions.
We map the set of real numbers to $F^2$, by
sending $x$ to $[a,b]$, where $a = \floor{x}$ and $b=\ceil{x}$.
If $x$ is Hastings, then $b$ is a point
on the shore north of Hastings.  It is the deliberate error to one side of the target.


On most modern processors, the rounding mode can be set to directed rounding, as
described in the standard.   When the rounding mode is set upward, the result of
any basic arithmetic floating-point operation $(\dot +)$, $(\dot -)$, $(\dot *)$, $(\dot /)$ applied to two floating
point numbers $(x,y)\mapsto x\dot\diamond y$ is defined by standard to be 
the $\ceil{x\diamond y}$.  That is, make the calculation in the field of real numbers and round up to the next floating point.
When the rounding mode is set downward, 
set $x\dot\diamond y = \floor{x\diamond y}$.  The notation $\dot\diamond$ does not
show rounding mode, but it should, because it is mode dependent.


Interval arithmetic, like the method of deliberate error, does not seek to eliminate
the sources of floating point round off error.  Rather it brings it under scientific
control.  


Let $I_F = \{[a,b] \in F^2 \mid a \le b\}$ be the set of floating point intervals.
Basic machine operations extend to intervals.  The sum $[a_3,b_3]$ of
$[a_1,b_1]$ and $[a_2,b_2]$, is defined as $a_3=\floor{a_1+a_2}$ and
$b_3 = \ceil{b_1+b_2}$.  Write $[a_3,b_3] = [a_1,b_1] \dot+ [a_2,b_2]$.
This addition of intervals
is not associative.  However, addition of intervals satisfy a crucial inclusion property.
If $x\in[a_1,b_1]$ and $y\in [a_2,b_2]$, and $z = x+y$,
then $z\in [a_3,b_3]$.  The other arithmetic operations can be extended to machine
interval operations in a similar way, so as to satisfy the corresponding inclusion
properties.  Division requires special treatment when $0$ belongs to an interval
denominator.  We will not go into those details here.  

These operations are easily implemented in code.  For example, here is the actual snippet of 
{\tt C++} code that implements the addition of intervals.  The functions
{\tt interMath::up()} and {\tt interMath::down()} set the rounding modes on the 
computer.  
\begin{verbatim}
inline interval interval::operator+(interval t2) const
        {
        interval t;
        interMath::up(); t.hi = hi+t2.hi;
        interMath::down(); t.lo = lo+ t2.lo;
        return t;
        }
\end{verbatim}

By implementing interval operations on a computer, we can develop a procedure that
takes as input 
an arbitrary arithmetic expression over the rational numbers and returns an
interval $[a,b]$ with floating point endpoints (augmented by $\pm\infty$ as usual)
that contains the rational value of that expression.
Since the associative and distributive laws fail, the returned interval
 $[a,b]$ depends on the actual syntax of the expresssion, on  the placement of parentheses,
and so forth.



\subsection{analysis}

This essay is not a treatise on interval arithmetic.  Its purpose
is to give an introduction, to demonstrate the possibility of rigorously
bounding the error of floating point performed according to IEEE standards.
We become increasingly sketchy.

The theory of interval analysis progresses step by step, starting with
basic arithmetic and developing through higher levels of analysis.
If $p$ is a polynomial with rational coefficents $p\in\ring{Q}[x_1,\ldots,x_n]$, then it has an interval
extension $\bar p : I_F^n \to I_F$.  Just as in the case of rational
numbers, the interval extension $\bar p$ depends on the syntax used to
expressed to polynomial $p$.    The polynomial extension is {\it monotonic}:
  if $z_i\in [a_i,b_i]$ for all $i$, then
$p(z_1,\ldots,z_n) \in \bar p([a_1,b_1],\ldots,[a_n,b_n])$.
Interval extensions of rational functions is similar.

Consider a function $f:[a,b]\to\ring{R}$ that
has a rational function approximation $r$ with known
error bound:  $|f(x) - r(x)|<\epsilon$ for $x\in[a_1,b_1]$,
with $\pm\epsilon\in F$.
Define a monotonic interval extension $\bar f$ of $f$ by
$\bar f([c,d]) = \bar r([c,d]) \dot + [-\epsilon,\epsilon]$, for
$[c,d]\subset [a,b]$.  
Multivariate functions are similar.   
%
A large class of analytic functions fall within this framework.
We have interval extensions of trigonometric functions, logs, and
exponentials.  Interval extensions of real-valued functions can be
added, multiplied and composed.  

To prove that a function $f$ is positive on a rectangular domain
$[a_1,b_1]\cdots[a_n,b_n]$, compute the
value of an interval extension $[c,d]=\bar f([a_1,b_1]\cdots[a_n,b_n])$.
By the monotonic property of interval extensions, if $c>0$, then $f$
is positive on the domain.  If this is done naively, with the
first interval extension $\bar f$ that comes to mind, the image interval
$[c,d]$ may be so large that no worthwhile information results.
There is a large mathematical literature describing various 
efficient algorithms
to compute image intervals $[c,d]$ with accuracy.
Kearfott's book is a recommended starting point, because it comes
close to describing the types of algorithms implemented for the
solution to the sphere packing problem \cite{Kea96}. 



\subsection{archive}

Although this book is long,  it represents only a fraction of the solution of
the sphere packing problem.  The other resources that are needed to understand
the full solution are available on the internet.

There is an archive of  several hundred
inequalities that have been proved by computer.  The list of inequalities can
be found at \cite{web}.\footnote{The exact reference is the file
http://flyspeck.googlecode.com/svn/trunk/inequalities/kep\_inequalities.ml.
There are now multiple copies of this file on the internet.  This particular
URL is preferred because it is under active version control.  Other copies are now
out of date.}    
The list of inequalities are in computer readable
form in the rigorous mathematical syntax of HOL-Light (for Higher Order Logic).

For example, one of the inequalities from that file reads as follows.
\begin{verbatim}
let I_572068135=
   all_forall `ineq 
    [((square (#2.3)), x1, (#6.3001));
     ((#4.0), x2, (#6.3001));
     ((#4.0), x3, (#6.3001));
     ((#4.0), x4, (#6.3001));
     ((#4.0), x5, (#6.3001));
     ((#4.0), x6, (#6.3001))
    ]
    ((((tau_sigma_x x1 x2 x3 x4 x5 x6) -.  ((#0.2529) *.  
         (dih_x x1 x2 x3 x4 x5 x6))) >. (--. (#0.3442))) \/ 
     ((dih_x x1 x2 x3 x4 x5 x6) <.  (#1.51)))`;;
\end{verbatim}
The first part of this snippet of code gives lower and upper bounds on each of the six variables $x_1,\ldots,x_6$.  The final part of this code gives an inequality (or rather a disjunction of two inequalities) of nonlinear real-valued functions that holds on the given domain.
The $\#$ symbol marks exact decimal constants.  The functions {\tt dih\_x}, {\tt tau\_sigma\_x} are defined rigorously in a separate file.  They correspond to the functions 
$\dih$ and $\tau$ in this book.  The symbols for arithmetic operations are followed by periods ($*.$ and so forth) to distinguish them from the corresponding operations on natural numbers.  

The inequality carries a nine-digit identifier {\tt 572068135}.    This number
is a tracking number that can be used in a search engine to locate everything known
about a given inequality.
For example,
if one googles this number, the search engine returns 
nine matches related to this inequality, including
a preprint on the arXiv, the website of the Annals of Mathematics, a Springer Link
to the relevant issue of the journal Discrete and Computational Geometry, a flyspeck
discussion group, the {\it C++} computer code proving the inequality, and the output
file from running that code.  A search on the pdf file of this book links this inequality to
Lemma~\ref{lemma:11.16}.  Every interval arithmetic calculation in this book carries
a nine-digit identifier, for easy tracking.

The interval arithmetic code that proves the inequalities is available on the arXiv,
at the permanent website of the Annals, and at my university website.  
Currently, the most convenient
way to access it perhaps is through the Google's Code Search, 
a custom search engine for computer code.
Go to http://code.google.com/p/flyspeck/ and click on the link that reads
{\it code for 1998 proof}.  

% mentioned already below.
%Sean McLaughlin has an ongoing project to give an independent verification of all of the code in the proof of the sphere packing problem.  The current version of that code is maintained at \cite{McL08}.
%{\tt http://code.google.com/p/kepler-code/}.

Further information about proving these inequalities by interval arithmetic can
be found in \cite{algorithm} and
\cite{part1}. 
\index{calc@\calc{123456789}}

\subsection{code verification}

S. Ferguson and I put considerable effort into developing trustworthy
code.  The two of us made entirely independent implementations of
the code. %automated inequality proving by interval arithmetic. 
(We shared algorithms
but not source code.)  This allowed us to check our answers against each
other to ensure mutual consistency, 
and to eliminate certain sources of bugs.  We also made independent
implementations in Mathematica and C of traditional floating point
versions of the functions used for the nonlinear
optimizations.  By requiring these different implementations to give
compatible answers, we eliminated further sources of bugs.

Each nonlinear inequality was also checked independently with the nonlinear
optimization package {\tt cfsqp}.  This is a collection of {\tt C} code
that searches for the minimum of a smooth function on a domain described
by a system of constraints.  As is the case with many nonlinear optimization
packages, there is no guarantee that the search will converge to
the true global minimum of the function.   By repeating the search
with a large collection of initial values for the search, it becomes
more probable that the true global minimum will be found.
In practice it works remarkably well.

This package was not used
in any proofs.  The numerical testing with {\tt cfsqp} was used to
discover false inequalities before they were shipped to the interval
arithmetic prover for verification.  Those that failed never shipped.
This extra level of testing adds an extra level of robustness to
this part of the proof.  Testing gives us (nonrigorous)
reasons to believe the inequalities, even if a bug should appear in
our interval arithmetic code.  This gives us some hope that an undetected bug
would be unlikely to affect the overall design of the solution to
the sphere packing problem.


There are certain types of bugs that would be very difficult to detect.
For example, since floating point arithmetic is not associative, a
misplaced pair of parentheses might throw a calculation off by a
machine epsilon.  
This potential source of bugs is evidence that the entire project 
was not sufficiently
automated:  the parentheses should all have been worked out automatically.
To make the calculations more robust, we have tried
to design the collection of inequalities so that they hold with a considerable
margin of error, rather than just squeeze by.  (There are only a few
inequalities that are sharp, and they are sharp for clear mathematical
reasons related to the theory of sphere packings.)  In a bug-free environment,
such precautions would not be necessary.  Nonetheless, we take precautions.

As far as I know, no comprehensive efforts were made by the referees and
editors to check the correctness of the computer code before the
publication of the 1998 solution to the sphere packing problem.  The editors' preface
to \cite{DCG} states that during the review process
``some computer experiments were done in a detailed check.''


The flyspeck project is a long-term project intended to make the solution
to the sphere packing problems one of the most thoroughly checked computer
proofs of all times.  Part of this project calls for a formal proof of correctness of the
computer code used in the interval verification of inequalities.

Flyspeck is still far from completion in 2008.  Nevertheless, 
there are various ongoing
projects
related to this second-generation verification of the interval code.
S. McLaughlin has an independent implementation of the interval arithmetic
code used in the sphere packing problem~\cite{McL08}.  
His work has exposed some data-entry bugs.  They are reported
in the comprehensive errata to the 1998 proof, 
which is maintained at \cite{errata} with additional discussion at~\cite{flydis}.  
No bugs have surfaced
over the past decade 
in the underlying interval arithmetic inequality proving algorithms.  The
reported errors have been at the data-entry level:  a mismatch between 
the data as typed
into the preprint and data in computer code.
\FIXX{All interval arithmetic should be rerun one final time before this book is published.}

One of the formal proof assistants under most active development
is the COQ system~\cite{COQ}.  R. Zumkeller
has implemented automated inequality proving with interval arithmetic
inside the theorem prover COQ, with the flyspeck project in
mind; although (as of 2008) the inequalities
that are used in this book have not yet been checked in this way \cite{Zu}.


\subsection{interval analysis and proof}


The editors of the Annals of Mathematics have posted a statement on computer-assisted proof.
At first, the editors planned to make a disclaimer directed at the
computer solution of the sphere packing problem.  Eventually,  they formulated a general policy
on computer-assisted proofs.  The policy mentions interval arithmetic as one way to
control sources of computer error.


\begin{quote}
%Statement by the Editors on Computer-Assisted Proofs

``Computer-assisted proofs of exceptionally important mathematical theorems will be considered by the Annals.

``The human part of the proof, which reduces the original mathematical problem to one tractable by the computer, will be refereed for correctness in the traditional manner. The computer part may not be checked line-by-line, but will be examined for the methods by which the authors have eliminated or minimized possible sources of error: (e.g., round-off error eliminated by interval artihmetic, programming error minimized by transparent surveyable code and consistency checks, computer error minimized by redundant calculations, etc. [Surveyable means that an interested person can readily check that the code is essentially operating as claimed]).

``We will print the human part of the paper in an issue of the Annals. The authors will provide the computer code, documentation necessary to understand it, and the computer output, all of which will be maintained on the Annals of Mathematics website online.'' \cite{Ann06}

%http://annals.princeton.edu/EditorsStatement.html
\end{quote}

A number of proofs in pure and applied mathematics have been based on interval analysis.  W. Tucker implemented a rigorous ODE solver with interval
arithmetic and used it to prove that the Lorenz equations
have a strange attractor \cite{Tuc02}. The existence of strange attractors is problem 14 on Smale's list
of 18 Centennial Problems \cite{Sma98}.  Another prominent problem solved
by interval methods is the double bubble conjecture, a generalization of
the isoperimetric problem in three dimensional Euclidean space.  A
sphere gives the solution to the classical isoperimetric problem.  The
work of J. Hass, M. Hutchings, and R. Schlafly shows that the surface
area minimizing way to enclose two regions of equal volume is the double
bubble, which consists of two partial spheres, separated by a flat 
circular disk \cite{HHS95}.






\subsection{historical note}

There are those who have tried to downplay the role of computers and interval
methods in the solution to the sphere packing problem.  In fact,
they play an absolutely central role.  To segregate the
computation destroys the proof.
After writing the paper {\it The Sphere Packing Problem}, I had all but given up
on solving the problem.  I had an extremely difficult nonlinear optimization problem
on my hands and no rigorous mathematical method to solve it.  In the summer  1993, 
filled with the excitement of Wiles's recently announced proof of Fermat's Last Theorem,
I happened upon
book on Pascal-XSC (a language extension of Pascal for interval analysis)
at the Seminary Coop Bookstore next to the University of
Chicago.  This book described the method I had lacked.
With fresh hope, in January 1994, 
I set aside all else and devoted full
effort to the sphere packing problem.   The interval code was the most difficult part of the computer code to implement because its speed was crucial.  Thanks to the improvements of S. Ferguson, eventually
the code could run from beginning to end in about three months.  
The interval verifications were the last part of the proof to be completed in August 1998. 


\clearpage
\section{Experiment}
\label{sec:experiment}


This essay describes some of the
motivation behind the partitions of space that have been used in the
solution to the sphere packing problem.  This discussion includes various
ideas that were tried, found wanting, and discarded. However, this
discussion provides motivation for some of the choices that appear
in the solution to the sphere packing problem.


Let $S$ be a regular tetrahedron of side length $2$.  If we place a
unit ball at each of the four vertices, the fraction of the
tetrahedral solid occupied by the part of the four balls within the
tetrahedron is $\dtet\approx 0.7797$. Let $O$ be a regular
octahedron of side length $2$.  If we place a unit ball at each of
the four vertices, the fraction of the octahedral solid occupied by
the four balls is $\doct\approx 0.72$. The face-centered cubic
packing can be obtained by packing eight regular tetrahedra and six
regular octahedra around each vertex. The density $\pi/\sqrt{18}$ of
this packing is a weighted average of $\dtet$ and $\doct$:
    $$\frac\pi{\sqrt{18}} = \frac13\dtet + \frac23\doct.$$

My early conception (around 1989) was that for every packing of
congruent balls, there should be a corresponding partition of space
into regions of high density and regions of low density. Regions of
high density should be defined as regions having density between
$\doct$ and $\dtet$, and regions of low density should be defined as
those regions of density at most $\doct$.  It was my intention to
prove that all regions of high density had to be confined to a set
of nonoverlapping tetrahedra whose vertices are centers of the balls
in the packing.

Thus, the question naturally arises of how much a regular
tetrahedron of edge length $2$ can be deformed before its density
drops below that of a regular octahedron $\doct$.  The following
graph (Figure~\ref{fig:t51}) shows the density of a tetrahedron with
five edges of length $2$ and a sixth edge of length $x$.
Numerically, we see that the density drops below $\doct$, when
$x=x_0\approx 2.504$. To achieve the design goal of confining
regions of high density to tetrahedra, we want a tetrahedron of edge
lengths $2,2,2,2,2,x$, for $x\le x_0$, to be counted as a region of
high density. Rounding upward, this example led to the cutoff
parameter of $2.51$ that distinguishes the tetrahedra (in the high
density region) from the rest of space. This is the origin of the
constant $2.51$ that appears in the proof.


\begin{figure}[htb]
  \centering
  \myincludegraphics{\ps/t51.eps}
  \caption{The origin of the constant $2.51$.}
  \label{fig:t51}
\end{figure}

Since the tetrahedra are chosen to have vertices at the centers of
the balls in the packing, it was quite natural to base the
decomposition of space on the Delaunay decomposition. According to
this early conception, space was to be partitioned into Delaunay
simplices.  A Delaunay simplex whose edge lengths are at most
$2.51$ is called a quasi-regular tetrahedron.  These were the
regions of presumably high density.  According to the strategy in
those early days, all other Delaunay simplices were to be shown to
belong to regions of density at most $\doct$.

The following problem occupied my attention for a long period.


\smallskip\noindent
{\bf Problem} Fix a saturated packing. Let $X(oct)$ be the part of
space of a saturated packing that is occupied by the Delaunay
simplices having at least one edge of length at least $2.51$.  Let
$X(tet)$ be the union of the complementary set of Delaunay
simplices.  Is it always true that the density of $X(oct)$ is at
most $\doct$?

Early on, I viewed the positive resolution of this problem as
crucial to the solution to the sphere packing problem.  Eventually,
when I divided the solution of the sphere packing problem into a five step
program, a variant of this problem became the second step of the
program. See \cite{part2}.

To give an indication of the complexity of this problem, consider
the simplex with edge lengths $(2,2,2,2,\ell,\ell)$, where $\ell =
\sqrt{2 (3 + \sqrt6)}\approx 3.301$.  Assume that the two longer
edges meet at a vertex.  This simplex can appear as the Delaunay
simplex in a saturated packing.  Its density is about $0.78469$.
This constant is not only greater than $\doct$; it is even greater
than $\dtet$, so that the problem is completely misguided at the
level of individual Delaunay simplices in $X(oct)$.  It is only in
when the union of Delaunay simplices is considered that we can
hope for an affirmative answer to the problem.

By the summer of 1994, I had lost hope of finding a partition of
the set $X(oct)$ into small clusters of Delaunay simplices with
the property that each cluster had density at most $\doct$.
Progress had ground to a halt.   The key insight came in the fall
of 1994 (on Nov 12, 1994 to be precise). On that day, I introduced
a hybrid decomposition that relied on the Delaunay simplices in
the regions $X(tet)$ formed by quasi-regular tetrahedra, but that
switched to the Voronoi decomposition in certain regions of
$X(oct)$. By April 1995, I had reformulated the problem, worked
out a proof of the problem \cite{part2} in its new form, and
submitted it for publication. I submitted a revised version of
\cite{part1} that same month.  The revision mentions the new
strategy: ``The rough idea is to let the score of a simplex in a
cluster be the compression $\Gamma(S)$ [a function based on the
Delaunay decomposition] if the circumradius of every face of $S$
small, and otherwise to let the score be defined by Voronoi cells
(in a way that generalizes the definition for quasi-regular
tetrahedra).'' See \cite[p.6]{part1}.

The situation is somewhat more complicated than the previous
paragraph suggests. Consider a Delaunay simplex $S$ with edge
lengths $(2,2,2,2,2,2.52)$. Such a simplex belongs to the region
$X(oct)$. However, if we break it into four pieces according to
the Voronoi decomposition, the density of the two of the pieces is
about $0.696<\doct$ and the density of the other two is about
$0.7368>\doct$. It is desirable not to have any separate regions
in $X(oct)$ of density greater than $\doct$.  Hence it is
preferable to keep the four Voronoi regions in $S$ together as a
single Delaunay simplex.  A second reason to keep $S$ together is
that the proof of the local optimality of the face-centered cubic
packing and hexagonal close packing seems to require it.  A third
reason was to treat pentahedral prisms.  (This is a thorny class
of counterexamples to a pure Delaunay simplex approach to the
solution to the sphere packing problem.  See \cite{spp}, \cite{remarks},
and \cite{Fer97}.)  For these reasons, we identify a class of
Delaunay simplices in $X(oct)$ (such as $S$) that are to be
treated according to a special set of rules. They are called {\it
quarters}.  As the name suggests, they often occur as the four
simplices comprising an octahedron that has been ``quartered.''

One of the great advantages of a hybrid approach is that there is
a tremendous amount of flexibility in the choice of the details of
the decomposition.  The details of the decomposition continued to
evolve during 1995 and 1996.  Finally, during a stay in Budapest
following the Second European Congress in 1996, I abandoned all
vestiges of the Delaunay decomposition, and adopted definitions of
quasi-regular tetrahedra and quarters that rely only on the metric
properties of the simplices (as opposed to the Delaunay criterion
based on the position of other sphere centers in relation to the
circumscribing sphere of the simplex).  This decomposition of
space is essentially what is used in the final proof.

The hybrid construction depends on certain choices of functions
(satisfying a rather mild set of constraints).  To solve the
sphere packing problem appropriate functions had to be selected, and an
optimization problem based on those functions had to be solved.
This function is called {\it the score}.  Samuel Ferguson and I
realized that every time we encountered difficulties in solving
the minimization problem, we could adjust the scoring function
$\sigma$ to skirt the difficulty.  The function $\sigma$ became
more complicated, but with each change we cut months -- or even
years -- from our work.  This incessant fiddling was unpopular
with my colleagues.  Every time I presented my work in progress at
a conference, I was minimizing a different function.  Even worse,
the function was mildly incompatible with what I did in earlier
papers \cite{part1} \cite{part2}, and this required going back and
patching the earlier papers.

The definition of the scoring function $\sigma$ did not become
fixed until it came time for Ferguson to defend his thesis, and we
finally felt obligated to stop tampering with it.  The final
version of the scoring function $\sigma$ is rather complicated.
The reasons for the precise form of $\sigma$ cannot be described
without a long and detailed description of dozens of sphere
clusters that were studied in great detail during the design of
this function. However, a few general design principles can be
mentioned.  These comments assume a certain familiarity with the
design of the proof.


(1) Simplices (with vertices at the centers of the balls in the
packing) should be used whenever careful estimates of the density
are required.  Voronoi cells should be used whenever crude
estimates suffice.  For Voronoi cells, it is clear what the
scoring function should be.



(2) The definition of the scoring function for quasi-regular
tetrahedra was fixed by \cite{part1} and this definition had to
remain fixed to avoid rewriting that long paper.

Because of these first two points, most of the design effort for
the function $\sigma$ was focused on quarters.

(3)  The decision to make the scoring for a quarter change when
the circumradius of a face reaches $\sqrt2$ is to make the proof
of the local optimality of the fcc and hcp packings run smoothly.
From \cite{part2}, we see that the cutoff value $\sqrt2$ is
important for the success of that proof.  The cutoff $\sqrt2$ is
also important for the proof that standard components (other than
quasi-regular tetrahedra) score at most $0\,\pt$.

(4) The purpose of adding terms to the scoring function $\sigma$
is to make
interval arithmetic comparisons
easier to carry out.  This is useful in arguments about ``erasing
upright quarters.''

\subsection{content}

In \cite{part1}, a five-step program was described to solve the
sphere packing problem.  It was planned that there would be five
papers, each proving one step in the program.  The papers
\cite{part1} and \cite{part2} carry out the first two steps in the
program. Because of the changes in the scoring function, it was
necessary to issue a short paper \cite{Form} mid-stream whose
purpose was to give some adjustments to the five-step program.
This paper adjusts the definitions from \cite{part1} and checks
that none of the results from \cite{part1} and \cite{part2} are
affected in an essential way by these changes. Following this, the
papers \cite{Hal98B} and \cite{Fer97} appeared in preprint form,
completing the third and fifth steps of the program. The fourth
step turned out to be particularly difficult. It occupies two
separate papers \cite{Hal98C} and \cite{Hal98D}.

The original series of papers suffers from the defect of being
written over a span of several years.  Some shifts in the
conceptual framework of the research are evident.   Based on
comments from referees, a revision of these papers was prepared in
2002. The revisions were small, except for the paper
\cite{Hal98D}, which was completely rewritten. The structure of
the proof remains the same, but it adds a substantial amount of
introductory material that lessens the dependence on \cite{part1}
and \cite{part2}.

The papers were reorganized again in 2003.  The series of papers
is no longer organized along the original five steps with a
mid-stream correction.  Instead, the proof is now arranged
according to the logical development of the subject matter.  Only
minor modifications have been made to the original proof.  (The
earlier versions are still available from \cite{arXiv}.)  In the
2003 revision, the exposition of the proof is entirely independent
of the earlier papers \cite{part1} and \cite{part2}.

An introduction to the ideas of the proof can be found in
\cite{CH}. An introduction to the algorithms can be found at
\cite{algorithm}. Speculation on a second-generation design of a
proof can be found in \cite{algorithm} and \cite{arbeitstagung}.


\subsection{complexity}

Why is this a difficult problem?  There are many ways to answer this
question.

This is an optimization problem in an infinite number of
variables.  In many respects, the central problem has been to
formulate a good finite dimensional approximation to the density
of a packing.  Beyond this, there remains an extremely difficult
problem in global optimization, involving nearly 150 variables.
We recall that even very simple classes of nonlinear optimization
problems, such as quadratic optimization problems, are NP-hard
\cite{HoPT95}.  A general highly nonlinear program of this size is
regarded by most researchers as hopeless (at least as far as
rigorous methods are concerned).

There is a considerable literature on many closely related nonlinear
optimization problems (the Tammes problem, circle packings, covering
problems, the Lennard-Jones potential, Coulombic energy minimization
of point particles, and so forth). Many of our expectations about
nonlattice packings are formed by the extensive experimental data
that have been published on these problems. The literature leads one
to expect a rich abundance of critical points, and yet it leaves one
with a certain skepticism about the possibility of establishing
general results rigorously.

The extensive survey of circle packings
in \cite{Mel97} gives a broad overview of the progress and limits
of the subject.
Problems involving a few circles can be
trivial to solve.
Problems involving several circles in the plane can be solved with
sufficient ingenuity.
With the aid of computers, various  problems involving
a few more circles can be
treated by rigorous methods.
Beyond that, numerical methods
give approximations but no rigorous solutions.
Melissen's account of
the 20-year quest for the best separated arrangement of 10
points in a unit square is particularly revealing of the complexities
of the subject.

Sphere packings have a particularly rich collection of (numerical)
local maxima that come uncomfortably close to the global maximum
\cite{spp}. These local maxima explain in part why a large number
(around 5000) of planar maps are generated as part of the proof of
the conjecture.  Each planar map leads to a separate nonlinear
optimization problem.



\subsection{computer}

As this project has progressed, the computer has replaced conventional
mathematical arguments more and more, until now
 nearly every aspect of the proof relies on
computer verifications.  Many assertions in this book
 are results of computer calculations.
To make the solution more accessible, I have
posted extensive resources \cite{web}.

Computers are used in various significant ways.  They will be
mentioned briefly here, and then developed more thoroughly elsewhere
in the collection, especially in the final paper.

1. {\it  Proof of inequalities by interval arithmetic}.  ``Sphere Packings
I'' describes a method of proving various inequalities in a small number
of variables by computer by interval arithmetic.

2.  {\it Combinatorics}.  A computer program classifies all of the planar maps
that are relevant to the sphere packing problem.

3. {\it  Linear programming bounds}.  Many of the nonlinear optimization
    problems for the scores of centered packings are replaced by linear
    problems that dominate the original score.  They are solved
    by linear programming methods by computer.  A typical problem has
    between 100 and 200 variables and 1000 and 2000 constraints.  Nearly
    100000
    such problems enter into the proof.

4. {\it Branch and bound methods}.  When linear programming methods do not
    give sufficiently good bounds, they have been combined with branch
    and bound methods from global optimization.

5.  {\it Numerical optimization}.  The exploration of the problem
    has been substantially
    aided by nonlinear optimization and symbolic math packages.

6. {\it Organization of output}.
    The organization of the few gigabytes of code and data that
    enter into the proof is in itself a nontrivial undertaking.




%\section{Sphere Packings in Two and Three Dimensions}

% circle packings.

%\section{The Face-Centered Cubic Packing}

%\section{Thirteen Spheres}

% Kissing numbers

%\section{Interval Analysis and Global Optimization}

% deliberate error

%\section{Linear Programming}

%\section{Tarski Arithmetic}

%\section{Fejes T\'oth on Sphere Packings}

%\section{Delaunay Simplices and Sphere Packings}

%\section{Mathematics and Computers}

%\section{Formal Proofs and the Flyspeck Project}

%\section{The Dodecahedral Theorem}

