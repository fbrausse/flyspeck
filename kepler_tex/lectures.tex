%\chapter{Essays}

This is a warm-up chapter that covers key ideas of the sphere packing problem.
The chapter is called ``Essays'' because it contains background material
needed to develop a firm intuitive understanding of what is really going on in the proof.   How might we think about the sphere packing problem?  
%How might we think about the computations that enter into the solution?


\section{Face Centered Cubic}

% all sorts of packing and tiling problems.
% Hilbert 18th.  Error-correcting codes.
% conjectures.




The face-centered cubic packing is the familiar pyramid arrangement
of balls on a square base.  It is also the pyramid arrangement on 
a triangular base.  The square base packing and triangular base
packing differ only in their orientation in space.  
Figure~\ref{XX} % not drawn.
shows how the triangular base packing fits between the peaks
of two adjacent square base pyramids.

\bigskip

% based on page 28 of kepler research scan/scan0001.tif
The fcc packing can also be viewed as an alternating tiling of regular
tetrahedra and regular octahedra.  Figure~\ref{XX} shows this alternating
arrangement in a small triangular base pyramid.  
%If the balls have
%unit radius, each tetrahedron and octahedron has edge length $2$.
The illustrated pyramid consists of one
tetrahedron at each vertex and one octahedron in the center.
By similarity, the pyramid has $8 = 2^3$ times the volume of each
smaller tetrahedron.
Using this dissection to solve for the volume of the octahedron, we find that
the volume of a regular octahedron is exactly four times
the volume of a regular tetrahedron of the same edge length.

Density, defined as a ratio of volumes, is insensitive to changes
of scale.  For convenience, we will always work with balls of unit radius.
This means that the distance between centers of balls in a sphere packing
will always be at least $2$.  We identify a sphere packing with
its set of centers.  Thus, for our purposes a sphere packing is just
a set of points in $\ring{R}^3$, whose elements are separated by
distances at least $2$.




The face-centered cubic packing can be described as the packing obtained
by starting with a cubic lattice, with a ball at each vertex and adding
a ball at the center of each face.  This construction explains the
name of the face-centered cubic packing.  The edge of each cube should be
$2\sqrt2$, and the diagonal of each face $4$, to leave room for balls
of unit diameter.  The density of the packing is equal to the density
within each cube.  Each cube has volume $(2\sqrt2)^3 = 16\sqrt2$.
Each cube contains a total of four balls:  half a ball along each
of six faces and one eighth of a ball at each of eight corners.
Thus, the density is
   $$
   \frac{   4 (4\pi/3)}{16\sqrt2} = \frac{\pi}{\sqrt{18}}.
   $$
See Figure~\ref{XX}.

The alternating arrangement of regular tetrahedra and octahedra can be
seen in this cubic picture.  There is one tetrahedron at each vertex
of the cube, extending to the centers of the three adjacent faces.
There an octahedron in the center of the cube, whose vertices lie
at the centers of the six faces.  There is an additional quarter of an
octahedron along each edge, extending to the midpoints of the two adjacent
faces.  This gives a total of eight tetrahedra and four octahedra.  
As each
octahedron has the volume of four tetrahedra, exactly $1/3$ of
the cube is filled with tetrahedra, the other $2/3$ with octahedra.
This decomposition determines the volume of a tetrahedron 
(pretend we didn't know). The volume $16\sqrt2$ 
of the cube equals $24$ tetrahedra $\ldots$, giving each a volume
of $2\sqrt{2}/3$.

The density of the face-centered cubic packing is the weighted density
of the densities of the tetrahedron and octahedron.  Write $\dtet$
and $\doct$ for these densities.  For example, $\dtet$ is the ratio
of the volume
of the part within the tetrahedron of the unit balls (at the four vertices)
to the volume of the tetrahedron.  As the cube is $1/3$ filled  with
tetrahedra and $2/3$ filled with octahedra, we have
$$
  \frac{\pi}{\sqrt{18}} = \frac{1}{3}\dtet + \frac{2}{3}\doct.
$$

The Voronoi cell of a point in a sphere packing $\Lambda$ consists of
all points in $\ring{R}^3$ that are closer to that point than to any
other point of $\Lambda$.
Each Voronoi cell of the face-centered cubic
packing is a rhombic dodecahedron (Figure~\ref{XX}).   %% not drawn.
The rhombic dodecahedron can be constructed from a cube by placing a
square based pyramid (with height half as great as an edge of its square
base) on each of the six faces (Figure~\ref{XX}).  %% not drawn. 
For the scale to be correct, the
initial cube should have edge $\sqrt{2}$. This construction follows
from the earlier cube we considered of side $2\sqrt2$.  If we center
a cube of side $\sqrt2$ at each ball in the packing. These cubes fill
the black grid of an infinite three dimensional checkerboard,
leaving a second grid of white cubes unfilled.  Each
white cube can be partitioned into  pyramids along
its faces with apex at the center of the cube.  Attaching these pyramids
along their bases to the adjoining black cubes gives the Voronoi cell.

Each Voronoi cell contains one black cube of side $\sqrt2$ and a total
of one white cube, for a total volume of $4\sqrt2$.  This constant is one of the
fundamental constants in this book.  The volume of every Voronoi cell
is compared against the volume of the rhombic dodecahedron.
The density of the face-centered
cubic is the ratio of the volume of a ball to the volume of its Voronoi
cell, which gives $\pi/\sqrt{18}$, yet again.





\subsubsection{hexagonal-close packing}

The following facts about packings are well-known.  However, there
is a popular and persistent misconception in the popular press
that the face-centered cubic packing is the only packing with
density $\pi/\sqrt{18}$. 

In the face-centered cubic packing, each ball is tangent to twelve
others.  For each ball in the packing, this arrangement of twelve
tangent balls is the same.  We call it the fcc pattern. 
In the
hexagonal-close packing, each ball is tangent to twelve others.
For each ball in the packing, the arrangement of twelve tangent
balls is again the same.  We call it the hcp pattern.  The fcc
pattern is different from the hcp pattern.  In the fcc pattern,
there are four different planes through the center of the central
ball that contain the centers of six other balls at the vertices
of a regular hexagon.  In the hcp pattern, there is only one such
plane.  We call the arrangement of balls tangent to a given ball
the {\it local tangent arrangement} of the ball.

There are uncountably many packings of density $\pi/\sqrt{18}$
that have the property that every ball is tangent to twelve others
and such that the tangent arrangement around each ball is either
the fcc pattern or the hcp pattern.

By {\it hexagonal layer}, we mean a translate of the two-dimensional
lattice of points $M$ in the $A_2$ arrangement. That is, $M$ is a
translate of the planar lattice generated by two vectors of length
$2$ and angle $2\pi/3$.  The face-centered cubic packing is an
example of a packing built from hexagonal layers.

If $M$ is a hexagonal layer, a second hexagonal layer $M'$ can be
placed parallel to the first so that each lattice point of $M'$ has
distance $2$ from three different vertices of $M$.  When the second
layer is placed in the manner, it is as close to the first layer as
possible. Fix $M$ and a unit normal to the plane of $M$. The normal
allows us to speak of the second layer $M'$ as being ``above'' or
``below'' the layer $M$. There are two different positions in which
$M'$ can be placed closely above $M$ and two different positions in
which $M'$ can be placed closely below $M$. As we build a packing,
layer by layer, ($M$, $M'$, $M''$, and so forth), there are two
choices at each stage of the close placement of the layer above the
previous layer. Running through different sequences of choices gives
uncountably many packings.  In each of these packings the tangent
arrangement around each ball is that of the twelve spheres in the
face-centered cubic or the twelve spheres in the hexagonal-close
packing.

Let $\Lambda$ be a packing built as a sequence of close-packed
hexagonal layers in this fashion.  If $P$ is any plane parallel to
the hexagonal layers, then there are at most three different
orthogonal projections of the layers $M$ to $P$.  Call these
projections $A$, $B$, $C$.  Each hexagonal layer has a different
projection than the layers immediately above and below it.  In the
fcc packing, the successive layers are $A,B,C,A,B,C,\ldots$.  In
the hcp packing, the successive layers are $A,B,A,B,\ldots$.  If
we represent $A$, $B$, and $C$ as the vertices of a triangle, then
the succession of hexagonal layers can be described by a walk
along the vertices of the triangle. Different walks through the
triangle describe different packings.

% In the face-centered
%cubic packing, every local tangent arrangement of $12$ tangent
%balls is the same.  We call this local tangent arrangement the
%fcc-pattern. Similarly, the local tangent arrangement of the
%hexagonal-close packing will be called the hcp-pattern.  These
%patterns are uniquely determined up to rigid motion.

In fact, the different walks through a triangle give all packings of
infinitely many equal balls in which the tangent arrangement around
every ball is either the fcc pattern of twelve balls or the hcp
pattern of twelve balls.

\bigskip


{

\narrower

\font\ninerm=cmr9 \ninerm

%\def\=#1{\accent"16 #1}


We justify the fact that different walks through a triangle give all
such packings. Assume first that a packing $\Lambda$ contains a ball
(centered at $v_0$) in the hcp pattern. The hcp pattern contains a
uniquely determined plane of symmetry. This plane contains $v_0$ and
the centers of six others arranged in a regular hexagonal. If $v$ is
the center of one of the six others in the plane of symmetry, its
local tangent arrangement of twelve balls must include $v_0$ and an
additional four of the twelve balls around $v_0$. These five centers
around $v$ are not a subset of the fcc pattern. They can be uniquely
extended to twelve centers arranged in the hcp pattern. This hcp
pattern has the same plane of symmetry as the hcp pattern around
$v_0$. In this way, as soon as there is a single center with the hcp
pattern, the pattern propagates along the plane of symmetry to
create a hexagonal layer $M$.

Once a packing $\Lambda$ contains a single hexagonal layer, the
condition that each ball be tangent to twelve others forces a
hexagonal layer $M'$ above $M$ and another hexagonal layer below
$M$.  Thus, a single hexagonal layer forces a sequence of
close-packed hexagonal layers in both directions.

We have justified the claim under the hypothesis that $\Lambda$
contains at least one ball with the hcp pattern.

Assume that $\Lambda$ does not contain any balls whose local
tangent arrangement is the hcp pattern.  Then every local tangent
arrangement is the fcc pattern, and $\Lambda$ itself is then the
face-centered cubic packing.  This completes the proof.


}

\subsection{Gauss}

Gauss proved that the face-centered packing has the greatest density
of any lattice packing in three dimensional Eulidan space.  
There is a short proof that
does not require any calculations.

Start with an arbitrary lattice $\Lambda$ in which every point has
distance at least two from every other.  Center a unit ball at each
point in the lattice.  A lattice of greatest density will certainly
have the property that some pair of balls touch.  The lattice property
then forces the formation of parallel infinite linear strings
of touching balls, like beads on a string.  The lattice of greatest
density will certainly have the property that two of these 
infinite parallel strings will touch.  The lattice property then forces
the formation of parallel sheets.  On each sheet the touching parallel
strings form a rhombic tiling.  The lattice of greatest density
will certainly have the property that each parallel sheet should sit
as snugly as possible on the sheet below.  That means that some ball (centered at $A$) of
one sheet will touch three balls (centered at $B,C,D$) 
on a the next layer down.

Since the balls on each sheet form a rhombic tile, two of the distances
between $B,C,D$, corresponding to two edges of the rhombus, 
are equal to $2$.  This means that $A$ together with
two of $B,C,D$ form an equilateral triangle.  Shifting attention to the plane
containing this equilateral triangle, the lattice property forces the entire
plane, as well as parallel planes, to be tiled with equilateral triangles.
From the earlier argument, each plane will sit as snugly as possible on
the sheet below.  Some ball of one sheet will touch the three balls forming
an equilateral triangle in the layer below.  These four balls form a regular
tetrahedron.  This tetrahedron 
uniquely identifies the lattice as the face-centered cubic.






\subsection{Thue}


As we have already mentioned, in 1890, 
Thue solved the  packing problem for congruent disks in the plane.
The optimal packing is the hexagonal packing (Figure~\ref{XX}). % not drawn
The density of this packing is $\pi/\sqrt{12}$.  (A unit disk has
area $\pi$; a hexagon of inradius $1$ has area $\sqrt{12}$.)
It admits an elementary solution that we sketch here.  B. Casselman
has an animated interactive demo of this solution \cite{BC00}.

Let $\Lambda$ be the set of centers of a collection of unit disks.
Take the Voronoi cell around each disk.  It is enough to show that
each Voronoi cell has area at least $\sqrt{12}$ (with equality exactly
when it is a hexagon of inradius $1$).  For simplicity,
we may assume that $0\in\Lambda$ is the center of our Voronoi cell.  

We truncate the Voronoi cell by intersecting it with a disk of radius
$\rho=2/\sqrt3$.  It is enough to show that the truncated Voronoi cell has
density  at most $\pi/\sqrt{12}$.  

There is not a point $w$ in the plane that has distance less than $\rho$
from three disk centers $v_1,v_2,v_3$.  Otherwise, one of the three
angles $\gamma$ at $w$ is at most $2\pi/3$; and $\cos\gamma\ge -0.5$  
The law of cosines applied
to the triangle $w,v_i,v_j$ with angle $\alpha$ gives the contradiction:
   $$
   4 \le c^2 = a^2 + b^2 - 2 a b \cos\gamma 
   \le a^2 + b^2 + a b < \rho^2 + \rho^2 + \rho^2 = 4.
   $$
This means that the boundary of the Voronoi cell consists of circular
arcs and line segments, each line segment extending on both ends to
the circle of radius $\rho$.  These parts are marked in
yellow and blue in  Figure~\ref{XX}. % not drawn.

The (yellow)
parts of the Voronoi cell that lie within a circular sector have density
$1/\rho^2 = 3/4 < \pi/\sqrt{12}$.  The (blue)
parts of the Voronoi cell that
lie with a triangle have density
   \begin{equation}\label{eqn:rog2d}
   \frac{\theta}{\rho^2 \cos\theta\sin\theta}
   \end{equation}
where $0 \le \theta\le \pi/6$.  An easy optimization gives the maximum
at $\theta=\pi/6$ with value $\pi/\sqrt{12}$.

In some ways it it unfortunate that the problem in two dimensions is so
elementary.  It gives us few clues about how to undertake the problem in
three dimensions.  It does however give a few meager hints.  It suggests the
introduction of Voronoi cells and the usefulness of truncation.
The little optimization problem on triangles in Equation~\ref{eqn:rog2d}
generalizes to $n$-dimensions.  This is Rogers's lemma.
It appears as Lemma~\ref{lemma:rogers} in this book.  It is an immensely
useful lemma in the study of sphere packing densities.

%\subsection{two dimensions}

\bigskip

There are many proofs of Thue's theorem.  Here we present a second proof
that is due to L. Fejes T\'oth.  In our discussion of the problem of thirteen
spheres (Section~\ref{sec:13s}), we  introduce the Delaunay
triangulation of a finite set $C$ of points on a sphere.
Its characteristic property is that the circumscribing circle of each
triangle contains no points of $C$ in its interior.

There is a corresponding Delaunay triangulation for infinite sets $\Lambda$
of points in the plane (or more generally in $n$-dimensions).  We
assume that $\Lambda$ is the set of centers of sphere packing so that
$|v-w|\ge2$, for $v\ne w\in \Lambda$.  If we enlarge a packing
$\Lambda\subset\Lambda'$, then the density can only go up.  In issues
of density, we may therefore assume that $\Lambda$ is saturated.  This
means that it is not a proper subset of another packing.  A saturated
packing of the plane (or $n$-space) has a Delaunay triangulation.  Its
characteristic property is that the circumscribing circle of each triangle
contains no points of $\Lambda$ in its interior.   Each triangle has
radius at most $2$, for otherwise the packing is not saturated: an additional
point can be placed at the center of the circumscribing circle.

\begin{proof}
Admitting the existence of the Delaunay triangulation, the proof 
of the packing problem in two dimensions is
elementary.  Each triangle contains a portion of a disk at each of
its three vertices.  The interior angles of a triangle sum to $\pi$, giving
half a disk per triangle.  If we show that each triangle has area at least
$\sqrt{3}$, then the density of each triangle is at most
 $(\pi/2)/(\sqrt{3}) = \pi/\sqrt{12}$.  To minimize the area of Delaunay
triangle $ABC$, we first replace it with a smaller similar triangle whose
shortest edge (say $BC$) has length $2$.  The third vertex $A$ is constrained
to have distance at lest $2$ from $B$ and $C$, and to lie in the region
giving circumradius at most $2$.  The constraints on $A$ form three circular
arcs as shown in Figure~\ref{XX} % not drawn.

The minimizing triangle is determined by the point $A$ closest to the
line $BC$.  There are three such points, all giving triangles of area
exactly $\sqrt3$.  This completes the proof.
\end{proof}

This proof suggests that a proof might also make use of the Delaunay
decomposition.  This indeed was the starting point of my work on the
problem, to find a three dimensional approach to the sphere packing
problem that generalizes the proof just presented \cite{Hal93}.
Next we turn to various experiments in three dimensions and how
they affect the design of the solution.




\clearpage




\section{Thirteen Spheres}\label{sec:13s}
\FIXX{Post code at repository}

The Newton-Gregory kissing number problem  was mentioned in the chapter on
history.  In the face-centered cubic packing each ball touches exactly twelve
others.  Is this the maximum, 
or is it possible to place thirteen congruent balls so that they
all touch a fixed (fourteenth) ball congruent to the others?
Gregory suspected so, Newton suspected not.  Ultimately Newton turned
out to be correct. 

This essay presents a proof of the problem that is based on a Leech's proof
of 1956.  For decades Leech's short proof stood as a model of elegance and clarity.
However,  when Leech's proof was abruptly dropped from the second
edition of {\it Proofs from the Book}, people started to ask questions \cite{AZ98}.  
B. Casselman, in a note on
the history of this problem, describes Leech's proof as ``cryptic'' and adds 
that ``although his reasoning has been accepted as correct, there are gaps in his
exposition $\ldots$  It would be valuable if someone were to publish an account
of Leech's proof that made it accessible to an elementary undergraduate course'' \cite{BC04}.
Maehara takes up the challenge in his proof for undergraduates \cite{Mae07}.

The general outline of Leech's proof is similar to the general outline of the solution
of the sphere packing problem.  By studying Leech's proof, we can see in a few pages
the structure of this entire book.  For that reason, the ideas in this small calculation
are particularly relevant. 

\begin{theorem} The kissing number in three dimensions is twelve.
\end{theorem}

We explain the strategy of the proof.    
Start with any arrangement
of nonoverlapping 
unit spheres tangent to a fixed unit sphere centered at the origin.  Each
center $v\ne0$ gives a point $v/|v|$ on the unit sphere.  The condition
that the spheres do not overlap implies that the length of the arc on the
unit sphere from one point to another is at least $\pi/3$.  
We study finite sets of points on the unit sphere that satisfy this length condition.
We work in spherical geometry on the
unit sphere at the origin.
Triangles are formed by arcs of great circles on the sphere and distances are
the lengths of the arcs.

Let $V$ be the maximum
number such that it is possible to arrange $V$ points on the unit
sphere $S^2$ with angular separation at least $\pi/3$.  Let $X\subset S^2$ be
such an arrangement, assumed to contain at least $13$ points. 
Take the Delaunay triangulation (explained below) 
of the sphere with vertices in $X$. Since  $V$ is as large as
possible, each triangle has circumradius at most $\pi/3$.  By estimating
the area of each triangle, we will find that the sum of the areas of the triangles is
greater than $4\pi$.  This is absurd, since the sum of
the areas is no greater than the area $4\pi$ of the sphere, which they tile.

The key to estimating the sum of the areas of a triangle is Euler's formula,
relating the number of vertices $V$, edges $E$ and faces $F$ of
a triangulation:
    $$V - E + F = 2,\quad 3 F = 2 E,\quad \Rightarrow F =
    (2V-4).$$
The relation $3 F = 2 E$ is obtained by counting the number of oriented edges of the
triangulation in two ways, either as three oriented edges per triangle, or as two oriented edges per 
unoriented edge.
Assuming $V\ge 13$, this gives $F\ge 22$. 

\subsection{partitioning the sphere}

The Delaunay triangulation is defined as follows.  Let $X$ be as above.  The
assumption that $V$ is as large as possible implies that the convex hull of $X$
contains the origin.  This convex hull is a polyhedron.  Triangulate faces of
this polyhedron so that each face is a triangle.  Whenever two points in $X$ are
joined by an edge of the polyhedron, join the points by an arc of a great circle.
This lifts each face of the polyhedron to a spherical triangle.  This is the Delaunay
triangulation.  Every point of $X$ lies in the same (closed) half space as the origin with
respect to any plane  through any triangular face of the polyhedron.  
Or expressed equivalently in 
spherical geometry, every point of $X$ lies in the same region with respect to the
circumscribing circle on the unit sphere defined by one of the spherical triangles.
The edges of polyhedron form a planar graph and so do the arcs on the unit sphere.


\subsection{trigonometry}

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/thirteenA.eps}
  \caption{As we move $A$ away from a critical
point, keeping the distance to $C$ fixed, we move to Lexell
circles of smaller area.}
  \label{fig:13:A}
\end{figure}
%\figsz{SCAN/thirteenA.eps}{}{fig:13:A}{0.7}

As we study how the area of a spherical triangle varies as it is deformed,
there are two basic results in spherical trigonometry that are used.

\begin{lemma}[Lexell] Let $ABC$ be a spherical triangle on the unit sphere.  As $A$ varies
in such a way to preserve area of the triangle $ABC$, it traces an arc of
a circle with endpoints at $B'$ and $C'$ (the points antipodal to $B$ and $C$).
The center of the circle is equidistant from $B$ and $C$.
\end{lemma}

\begin{proof} As this result is not used elsewhere in the book, we omit the proof.
See \cite{Fej72}.
\end{proof}

\begin{lemma}[Girard] Let $ABC$ be a spherical triangle on a unit sphere. If its angles
are $\alpha,\beta,\gamma$, then its area is
   $$
   \alpha+\beta+\gamma-\pi.
   $$
\end{lemma}

\begin{proof} This appears as Lemma~\ref{lemma:prim-volume}.
\end{proof}

\begin{lemma} Fix the length of two sides of a spherical triangle.
The area of the triangle, viewed as a function of the length of
the third side, does not have a local minimum on any interval.
\end{lemma}

\begin{proof}  Fix both endpoints $BC$ of one of sides of the triangle.
Assume that the length $c$ of $AB$ is fixed, which $BC$\FIXX{grammar}
varies. A critical point of the area function can be viewed
geometrically as a point of tangency between the Lexell circle 
and the circle $S$ of radius $c$ centered at $B$.
We see geometrically that any such point of tangency is a local
maximum for the area rather than a local minimum, because as we
move away from the point of tangency along $S$ we sweep into
Lexell circles representing smaller areas.
\end{proof}

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/thirteenB.eps}
  \caption{As we move $A$ away from a critical
point, keeping the circumradius of $ABC$ fixed, we move to Lexell
circles of smaller area.}
  \label{fig:13:B}
\end{figure}
%\figsz{SCAN/thirteenB.eps}{}{fig:13:B}{0.7}

\begin{lemma} Fix two vertices of a spherical triangle
and its circumscribing circle.  The area of the triangle, viewed
as a function of the position of the third vertex on the
circumscribing circle, does not have a local minimum.
\end{lemma}

\begin{proof}  Again, a critical point is represented
geometrically as a point of tangency between the Lexell circle and
the circumscribing circle.  As in the previous lemma, this is a
local maximum.
\end{proof}


\subsection{bounds}

It is then an easy task to find a lower area bound on collections
of triangles subject to edge length constraints or to circumradius
constraints, because there are no interior point minima.  It is
enough to check all endpoints to find the minimum.  We summarize
our findings in a table listing upper and lower bounds on each
side and an upper bound $M$ on the circumradius.  A dash indicates
no constraint.   To make the cases disjoint, we assume that the upper
bounds are weak inequalities and the lower bounds are strict.   Let
    $$
    b = \pi/3, \quad c = 1.42, \quad d = 1.7,\quad
    \Delta = 3\arccos(1/3)-\pi,\quad \epsilon = 0.4381.
    $$
We then have
$$
    \begin{array}{lllllllll}
 &\min_1&\max_1&\min_2&\max_2&\min_3&\max_3&M    &\min_{\op{area}}\\
 1:&c &- &c &- &c &- &b &\Delta+\epsilon\\
 2:&b &c &c &- &d &- &b &\Delta+\epsilon\\
 3:&b &c &c &d &c &d &- &\Delta+\epsilon/2\\
 4:&b &c &b &c &c &d &- &\Delta+\epsilon/4\\
 5:&b &c &b &c &d &- &b &\Delta\\
 6:&b &c &b &c &b &c &- &\Delta
    \end{array}
$$
For example, the first row of the table 
asserts that any spherical triangle whose three sides are at least
$c$ and whose circumradius is at most $b$ has area at least $\Delta+\epsilon$.
 The rows cover all
possibilities for edge lengths for triangles with circumradius at most $b$.
Each triangle has area at least $\Delta$.


\subsection{assembly}

We draw out some elementary consequences of this table.\FIXX{CHeck from here.}

\begin{itemize}
  \item $V=13$ and $F=(2V-4)=22$.  In fact, if $V\ge14$ then $F= (2V-4)\ge 24$ and
  the total area of the triangles is at least $24\Delta > 4\pi$.
  \item With a calculator we compute that $4\pi < 22\Delta+\epsilon$.
  Thus, the excesses of the areas over
  $\Delta$ in the table must sum to a constant strictly less than
  $\epsilon$.  Thus, triangles in the first two rows of the table
  never occur.  In particular, a triangle with two edges longer
  than $c$ has edges shorter than $d$.
  \item A triangle in the third row never appears, because it
  would be flanked on both sides by triangles from the first four rows,
  giving excesses $\epsilon/2 + \epsilon/4 + \epsilon/4$ of at
  least $\epsilon$. In particular, a triangle never has two edges
  of length at least $c$.
  \item Triangles with one edge at least $c$ appear in pairs
  along the longer edge, forming quadrilaterals.  Changing the
  triangulation by switching to the opposite diagonal if
  necessary, we may assume that the shorter diagonal of the
  quadrilateral is an edge of the triangulation.  (We no longer
  have a circumradius bound, because the resulting triangles might not be Delaunay
  triangles.)
  \item 
  %The table omits a bound for triangles in the fifth row,
  %corresponding to quadrilaterals with a diagonal (in fact both diagonals) of length at
  %least $d$. 
  By the spherical law of cosines (Lemma~\ref{lemma:sloc}), the angles of the quadrilateral are
  at least $\pi/2$.  Thus, we may deform the quadrilateral, moving
  vertices internally to shrink the area, until a diagonal has length $d$, at which
  point we use the fourth row to get the bound.  We cannot deform all the way
  to a
  rhombus of side $\pi/3$, because such a rhombus has a diagonal
  of length at most $d$.
  \item There cannot be two edges longer than $c$.  Otherwise we
  get two quadrilaterals, each with area at least $2\Delta +
  \epsilon/2$, for total triangulation area of at least $22\Delta
  + \epsilon$.
  \item In summary, the triangulation has $13$ vertices, $22$
  faces, and at most one edge longer than $c$.  (Call this the long edge.)
  If there is an
  edge of length greater than $c$, the opposite diagonal of the
  corresponding quadrilateral also has length at least $c$.
\end{itemize}


\subsection{properties of  graphs of potential counterexamples}


The angles of every triangle, except those along a long edge, are
greater than $\pi/6$. (Again the angle function has no local
minimum so this fact is easy to check by using the spherical law of cosines (Lemma~\ref{lemma:sloc}).) So
the degrees at such vertices are at most $5$.  The degrees of the
vertices meeting a long edge are at most $6$.



There must be a vertex of degree six.  In particular, a long edge
exists.  In fact, we can count the number of oriented edges in the
triangulation two ways.  The oriented edges can be arranged three
to each triangle, or they can be arranged by originating vertex.
Assuming no degree six vertex, we get the contradiction.
    $$66 = 3 F \le 5 V = 65.$$

Let us call a triangle {\it oblong}, if it has a long edge.

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/thirteenC.eps}
  \caption{Vertices of degree four have total
excess of at least $\epsilon/2$.}
  \label{fig:13:C}
\end{figure}
%\figsz{SCAN/thirteenC.eps}{fig:13:C}{1.0}

\begin{lemma}  There is no vertex of degree four, except possibly
when one of its four triangles is oblong.
\end{lemma}

\begin{proof}  Assume a vertex of degree four that has no oblong triangles.
We calculate an upper bound on the angle $\gamma_{\max}$, and a
lower bound on the area $D_{\min}$, given bounds $[b,c]$ on the
first edge, $[b,c]$ on the second edge, and $[\min_3,\max_3]$ on
the edge opposite the angle $\gamma$.  The upper bound on the
angle $\gamma_{\max}$ occurs when the opposite side is as long as
possible, and each adjacent side is extremal in length or
terminates at a right angle.  We calculate each possibility and
take the maximum.  Let $b'=1.15$ and $b''=1.251$.

$$
    \begin{array}{llllll}
      &\min_3&\max_3   &\gamma_{\max} - \pi/2&   D_{\min}\\
    1:&b     &b'       &-0.21              &\Delta\\
    2:&b'    &b''      &-0.08              &\Delta + \epsilon/12\\
    3:&b''   &c        &0.14               &\Delta + \epsilon/6\\
    \end{array}
$$
We make the following observations about the table.  We can
discard any combination of four rows that gives total angle less
than $2\pi$  because the four angles around a a degree four vertex
sum to $2\pi$.  We can discard any combination of four rows that
gives excess area at least $\epsilon/2$, because the two oblong
triangles also have combined excesses $\epsilon/2$, for a total of
at least $\epsilon$.
\begin{itemize}
    \item There cannot be three or more triangles from the third row,
    because this would give excess area $\epsilon/2$.
    \item  There cannot be fewer than two triangles from the third
    row, because the angle sum would be less than $2\pi$. So there
    are exactly two triangles from the third row.
    \item There cannot be two triangles from the second row,
    because combined with the two in the third row, the
    excess too great.
    \item If there are two from the third row, and at most one
    from the second, then the angle sum is less than $2\pi$.  So
    degree four vertices cannot exist (if none of the triangles at
    that vertex has a long edge).
\end{itemize}
\end{proof}

%%  We could have done this as a linear program.
%%

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/thirteenD.eps}
  \caption{A quadrilateral of degrees $4665$ can
be transformed to degrees $6555$ by swapping
diagonals.}
  \label{fig:13:D}
\end{figure}
%\figsz{SCAN/thirteenD.eps}{}{fig:13:D}{0.7}

The final bits of the proof are purely combinatorial. If there is
one degree six vertex, then counting oriented edges, we get
    $$66 = 3 F \le 6 + 5(V-1) = 66,$$
so equality holds and every other vertex has degree $5$.  If there
are two degree six vertices (both endpoints of the long edge),
then the number of oriented edges is
    $$
    66 = 3 F = 6 (2) + 5 (V-3) + 4 (1) = 66,
    $$
so there must also be a degree $4$.  By the lemma, this vertex of
degree four lies on the quadrilateral formed by the two long-edged
triangles.  Changing the diagonal of this quadrilateral, we get a
triangulation with one vertex of degree six and all other vertices
of degree five.

Remove the vertex of degree six and all its edges from the graph.
We are left with a planar graph with twelve vertices giving the
triangulation of a hexagon.  The six vertices of the hexagon have
degree four and the six internal vertices have degree five.  Such
a triangulation does not exist by Lemma~\ref{lemma:trihex}.
%hexagon}.

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/thirteenE.eps}
  \caption{Removing a vertex of degree $6$ leaves a
triangulation of a hexagon (with $6$ interior vertices of degree
$5$).}
  \label{fig:13:E}
\end{figure}
%\fig{SCAN/thirteenE.eps}{}{fig:13:E}

\subsection{classification of graphs}

\begin{lemma}\label{lemma:trihex}  
There does not exist a  triangulation of the hexagon such that
    \begin{itemize}
     \item   The six vertices on the hexagon have degree four,
     \item   There are six internal vertices, each of degree five.
    \end{itemize}
\end{lemma}

%By a topological triangulation, we mean that the sides of the
%triangles may be arbitrary simple paths in the plane; they do not
%need to be straight line segments.

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/trihexA.eps}
  \caption{Mapping a triangulation of the hexagon to
the icosahedron}
  \label{fig:th:A}
\end{figure}
%\fig{SCAN/trihexA.eps}}{fig:th:A}

\begin{proof}  Assume for a contradiction, that it exists.  Map
the triangulated hexagon to the icosahedron as follows.  Fix an
initial triangle $T$ in the hexagon.  Map it to any triangle $T'$
on the icosahedron.  For any other triangle $S$, pick a path of
triangles from $T$ to $S$.  Follow the corresponding path on the
icosahedron.  Map $S$ to the final triangle $S'$ along the path on
the icosahedron.  Since the interior of the hexagon is simply connected,
the map does not depend on the path.

%\begin{figure}[htb]
%  \centering
%  \myincludegraphics{SCAN/trihexB.eps}
%  \caption{The image of $S$ does not depend on the
%path from $T$ to $S$.}
%  \label{fig:th:B}
%\end{figure}
%%\fig{SCAN/trihexB.eps}{.}{fig:th:B}
%
%The triangle $S'$ does not depend on the chosen path from $T$ to
%$S$.  In fact any path from $T$ to $S$ can be deformed to any
%other path, by a succession of two elementary changes. The first
%elementary change replaces the path $\ldots ABA\ldots $ with
%$\ldots A\ldots$, or vice versa.  Such a path contraction does not
%affect the terminal triangle $S'$ on the path.  The second
%elementary change replaces a loop around a vertex (of degree five)
%$ABCDEA$ with $A$, or vice versa.  This too, does not alter the
%terminal triangle.  Since we have path independence, if $T=S$,
%then $T'=S'$.
%
%\begin{figure}[htb]
%  \centering
%  \myincludegraphics{SCAN/trihexC.eps}
%  \caption{On path can be transformed to another by
%passing through edges and vertices.}
%  \label{fig:th:C}
%\end{figure}
%%\fig{SCAN/trihexC.eps}{}{fig:th:C}
%
Now consider the path $T=T_1,\ldots,T_{13}=T$ which runs around
the rim of the hexagon.  It  maps to a path on the
icosahedron $T'_1,\ldots,T'_{13}$.  Note that $T_1=T_{13}$ but
$T'_1\ne T_{13}'$, a contradiction.
\end{proof}

\begin{figure}[htb]
  \centering
  \myincludegraphics{SCAN/trihexD.eps}
  \caption{The triangle $T_1=T_{13}$ must map to both $T'_1$ and $T'_{13}$.}
  \label{fig:th:D}
\end{figure}
%\fig{SCAN/trihexD.eps}{}{fig:th:D}


\subsection{discussion}




A number of false proofs of this theorem have been published over the decades \cite{Hal94}.  
A number of new proofs have appeared in the past several years.  
For example, O. Musin solves it
with Delsarte's method (Legendre polynomials) \cite{Mus06}.  C. Bachoc and F. Vallentin use semidefinite
programming to give a solution \cite{BV06}.  Other proofs appear in 
\cite{Mae01}, \cite{Ans02},  \cite{Bor03}.   K. B\"or\"oczky and L. Szab\'o have calculated
bounds on how close a collection of $13$ spheres can come to tangency \cite{BoSz03}.  

Part of the uncertainty that has been expressed about
Leech's proof is that it is essentially a computer proof written before
the age of computer proofs.  {\tt CAML} code  to check the
calculations appears at \FIXX{give source}.  

The differences between the proof as presented and Leech's are minor.  Leech
uses the constant $\arccos(1/7)\approx 1.427$ instead of $c=1.42$.  Leech estimates
areas of polygons, rather than triangles; yet he estimates polygons by triangulating,
so it is the same in the end.  Leech does not include the little argument with
the icosahedron,  resorting to a brute search for a triangulation.

\subsection{summary}\label{sec:summary}

 Leech's proof, McLaughlin's proof of the dodecahedral theorem, and
the solution to the sphere packing problem run parallel to one another.
We give a summary of Leech's proof in a way that draws out the parallels.
The three problems optimize different objective functions $f$ and seek different
bounds $b$ on that function. In the case of Leech's proof, the objective
function $f$ is the total 
area of the tiles of a triangulation.  The bound $b=4\pi$ is the area of
the sphere.  We will return again and again to the following high-level description.  It maps for the reader where this book goes.

{\it

A planar graph partitions the
unit sphere.  An objective function is defined.
The objective function is a sum
of contributions from  faces of the planar graph.  
A computer calculates bounds
on the contribution from each face.  In an assembly phase of the proof, 
the target total contribution ($b$) is compared
with to the sum of bounds assembled from different faces. 
This comparison places
strong combinatorial restrictions on the planar graph.  
A classification of all 
such graphs shows what possibilities remain.  
Case-by-case analysis excludes the final possibilities.

}

In the original proof of the sphere packing problem, we partitioned the sphere
by means of a planar graph, just as in Leech.  In the revision of the proof contained
here,  we replace the spherical arcs on the unit sphere with what we call blades.
Each blade is the union of the rays through the origin through points of an arc.
Even though the language is slightly different, it amounts to the same thing.



\clearpage
\section{Dodecahedral Theorem}

A packing of congruent unit radius balls 
in Euclidean space determines regions called Voronoi cells around each ball.  
We identify the packing with the set $\Lambda$ of centers of the balls.  The
cell $\Omega(\Lambda,v)$ around a ball at $v$ 
consists of points of space that are closer to its center than
to any other ball center in the packing.

The dodecahedral theorem asserts that in any packing of congruent balls of Euclidean
space every Voronoi cell has volume at least that of a regular dodecahedron that
circumscribes a unit ball.  This was conjectured by L. Fejes T\'oth in 1943 and proved
by S. McLaughlin in 1998 \cite{McL98}.  This bound is realized by a finite 
packing $\Lambda_{dodec}$
(of twelve balls and a thirteenth  at the origin) obtained
by placing a ball at the center of each face of a regular dodecahedron.  The
theorem can then be stated as the inequality
  $$
  \op{vol}(\Omega(\Lambda,v)) \ge \op{vol}(\Omega(\Lambda_{dodec},0))
  $$
for every $v\in\Lambda$, and for every set of points $\Lambda\subset \ring{R}^3$
whose pairwise distances are at least the diameter $2$.
K. Bezdek conjectures that the surface area of any Voronoi cell in a packing
of unit balls is at least that of a regular dodecahedron of inradius $1$.
This strengthened version of the dodecahedral problem is still
open \cite{Bez04}.

The proof of the dodecahedral theorem is structurally very close to the solution of
the sphere packing problem.  By studying the parallels between the two proofs, we 
will be better prepared to tackle the full complexity of the sphere packing problem.

\subsection{truncation}

The distance from the center of the regular dodecahedron to a vertex is
$t_{dodec}=\sqrt{3}\tan(\pi/5)\approx 1.258$.   This parameter is used to truncate
Voronoi cells, to make their volumes easier to estimate.  A similar truncation
takes place in the solution to the sphere packing problem with truncation parameter
$t_0 = 1.255$.  It is a complete coincidence that these two truncation parameters 
are so close to one another.  (Section~\ref{sec:experiment} 
gives the reasons behind the parameter $t_0$.)  A great deal of duplicated effort
might have been avoided if these two parameters were equal.  However, the parameter
$t_{dodec}$ cannot be replaced with anything smaller, 
and although the parameter $t_0$ could 
easily have been made larger, 
its value was already too deeply entrenched in published papers 
by the time McLaughlin started work
on the dodecahedral conjecture.

As a result, there are many definitions in the proof of the dodecahedral theorem
and in the solution to the sphere packing problem, identical in every respect except
for the choice of parameter: $t_0,t_{dodec}$.  This is indeed unfortunate.  However,
there is no easy remedy.  As a first step towards unifying the proofs,
in revising the solution to the sphere packing problem for
this book, we have stated many results in a form that holds for all $t\in[t_0,t_{dodec}]$.
%As a matter of terminology, we will say something for the dodecahedral theorem
%is a perturbation of something in the solution of the sphere packing problem,
%if it is obtained by mindlessly replacing $t_0$ with $t_{dodec}$, wherever that% constant
%appears. 

\subsection{formulation}

Let $\Lambda$ be a sphere packing and let $v_0\in\Lambda$.
Let $\Lambda(v_0,r)$ be the set of points of $\Lambda$ at distance at most $r$ from $v_0$.
Let $B(v,r)$ be an open ball
of radius $r$ centered at $v$.  

Let $\CalQ_{dodec}(\Lambda,v_0)$ 
be the set of all $S=\{v_0,v_1,v_2,v_3\}\subset\Lambda(v_0,2t_{dodec})$
consisting of four distinct points such that $|v_i-v_j|\le 2t_{dodec}$ for all $i,j$
and such that the circumradius of each triangle $\{v_i,v_j,v_k\}\subset S$ is at most
$\sqrt2$.  A lemma (the dodecahedral analogue of Claim~\ref{thm:nonoverlap})
asserts the interiors of the convex hulls of distinct $S,S'\in\CalQ(v_0)$
are disjoint. Write $\op{conv}(S)$ for the convex hull of $S\in\CalQ_{dodec}(v_0)$.

Define a truncation $\Omega_{trunc}(\Lambda,v_0)$ 
of the Voronoi cell $\Omega(\Lambda,v_0)$ by the set
   $$
   \{x \in \Omega(\Lambda,v_0) \mid   x\in B(v_0,t_{dodec}) \quad\text{\bf  or }\quad x \in \op{conv}(S)
     \text{ for some } S\in \CalQ_{dodec}(v_0) \}. 
   $$
That is, we truncate the Voronoi cell by intersecting it with a ball of radius $t_{dodec}$,
except inside regions protected by the sets $S\in\CalQ_{dodec}(\Lambda,v_0)$.
Note that for the special packing $\Lambda_{dodec}$, we have 
$\Omega_{trunc}(\Lambda_{dodec},0) = \Omega(\Lambda_{dodec},0)$.

The strong form of the dodecahedral theorem is the following.

\begin{theorem}
For every packing $\Lambda$ and every $v_0\in\Lambda$,
   $$
   \op{vol}(\Omega_{trunc}(\Lambda,v_0))\ge \op{vol}(\Omega(\Lambda_{dodec},0)).
   $$
Equality holds exactly when $\Omega(\Lambda,v_0)$ is congruent to
$\Omega(\Lambda_{dodec},0)$.
\end{theorem}

Let us compare what we have done so far with what we will do in
 the solution of the sphere packing problem. In Definition~\ref{def:q-system}
 a collection of ``protected simplices''
$\CalQ(\Lambda,v_0)$ is defined.  A modification of the Voronoi cell (called the
$V$-cell) is defined.  A truncation of the $V$-cell is defined.
The main step in the solution of the sphere packing problem
is to show that the volume of a truncated $V$-cell, with modifications coming from
$\CalQ(\Lambda,v_0)$, is always at least as great as the volume of a rhombic dodecahedron.
So far the parallel is strong.

\subsection{few spheres}

L. Fejes T\'oth proved  the dodecahedral theorem 
under the extra hypothesis that $\Lambda(v_0,2t_{dodec})\setminus\{v_0\}$
has at most $12$  elements.  He never published the details when
there is truncation. Here
we sketch a proof of Fejes T\'oth's theorem.

\begin{lemma}  If $\Lambda(v_0,2t_0)\setminus\{v_0\}$ has at most
$12$ elements, then
     $$
   \op{vol}(\Omega_{trunc}(\Lambda,v_0))\ge \op{vol}(\Omega(\Lambda_{dodec},0)).
   $$
Equality holds exactly when $\Omega(\Lambda,v_0)$ is congruent to
$\Omega(\Lambda_{dodec},0)$.
\end{lemma}

\begin{proof} (Sketch) By translation, we may assume that $v_0=0$.
For each nonzero $v$ in $\Lambda(v_0,2t_0)$, 
let $v'=2v/|v|$,We have $|v'|\le|v|$.
The Voronoi cell $\Omega'$ at the origin for $X = \{v'\mid v\in\Lambda(0,2t_0)\}$ is a subset of $\Omega(\Lambda,v_0)$ (because face planes move to parallel planes closer to the origin).  
Set $\Omega'' = \Omega'\cap B(0,t_0)$.
Thus, it is enough to show
that 
   $$\op{vol}(\Omega'')\ge \op{vol}(\Omega(\Lambda_{dodec},0))$$
and to analyze cases of equality.

As in Leech's proof of $13$ spheres, 
take the Delaunay triangulation of $X$ (now on the sphere of radius $2$).
For each triangle, there is a triple of points $\{v_1,v_2,v_3\}\subset X$.
In Leech's proof, the area of the sphere
is a sum
of contributions from each face of the planar graph.
In this proof, the volume of $\Omega''$ is a sum of contributions
from each face of the planar graph.  Explicit formulas for the
contributions are developed later in this book.  They are as follows.
Let $t$ be the circumradius of $S=\{0,v_1,v_2,v_3\}$.  Set
   $$\sigma_{dodec}(S) =
     \begin{cases}
       \op{volan}(0,S) & t < t_{dodec}\\
       \op{volV}(0,S) & t\ge t_{dodec}\\
     \end{cases}
     $$
These two functions are defined in Section~\ref{XX}. 
They are given in completely explicit terms.
%% These functions don't seem to be defined yet.
%% volV should be the basic quoin, Adih formula with t_dodec truncation,
%% without regard for whether it is "geometric".
These functions are defined in such a way that
the sum of $\sigma_{dodec}$ for all Delaunay triangles is precisely
the volume of $\Omega''$.

The function $\sigma_{dodec}(\{0,v_1,v_2,v_3\})$ can be viewed as a function of three
variables $y_i = |v_j-v_k|$, for $(i,j,k)=(1,2,3),(2,3,1),(3,1,2)$.
The area $A$ of the Delaunay triangle can also be viewed as a function of
$y_1,y_2,y_3$.
We hold $y_1$ constant, vary $y_2$, and define $y_3$ as an implicit
function of $y_2$ on a level curve of $A(y_1,y_2,y_3)$.

By computing the derivative of $\sigma_{dodec}(y_1,y_2,y_3)$ along
the level curve, we see that if $y_2 < y_3$, then $\sigma_{dodec}$
is decreasing in $y_2$.  (This involves two calculations, one
when $t<t_{dodec}$ and another when $t\ge t_{dodec}$.)  Thus, we 
may decrease volume for constant Delaunay triangle areas by making
$y_1=y_2=y_3$.\endnote{The code for the dodecahedral problem for $12$ spheres
appears in a Mathematica Notebook at XX.  This notebook supplements the
arguments in the text 
with calculations that show that the circumradius of the simplex
is properly behaved under these deformations.}
(The triangles no longer fit together, but this
does not matter.  Their areas still sum to $4\pi$.)

By computing the derivative of $\sigma_{dodec}(y,y,y)+\sigma_{dodec}(z,z,z)$
along a level curve of $A(y,y,y)+A(z,z,z)$, we see that if $y<z$,
then the volume may be decreased by increasing $y$ (and decreasing $z$).
(This involves three calculations, depending how the two circumradii
compare with $t_{dodec}$.)  Thus, we may decrease volume by
making all triangles congruent and equilateral.  The areas of the
triangles still sum to $4\pi$.  

If $V$ is the cardinality
of $X$, then by the Euler formula the number $F$ 
of triangles is $(2V-4)$, which is at most $20$,
and each triangle has area $4\pi/F$.  Our bound on volume is
   $$
   F\, \sigma_{dodec}(y,y,y), \text { where } F\, A(y,y,y)=4\pi.
   $$
Extend $F$ as a real-valued variable, and
let the area formula define $y$ as an implicit function of $F$.
The function $F\sigma_{dodec}(y(F),y(F),y(F))$ 
is decreasing in $F$, so a lower bound is obtained
when $F$ is as large as possible.  The lower bound 
$20\,\sigma_{dodec}(y(20),y(20),y(20))$ equals the volume of
$\Omega(\Lambda_{dodec},0)$ (as the Delaunay triangulation
for $\Lambda_{dodec}$ also consists of $20$
congruent triangles with total area $4\pi$).  
This proves the bound. 
All the derivatives involved in these
calculations are explicit.  Hence the case of equality is easily
traced.
\end{proof}

Fejes Toth's proof uses simple convexity arguments to prove an
apparently difficult result.  This method of proof has become
 is an essential part of the solution to the sphere packing
problem.   Ferguson's thesis calculates the volume of a truncated
Voronoi cell, subject to a fixed area constraint 
\cite[sec.~16.9.5]{Fer97}.  In fact, his thesis provides details
to the sketch provided above, by giving the derivatives explicitly,
for truncated Voronoi cells.  Many of his calculations have been
reproduced in this book.  See, in particular, 
Lemma~\ref{lemma:quoin-equilize}.


\subsection{numerous spheres}

Completely different methods treat the case when $\Lambda(v_0,2t_{dodec})\setminus\{v_0\}$ contains at least $13$ points.  This part of the proof is
much more difficult than the case treated by Fejes T\'oth.
Although the details are
entirely different, the top-level description of the proof of the
dodecahedral conjecure sounds strickingly similar to the top-level
description of the solution to the sphere packing problem.  Here
we describe the common top-level structure.  We use substantially
the same language that was used to give a summary of Leech's proof
of the kissing number problem in three dimensions in Section~\ref{sec:summary}.

Let $\Lambda$ be a sphere packing, fix $v_0\in\Lambda$.
Let $t$ be the truncation
parameter, which $t_{dodec}=1.258\ldots$ here, and  $t_0=1.255$ for the
sphere packing problem.  We seek to minimize the objective function 
$$
\op{vol}(\Omega_{trunc}(\Lambda,v_0)).
$$
The target value for the minimization is $b=\op{vol}(\Omega(\Lambda_{dodec},0))$.

As in Leech's proof, a planar graph is used to partition the
unit sphere.  It is formed as follows.  Consider all sets
with three elements $\{v_0,v_1,v_2\}\subset \Lambda(v_0,2t)$
such that $|v_1-v_2|\le 2t$.  Form a {\it blade}:
   $$\{v_0 + s_1 (v_1-v_0) + s_2 (v_2-v_0) \mid s_1,s_2\ge 0\}$$
from each triple $\{v_0,v_1,v_2\}$.  The complement of the union of
blades breaks into finitely many connected componenents  $U_1,\ldots,U_r$
called {\it standard components}.    The combinatorial structure of
the blades determines a planar graph (or more precisely a hypermap as
defined in Chapter~\ref{XX}).  When the graph is connected, the
set of standard components is in bijection with the set of faces of the
planar graph (Lemma~\ref{XX}).  For simplicity assume that the graph
is connected.


An objective function is defined.  The objective function is a sum
of contributions from each face of the planar graph.  For the dodecahedral
problem, these contributions
are defined as $$\op{vol}(\Omega_{trunc}(\Lambda,v_0)\cap U_i).$$

A computer calculates bounds
on the contribution from each face.  In an assembly phase of the proof, 
the target total contribution $b$
is compared
with to the sum of bounds assembled from different faces. This comparison places
strong combinatorial restrictions on the planar graph.  A graph that satisfies these restrictions is called a tame planar graph.  (There is one class of tame planar graphs for the dodecahedral problem and a second class of graphs for the sphere packing problem. Calling them by the same name is an abuse of language.)  A computer classification of all 
tame planar graphs shows what possibilities remain.   

Case-by-case analysis excludes the final possibilities.  A linear program is
attached to each tame planar graph.  This linear program is a linear relaxation
of a  nonlinear optimization problem: minimize the objective function
 constrained to having the combinatorics of the given planar graph.
The linear programming bound gives a bound on the nonlinear optimization problem. In most cases, this linear programming bound is good enough to exclude
the tame planar graph as potential counterexample.  When this linear program does not  bound the objective function away from $b$, 
it is replaced with a finite sequence of linear
programs, obtained by branch and bound methods.  
Every tame graph is excluded in this way.  
No packing can have a Voronoi cell with volume smaller than that
of the regular dodecahedron.  The Voronoi cells of minimal volume are all
congruent to a regular dodecahedron.


\subsection{differences}

Although the proof of the dodecahedral theorem runs parallel
to the solution to the sphere packing problem, there are special
difficulties that arise in the proof of the dodecahedral theorem.
In no sense is it a corollary of the sphere packing problem.
In the sphere packing problem, there turn out to be many ways to
reduce the infinite sphere problem to a problem about finite clusters
of spheres.  Through this multiplicity of reductions, it is
possible to design many difficulties away.  If one reduction is
not satisfactory, one can walk away from it and work with
another.
With the dodecahedral problem, there is no such flexibility.
The problem about finite clusters of spheres is fixed from the outset.

\clearpage






\section{Experiments}

\label{sec:experiment}





This essay describes some of the
motivation behind the partitions of space that have been used in the
solution to the sphere packing problem.  This discussion includes various
ideas that were tried, found wanting, and discarded. This
discussion also provides motivation for some of the choices that appear
in the solution to the sphere packing problem.

An introduction to the ideas of the proof can be found in
\cite{CH}. An introduction to the algorithms can be found at
\cite{algorithm}. Speculation on a second-generation design of a
proof can be found in \cite{algorithm} and \cite{arbeitstagung}.
Around the time those articles were written, I had plans for
a second-generation proof, but the complexities were significant,
and I backed away.  The proof in this book contains the same gut
as the 1998 proof, even if significant  cosmetic changes 
mask the common origin.



Let $S$ be a regular tetrahedron of side length $2$.  If we place a
unit ball at each of the four vertices, the fraction of the
tetrahedral solid occupied by the part of the four balls within the
tetrahedron is $\dtet\approx 0.7797$. Let $O$ be a regular
octahedron of side length $2$.  If we place a unit ball at each of
the four vertices, the fraction of the octahedral solid occupied by
the four balls is $\doct\approx 0.72$. The face-centered cubic
packing can be obtained by packing eight regular tetrahedra and six
regular octahedra around each vertex. The density $\pi/\sqrt{18}$ of
this packing is a weighted average of $\dtet$ and $\doct$:
    $$\frac\pi{\sqrt{18}} = \frac13\dtet + \frac23\doct.$$

My early conception (around 1989) was that for every packing of
congruent balls, there should be a corresponding partition of space
into regions of high density and regions of low density. Regions of
high density should be defined as regions having density between
$\doct$ and $\dtet$, and regions of low density should be defined as
those regions of density at most $\doct$.  It was my intention to
prove that all regions of high density had to be confined to a set
of nonoverlapping tetrahedra whose vertices are centers of the balls
in the packing.  One could then translate results about the sparsity
of high density tetrahedra into bounds on density 
(Figure~\ref{XX}). %not drawn


Thus, the question naturally arises of how much a regular
tetrahedron of edge length $2$ can be deformed before its density
drops below that of a regular octahedron $\doct$.  The following
graph (Figure~\ref{fig:t51}) shows the density of a tetrahedron with
five edges of length $2$ and a sixth edge of length $x$.
Numerically, we see that the density drops below $\doct$, when
$x=x_0\approx 2.504$. To achieve the design goal of confining
regions of high density to tetrahedra, we want a tetrahedron of edge
lengths $2,2,2,2,2,x$, for $x\le x_0$, to be counted as a region of
high density. Rounding upward, this example led to the cutoff
parameter of $2.51$ that distinguishes the tetrahedra (in the high
density region) from the rest of space. This is the origin of the
constant $2.51$ that appears in the proof.


\begin{figure}[htb]
  \centering
  \myincludegraphics{\ps/t51.eps}
  \caption{The origin of the constant $2.51$.}
  \label{fig:t51}
\end{figure}

Since the tetrahedra are chosen to have vertices at the centers of
the balls in the packing, it was quite natural to base the
decomposition of space on the Delaunay decomposition. According to
this early conception, space was to be partitioned into Delaunay
simplices.  A Delaunay simplex whose edge lengths are at most
$2.51$ is called a quasi-regular tetrahedron.  These were the
regions of presumably high density.  According to the strategy in
those early days, all other Delaunay simplices were to be shown to
belong to regions of density at most $\doct$.

The following problem occupied my attention for a long period.


\smallskip\noindent
{\bf Problem} Fix a saturated packing. Let $X(oct)$ be the part of
space of a saturated packing that is occupied by the Delaunay
simplices having at least one edge of length at least $2.51$.  Let
$X(tet)$ be the union of the complementary set of Delaunay
simplices.  Is it always true that the density of $X(oct)$ is at
most $\doct$?

Early on, I viewed the positive resolution of this problem as
crucial to the solution to the sphere packing problem.  Eventually,
when I divided the solution of the sphere packing problem into a five step
program, a variant of this problem became the second step of the
program. See \cite{part2}.

To give an indication of the complexity of this problem, consider
the simplex with edge lengths $(2,2,2,2,\ell,\ell)$, where $\ell =
\sqrt{2 (3 + \sqrt6)}\approx 3.301$.  Assume that the two longer
edges meet at a vertex.  This simplex can appear as the Delaunay
simplex in a saturated packing.  Its density is about $0.78469$.
This constant is not only greater than $\doct$; it is even greater
than $\dtet$, so that the problem is completely misguided at the
level of individual Delaunay simplices in $X(oct)$.  It is only in
when the union of Delaunay simplices is considered that we can
hope for an affirmative answer to the problem.

By the summer of 1994, I had lost hope of finding a partition of
the set $X(oct)$ into small clusters of Delaunay simplices with
the property that each cluster had density at most $\doct$.
Progress had ground to a halt.   The key insight came in the fall
of 1994 (on Nov 12, 1994 to be precise). On that day, I introduced
a hybrid decomposition that relied on the Delaunay simplices in
the regions $X(tet)$ formed by quasi-regular tetrahedra, but that
switched to the Voronoi decomposition in certain regions of
$X(oct)$. By April 1995, I had reformulated the problem, worked
out a proof of the problem \cite{part2} in its new form, and
submitted it for publication. I submitted a revised version of
\cite{part1} that same month.  The revision mentions the new
strategy: ``The rough idea is to let the score of a simplex in a
cluster be the compression $\Gamma(S)$ [a function based on the
Delaunay decomposition] if the circumradius of every face of $S$
small, and otherwise to let the score be defined by Voronoi cells
(in a way that generalizes the definition for quasi-regular
tetrahedra).'' See \cite[p.6]{part1}.

The situation is somewhat more complicated than the previous
paragraph suggests. Consider a Delaunay simplex $S$ with edge
lengths $(2,2,2,2,2,2.52)$. Such a simplex belongs to the region
$X(oct)$. However, if we break it into four pieces according to
the Voronoi decomposition, the density of the two of the pieces is
about $0.696<\doct$ and the density of the other two is about
$0.7368>\doct$. It is desirable not to have any separate regions
in $X(oct)$ of density greater than $\doct$.  Hence it is
preferable to keep the four Voronoi regions in $S$ together as a
single Delaunay simplex.  A second reason to keep $S$ together is
that the proof of the local optimality of the face-centered cubic
packing and hexagonal close packing seems to require it.  A third
reason was to treat pentahedral prisms.  (This is a thorny class
of counterexamples to a pure Delaunay simplex approach to the
solution to the sphere packing problem.  See \cite{spp}, \cite{remarks},
and \cite{Fer97}.)  For these reasons, we identify a class of
Delaunay simplices in $X(oct)$ (such as $S$) that are to be
treated according to a special set of rules. They are called {\it
quarters}.  As the name suggests, they often occur as the four
simplices comprising an octahedron that has been ``quartered.''

One of the great advantages of a hybrid approach is that there is
a tremendous amount of flexibility in the choice of the details of
the decomposition.  The details of the decomposition continued to
evolve during 1995 and 1996.  Finally, during a stay in Budapest
following the Second European Congress in 1996, I abandoned all
vestiges of the Delaunay decomposition, and adopted definitions of
quasi-regular tetrahedra and quarters that rely only on the metric
properties of the simplices (as opposed to the Delaunay criterion
based on the position of other sphere centers in relation to the
circumscribing sphere of the simplex).  This decomposition of
space is essentially what is used in the final proof.

The hybrid construction depends on certain choices of functions
(satisfying a rather mild set of constraints).  To solve the
sphere packing problem appropriate functions had to be selected, and an
optimization problem based on those functions had to be solved.
This function is called {\it the score}.  Samuel Ferguson and I
realized that every time we encountered difficulties in solving
the minimization problem, we could adjust the scoring function
$\sigma$ to skirt the difficulty.  The function $\sigma$ became
more complicated, but with each change we cut months -- or even
years -- from our work.  This incessant fiddling was unpopular
with my colleagues.  Every time I presented my work in progress at
a conference, I was minimizing a different function.  Even worse,
the function was mildly incompatible with what I did in earlier
papers \cite{part1} \cite{part2}, and this required going back and
patching the earlier papers.

The definition of the scoring function $\sigma$ did not become
fixed until it came time for Ferguson to defend his thesis, and we
finally felt obligated to stop tampering with it.  The final
version of the scoring function $\sigma$ is rather complicated.
The reasons for the precise form of $\sigma$ cannot be described
without a long and detailed description of dozens of sphere
clusters that were studied in great detail during the design of
this function. However, a few general design principles can be
mentioned.  These comments assume a certain familiarity with the
design of the proof.


(1) Simplices (with vertices at the centers of the balls in the
packing) should be used whenever careful estimates of the density
are required.  Voronoi cells should be used whenever crude
estimates suffice.  For Voronoi cells, it is clear what the
scoring function should be.



(2) The definition of the scoring function for quasi-regular
tetrahedra was fixed by \cite{part1} and this definition had to
remain fixed to avoid rewriting that long paper.

Because of these first two points, most of the design effort for
the function $\sigma$ was focused on quarters.

(3)  The decision to make the scoring for a quarter change when
the circumradius of a face reaches $\sqrt2$ is to make the proof
of the local optimality of the fcc and hcp packings run smoothly.
From \cite{part2}, we see that the cutoff value $\sqrt2$ is
important for the success of that proof.  The cutoff $\sqrt2$ is
also important for the proof that standard components (other than
quasi-regular tetrahedra) score at most $0\,\pt$.

(4) The purpose of adding terms to the scoring function $\sigma$
is to make
interval arithmetic comparisons
easier to carry out.  This is useful in arguments about ``erasing
upright quarters.''



\subsection{complexity}

Why is this a difficult problem?  There are many ways to answer this
question.

This is an optimization problem in an infinite number of
variables.  In many respects, the central problem has been to
formulate a good finite dimensional approximation to the density
of a packing.  Beyond this, there remains an extremely difficult
problem in global optimization, involving nearly 150 variables.
We recall that even very simple classes of nonlinear optimization
problems, such as quadratic optimization problems, are NP-hard
\cite{HoPT95}.  A general highly nonlinear program of this size is
regarded by most researchers as hopeless (at least as far as
rigorous methods are concerned).

There is a considerable literature on many closely related nonlinear
optimization problems (the Tammes problem, circle packings, covering
problems, the Lennard-Jones potential, Coulombic energy minimization
of point particles, and so forth). Many of our expectations about
nonlattice packings are formed by the extensive experimental data
that have been published on these problems. The literature leads one
to expect a rich abundance of critical points, and yet it leaves one
with a certain skepticism about the possibility of establishing
general results rigorously.

The extensive survey of circle packings
in \cite{Mel97} gives a broad overview of the progress and limits
of the subject.
Problems involving a few circles can be
trivial to solve.
Problems involving several circles in the plane can be solved with
sufficient ingenuity.
With the aid of computers, various  problems involving
a few more circles can be
treated by rigorous methods.
Beyond that, numerical methods
give approximations but no rigorous solutions.
Melissen's account of
the 20-year quest for the best separated arrangement of 10
points in a unit square is particularly revealing of the complexities
of the subject.

Sphere packings have a particularly rich collection of (numerical)
local maxima that come uncomfortably close to the global maximum
\cite{spp}. These local maxima explain in part why a large number
(around 5000) of planar maps are generated as part of the proof of
the conjecture.  Each planar map leads to a separate nonlinear
optimization problem.








\chapter{Essays on Computation}


As this project  progressed, the computer  replaced conventional
mathematical arguments more and more, until now
 nearly every aspect of the proof relies on
computer verifications.  Many assertions in this book
 are results of computer calculations.
To make the solution more accessible, I have
posted extensive resources \cite{web}.

There are three major pieces of computer code that enter into the proof.

\begin{itemize}
\item  {\it Combinatorics}.  A computer program classifies all of the planar hypermaps
that are relevant to the sphere packing problem.
\item {\it  Interval analysis}.  ``Sphere Packings
I'' describes a method of proving various inequalities in a small number
of variables by computer by interval arithmetic.
\item {\it  Linear programming}.  Many of the nonlinear optimization
    problems for the scores of centered packings are replaced by linear
    problems that dominate the original score.  They are solved
    by linear programming methods by computer.  A typical problem has
    between 100 and 200 variables and 1000 and 2000 constraints.  Nearly
    100000
    such problems enter into the proof.
\end{itemize}

Computers are used in various other ways.  They will be
discussed in the essays in this chapter.


\begin{itemize}
\item {\it Formal proof}.
    Various parts of the solution of the sphere packing problem have now been
    formally verified.
\item  {\it Numerical optimization}.  The exploration of the problem
    has been substantially
    aided by nonlinear optimization and symbolic math packages.
\item {\it Branch and bound methods}.  When linear programming methods do not
    give sufficiently good bounds, they have been combined with branch
    and bound methods from global optimization.
\item {\it Computer Algebra Systems}  Mathematica was used for many minor calculations,
  such as the calculation of exact  explicit formulas for derivatives.
\item {\it Organization of output}.
    The organization of the few gigabytes of code and data that
    enter into the proof is in itself a nontrivial undertaking.
\end{itemize}



\section{Formal Proof}\FIXX{Insert}

\clearpage
\section{Tame Hypermaps}



\section{Interval Analysis}%DCG 8.3, p75
\label{sec:bounds-simplex}

Interval analysis is a method to obtain trustworthy results from a computer.
This essay gives a basic introduction to this method.

\subsection{deliberate error}

Interval arithmetic can trace its origins to the method of deliberate error.
This is an ancient method of navigation with imperfect instruments.  When William
the Conqueror
crossed the English Channel in 1066, he deliberately steered to the north of Hastings.

\begin{quote}
%``There is no direct evidence of how a twelfth-century pilot found his way across
%the English Channel $\ldots$
``Any pilot even now who had to make that crossing without a chart or compass,
as William's pilots did, would use the ancient method of Deliberate Error: he would
not steer directly towards his objective but to one side of it, so that when he
saw the coast he would know which way to turn.'' \cite[p81]{How81}
% 1066: The Year of the Conquest, Penguin paperback edition 1981. page 148.
\end{quote}

Deliberate error implements ``better safe than sorry.''
To find an address on a one-way street,  a  driver enters the
street too early, to point in the right direction.  
When searching for a familiar landmark
on a two-way street, it is more efficient  
to start the search safely to one side of the landmark, 
rather than search in ever
expanding zigzags in both directions.  
Deliberate error 
is the idea of trapping a target inside a large enough net, in the
way that
Khachiyan traps the optimal solutions of a linear program inside
an ellipsoid.


The method of deliberate error cushions the adverse effects of imperfect technology.
The method does not aim to minimize the imperfections.  It works with a faulty
chart and compass.
%The method of deliberate error seeks not to minimize the error of imperfect technolo

\subsection{arithmetic}

The method of deliberate error, implemented to control for round-off errors on a computer,
leads to interval arithmetic.  To approximate the circumference of a circle, Archimedes inscribes a polygon and circumscribes a second polygon, trapping the unknown within a known interval.  This is interval arithmetic.


Fixed precision 
floating point numbers on a computer exactly represent a finite
number of rational numbers.  The remaining uncountable set of real numbers cannot
be precisely represented.  For example, 64-bit floating point numbers can encode at most
$2^{64}$ different real numbers.    This finite set of representable numbers is not
closed under addition, multiplication, subtraction, or division.  For example,
the input $2.0 + 2.0$ returns $4.0$, because all the numbers involved
are precisely representable.  However, through round-off error, my computer returns 
\begin{verbatim}0.0
\end{verbatim} 
in response to
\begin{verbatim}
  -1.0 + (1.0 + 1.0e-16)
\end{verbatim}
and 
\begin{verbatim}1.0e-16
\end{verbatim} 
in response to 
\begin{verbatim}
  (-1.0 + 1.0) + 1.0e-16.
\end{verbatim}
As this example shows, machine addition is not even associative.  We would quickly be led to absurd
conclusions, if we were to try to reason about floating point operations as if
they formed a group, a ring, or a field.  To reason correctly about floating point, we
must accept these operations on their own terms.  To distinguish the imprecise
machine (floating point) operations from standard operations on the field of real numbers, in the rest of this section (except in computer code listings), 
we place a dot over the floating point operations $(\dot +)$, $(\dot -)$, and so forth.

The precise behavior of floating point operations on a computer is governed by 
the IEEE-754 floating point standard \cite{Gol}.  
By giving a precise specification of properties
of floating point arithmetic, the standard makes it possible to reason about the
behavior of floating point.  It is possible to prove theorems about the behavior
of IEEE floating point.  For example, it is possible to prove that a computer that
correctly implements the standard should return the values in reponse to the inputs
given above (in the nearest rounding mode).  

Let $F$ be the set of machine-representable floating point numbers.  We may assume that $F$ contains two 
 special symbols $\pm\infty$.  The total order on the real numbers is extended to $\ring{R}\cup\{\pm\infty\}$, with $-\infty < x < \infty$ for all $x\in\ring{R}$.
Because of these two special symbols, the floating point floor $x\to\floor{x}_F\in F$ and ceiling $x\mapsto\ceil{x}_F\in F$ functions, with domain $\ring{R}$ can be defined.   In this section, we drop the subscript $F$ on the
floor and ceiling functions.
We map the set of real numbers to $F^2$, by
sending $x$ to $[a,b]$, where $a = \floor{x}$ and $b=\ceil{x}$.
If $x$ is Hastings, then $b$ is a point
on the shore north of Hastings.  It is the deliberate error to one side of the target.  





On most modern processors, the rounding mode can be set to directed rounding, as
described in the standard.   When the rounding mode is set upward, the result of
any basic arithmetic floating-point operation $(\dot +)$, $(\dot -)$, $(\dot *)$, $(\dot /)$ applied to two floating
point numbers $(x,y)\mapsto x\dot\diamond y$ is defined by standard to be 
the $\ceil{x\diamond y}$.  That is, make the calculation in the field of real numbers and round up to the next floating point.
When the rounding mode is set downward, 
set $x\dot\diamond y = \floor{x\diamond y}$.  The notation $\dot\diamond$ does not
show rounding mode, but it should, because it is mode dependent.


Interval arithmetic, like the method of deliberate error, does not seek to eliminate
the sources of floating point round off error.  Rather it brings it under scientific
control.  


Let $I_F = \{[a,b] \in F^2 \mid a \le b\}$ be the set of floating point intervals.
Basic machine operations extend to intervals.  The sum $[a_3,b_3]$ of
$[a_1,b_1]$ and $[a_2,b_2]$, is defined as $a_3=\floor{a_1+a_2}$ and
$b_3 = \ceil{b_1+b_2}$.  Write $[a_3,b_3] = [a_1,b_1] \dot+ [a_2,b_2]$.
This addition of intervals
is not associative.  However, addition of intervals satisfy a crucial inclusion property.
If $x\in[a_1,b_1]$ and $y\in [a_2,b_2]$, and $z = x+y$,
then $z\in [a_3,b_3]$.  The other arithmetic operations can be extended to machine
interval operations in a similar way, so as to satisfy the corresponding inclusion
properties.    Division requires special treatment when $0$ belongs to an interval
denominator.  We will not go into those details here.  

These operations are easily implemented in code.  For example, here is the actual snippet of 
{\tt C++} code that implements the addition of intervals.  The functions
{\tt interMath::up()} and {\tt interMath::down()} set the rounding modes on the 
computer.  
\begin{verbatim}
inline interval interval::operator+(interval t2) const
        {
        interval t;
        interMath::up(); t.hi = hi+t2.hi;
        interMath::down(); t.lo = lo+ t2.lo;
        return t;
        }
\end{verbatim}

By implementing interval operations on a computer, we can develop a procedure that
takes as input 
an arbitrary arithmetic expression over the rational numbers and returns an
interval $[a,b]$ with floating point endpoints (augmented by $\pm\infty$ as usual)
that contains the rational value of that expression.
The true answer, still unknown, lies trapped between the interval endpoints.
Since the associative and distributive laws fail, the returned interval
 $[a,b]$ depends on the actual syntax of the expresssion, on  the placement of parentheses,
and so forth.



\subsection{analysis}

This essay is not a treatise on interval arithmetic.  Its purpose
is to give an introduction, to demonstrate the possibility of rigorously
bounding the error of floating point performed according to IEEE standards.
We become increasingly sketchy.

The theory of interval analysis progresses step by step, starting with
basic arithmetic and developing through higher levels of analysis.
If $p$ is a polynomial with rational coefficents $p\in\ring{Q}[x_1,\ldots,x_n]$, then it has an interval
extension $\bar p : I_F^n \to I_F$.  Just as in the case of rational
numbers, the interval extension $\bar p$ depends on the syntax used to
expressed to polynomial $p$.    The polynomial extension is {\it monotonic}:
  if $z_i\in [a_i,b_i]$ for all $i$, then
$p(z_1,\ldots,z_n) \in \bar p([a_1,b_1],\ldots,[a_n,b_n])$.
Interval extensions of rational functions is similar.

Consider a function $f:[a,b]\to\ring{R}$ that
has a rational function approximation $r$ with known
error bound:  $|f(x) - r(x)|<\epsilon$ for $x\in[a_1,b_1]$,
with $\pm\epsilon\in F$.
Define a monotonic interval extension $\bar f$ of $f$ by
$\bar f([c,d]) = \bar r([c,d]) \dot + [-\epsilon,\epsilon]$, for
$[c,d]\subset [a,b]$.  
Multivariate functions are similar.   
%
A large class of analytic functions fall within this framework.
We have interval extensions of trigonometric functions, logs, and
exponentials.  Interval extensions of real-valued functions can be
added, multiplied and composed.  

To prove that a function $f$ is positive on a rectangular domain
$[a_1,b_1]\cdots[a_n,b_n]$, compute the
value of an interval extension $[c,d]=\bar f([a_1,b_1]\cdots[a_n,b_n])$.
By the monotonic property of interval extensions, if $c>0$, then $f$
is positive on the domain.  If this is done naively, with the
first interval extension $\bar f$ that comes to mind, the image interval
$[c,d]$ may be so large that no worthwhile information results.
If we naively 
compute $x+(-x)$ by interval arithmetic for $x$ in the interval $[-1,1]$,
we find that $x+(-x)$ lies in the interval sum
$[-1,1]\dot + [-1,1] = [-2,2]$.  Useless!
There is a large mathematical literature describing various 
efficient algorithms
to compute image intervals $[c,d]$ with accuracy.
Kearfott's book is a recommended starting point, because it comes
close to describing the types of algorithms implemented for the
solution to the sphere packing problem \cite{Kea96}. 



\subsection{archive}

Although this book is long,  it represents only a fraction of the solution of
the sphere packing problem.  The other resources that are needed to understand
the full solution are available on the internet.

There is an archive of  several hundred
inequalities that have been proved by computer.  The list of inequalities can
be found at \cite{web}.\endnote{The archive of interval arithmetic
inequalities
appears at 
http://flyspeck.googlecode.com/svn/trunk/inequalities/kep\_inequalities.ml.
There are now multiple copies of this file on the internet.  This particular
URL is preferred because it is under active version control.  Other copies are now
out of date.}    
The list of inequalities are in computer readable
form in the rigorous mathematical syntax of HOL-Light (for Higher Order Logic).

For example, one of the inequalities from that file reads as follows.
\begin{verbatim}
let I_572068135=
   all_forall `ineq 
    [((square (#2.3)), x1, (#6.3001));
     ((#4.0), x2, (#6.3001));
     ((#4.0), x3, (#6.3001));
     ((#4.0), x4, (#6.3001));
     ((#4.0), x5, (#6.3001));
     ((#4.0), x6, (#6.3001))
    ]
    ((((tau_sigma_x x1 x2 x3 x4 x5 x6) -.  ((#0.2529) *.  
         (dih_x x1 x2 x3 x4 x5 x6))) >. (--. (#0.3442))) \/ 
     ((dih_x x1 x2 x3 x4 x5 x6) <.  (#1.51)))`;;
\end{verbatim}
The first part of this snippet of code gives lower and upper bounds on each of the six variables $x_1,\ldots,x_6$.  The final part of this code gives an inequality (or rather a disjunction of two inequalities) of nonlinear real-valued functions that holds on the given domain.
The $\#$ symbol marks exact decimal constants.  The functions {\tt dih\_x}, {\tt tau\_sigma\_x} are defined rigorously in a separate file.  They correspond to the functions 
$\dih$ and $\tau$ in this book.  The symbols for arithmetic operations are followed by periods ($*.$ and so forth) to distinguish them from the corresponding operations on natural numbers.  

The inequality carries a nine-digit identifier {\tt 572068135}.    This number
is a tracking number that can be used in a search engine to locate everything known
about a given inequality.
For example,
if one googles this number, the search engine returns 
nine matches related to this inequality, including
a preprint on the arXiv, the website of the Annals of Mathematics, a Springer Link
to the relevant issue of the journal Discrete and Computational Geometry, a flyspeck
discussion group, the {\it C++} computer code proving the inequality, and the output
file from running that code.  A search on the pdf file of this book links this inequality to
Lemma~\ref{lemma:11.16}.  Every interval arithmetic calculation in this book carries
a nine-digit identifier, for easy tracking.

The interval arithmetic code that proves the inequalities is available on the arXiv,
at the permanent website of the Annals, and at my university website.  
Currently, the most convenient
way to access it perhaps is through the Google's Code Search, 
a custom search engine for computer code.\endnote{The interval arithmetic code that proves the inequalities is available at
 http://code.google.com/p/flyspeck/ Click on the link that reads
{\it code for 1998 proof}}.  

% mentioned already below.
%Sean McLaughlin has an ongoing project to give an independent verification of all of the code in the proof of the sphere packing problem.  The current version of that code is maintained at \cite{McL08}.
%{\tt http://code.google.com/p/kepler-code/}.

Further information about proving these inequalities by interval arithmetic can
be found in \cite{algorithm} and
\cite{part1}. 
\index{Index}{calc@\calc{123456789}}

\subsection{code verification}

S. Ferguson and I put considerable effort into developing trustworthy
code.  The two of us made entirely independent implementations of
the code. %automated inequality proving by interval arithmetic. 
(We shared algorithms
but not source code.)  This allowed us to check our answers against each
other to ensure mutual consistency, 
and to eliminate certain sources of bugs.  We also made independent
implementations in Mathematica and {\tt C} of traditional floating point
versions of the functions used for the nonlinear
optimizations.  By requiring these different implementations to give
compatible answers, we eliminated further sources of bugs.

Each nonlinear inequality was also checked independently with the nonlinear
optimization package {\tt cfsqp}.  This is a collection of {\tt C} routines
that searches for the minimum of a smooth function on a domain described
by a system of constraints.  As is the case with many nonlinear optimization
packages, there is no guarantee that the search will converge to
the true global minimum of the function.   By repeating the search
with a large collection of initial values for the search, it becomes
more probable that the true global minimum will be found.
In practice it works remarkably well.

This package was not used
in any proofs.  The numerical testing with {\tt cfsqp} was used to
discover false inequalities before they were shipped to the interval
arithmetic prover for verification.  Those that failed never shipped.
This extra level of testing adds an extra level of robustness to
this part of the proof.  Testing gives us (nonrigorous)
reasons to believe the inequalities, even if a bug should appear in
our interval arithmetic code.  This gives us some hope that an undetected bug
would be unlikely to affect the overall design of the solution to
the sphere packing problem.


There are certain types of bugs that would be very difficult to detect.
For example, since floating point arithmetic is not associative, a
misplaced pair of parentheses might throw a calculation off by a
machine epsilon.  
This potential source of bugs is evidence that the entire project 
was not sufficiently
automated:  the parentheses should all have been worked out automatically.
To make the calculations more robust, we have tried
to design the collection of inequalities so that they hold with a considerable
margin of error, rather than just squeeze by.  (There are only a few
inequalities that are sharp, and they are sharp for clear mathematical
reasons related to the theory of sphere packings.)  In a bug-free environment,
such precautions would not be necessary.  Nonetheless, we take precautions.

As far as I know, no comprehensive efforts were made by the referees and
editors to check the correctness of the computer code before the
publication of the 1998 solution to the sphere packing problem.  The editors' preface
to \cite{DCG} states that during the review process
``some computer experiments were done in a detailed check.''


The flyspeck project is a long-term project intended to make the solution
to the sphere packing problems one of the most thoroughly checked computer
proofs of all times.  Part of this project calls for a formal proof of correctness of the
computer code used in the interval verification of inequalities.

Flyspeck is still far from completion in 2008.  Nevertheless, 
there are various ongoing
projects
related to this second-generation verification of the interval code.
S. McLaughlin has an independent implementation of the interval arithmetic
code used in the sphere packing problem~\cite{McL08}.  
His work has exposed some data-entry bugs.  They are reported
in the comprehensive errata to the 1998 proof, 
which is maintained at \cite{errata} with additional discussion at~\cite{flydis}.  
No bugs have surfaced
over the past decade 
in the underlying interval arithmetic inequality proving algorithms.  The
reported errors have been at the data-entry level:  a mismatch between 
the data as typed
into the preprint and data in computer code.
\FIXX{All interval arithmetic should be rerun one final time before this book is published.}

One of the formal proof assistants under most active development
is the COQ system~\cite{COQ}.  R. Zumkeller
has implemented automated inequality proving with interval arithmetic
inside the theorem prover COQ, with the flyspeck project in
mind; although (as of 2008) the inequalities
that are used in this book have not yet been checked in this way \cite{Zu}.


\subsection{interval analysis and proof}


The editors of the Annals of Mathematics have posted a statement on computer-assisted proof.
At first, the editors planned to make a disclaimer directed at the
computer solution of the sphere packing problem.  Eventually,  they formulated a general policy
on computer-assisted proofs.  The policy mentions interval arithmetic as one way to
control sources of computer error.


\begin{quote}
%Statement by the Editors on Computer-Assisted Proofs

``Computer-assisted proofs of exceptionally important mathematical theorems will be considered by the Annals.

``The human part of the proof, which reduces the original mathematical problem to one tractable by the computer, will be refereed for correctness in the traditional manner. The computer part may not be checked line-by-line, but will be examined for the methods by which the authors have eliminated or minimized possible sources of error: (e.g., round-off error eliminated by interval artihmetic, programming error minimized by transparent surveyable code and consistency checks, computer error minimized by redundant calculations, etc. [Surveyable means that an interested person can readily check that the code is essentially operating as claimed]).

``We will print the human part of the paper in an issue of the Annals. The authors will provide the computer code, documentation necessary to understand it, and the computer output, all of which will be maintained on the Annals of Mathematics website online.'' \cite{Ann06}

%http://annals.princeton.edu/EditorsStatement.html
\end{quote}

A number of proofs in pure and applied mathematics have been based on interval analysis.  W. Tucker implemented a rigorous ODE solver with interval
arithmetic and used it to prove that the Lorenz equations
have a strange attractor \cite{Tuc02}. The existence of strange attractors is problem 14 on Smale's list
of 18 Centennial Problems \cite{Sma98}.  Another prominent problem solved
by interval methods is the double bubble conjecture, a generalization of
the isoperimetric problem in three dimensional Euclidean space.  A
sphere gives the solution to the classical isoperimetric problem.  The
work of J. Hass, M. Hutchings, and R. Schlafly shows that the surface
area minimizing way to enclose two regions of equal volume is the double
bubble, which consists of two partial spheres, separated by a flat 
circular disk \cite{HHS95}.

Interval arithmetic has also yielded a number  of new results on the 
problem of packing circles in a square. M. Cs. Mark\'ot and T. Csendes
have obtained optimality proofs for packings of $28$, $29$, and $30$ circles in
a square.
See Figure~\ref{XX}. %% not drawn, get book
This is an area of substantial active research. See, for example, 
\cite{Sza07} and \cite{Mar07}.




\subsection{historical note}

There are those who have tried to downplay the role of computers and interval
methods in the solution to the sphere packing problem.  In fact,
they play an absolutely central role.  To segregate the
computation destroys the proof.
After writing the paper {\it The Sphere Packing Problem}, I had all but given up
on solving the problem.  I had an extremely difficult nonlinear optimization problem
on my hands and no rigorous mathematical method to solve it.  In the summer  1993, 
filled with  wreckless excitement in the days following 
Wiles's announcement of Fermat's Last Theorem,
I happened upon
book on Pascal-XSC (a language extension of Pascal for interval analysis)
at the Seminary Coop Bookstore next to the University of
Chicago.  This book described the method I had lacked.
With fresh hope, in January 1994, 
I set aside all else and devoted full
effort to the sphere packing problem.   The interval code was the most difficult part of the computer code to implement because its speed was crucial.  Thanks to the improvements of S. Ferguson, eventually
the code could run from beginning to end in about three months.  
The interval verifications were the last part of the proof to be completed in August 1998. 


\clearpage
\section{Linear Programming}

The solution of the sphere packing problem is based on the {\it vanishing
box trick.}  At the outset, the set of 
counterexamples to the problem has an unknown size and structure.
These counterexamples are all placed in a large box.   
The width of the box is measured and is found to be negative.
Therefore the box is empty, it contains no counterexamples, and
the problem is solved.

It may seem that such a strategy is too hopelessly naive to work.
In fact, precisely this strategy is pursued, and it works beautifully.
The act of placing a counterexample in a box is called linear 
relaxation.  The counterexamples form an unknown nonlinear set.
The box that contains them is defined by linear constraints.  
Relaxation refers to a relaxation of constraints, so that the
box contains the counterexamples -- it is not merely a linear
approximation to the set of counterexamples.  The box needn't be
rectangular.  Any polyhedron will do.  To determine that a box
has negative width is to say that it gives an {\it infeasible} linear
program.  With the strategy, there is no need to limit ourselves
to a single box, we can use a finite collection of boxes that
contain the set of counterexamples.  This leads to branch and bound
methods.


In this essay we will discuss these methods in further detail,
including linear programming, linear relaxation,  infeasibility,
and branch and bound methods.

The introduction of linear programming methods to the sphere packing
problem was gradual and tranquil.  
Many of the
inequalities in the sphere packing problem have an obvious linear form.
As an engineering student
at Stanford, I had studied
linear and nonlinear optimization.
Given my background, 
it was natural for me to express them as a linear program.  
Once linear programming had made an appearance, the 
repeated successes of the method led me to rely on it more and more.
In the end, approximately $10^5$ linear programs appear in the solution,
each involving some $200$ variables and around $2000$ constraints.
This is a inconsequential task for modern algorithms.  

The consequential
part is to organize the output of this many 
linear programs into a comprehensible proof narrative.  How do I convince
you the reader that this vast collection of linear programs 
constitutes the proof of a theorem?  
What is the precise statement
of that theorem, expressed in a way that does not refer to $10^5$ separate
problems? There is an entire chapter of this book devoted to these questions.
This introductory essay gives enough background to carry us through
to that chapter.


\subsection{preliminaries}

There is a vast mathematical literature describing algorithms to
solve the constrained optimization problem of finding a column vector
$x\in\ring{R}^n$ that maximizes the objective function $ x$
    \begin{equation}\label{eqn:lp1}
    \max_{x}  c x
    \end{equation}
where $c\in\ring{R}^n$ is a fixed row vector, subject to a system of linear inequalities,
    \begin{equation}\label{eqn:lp2}
    A x\le b
    \end{equation} 
for some matrix $A$ and vector $b\in
\ring{R}^m$.  (An inequality $A x\le b$ of vectors means that
each component of vectors satisfy the inequality.)
Such a maximization problem is  called a {\it linear program}, an
unusual name for what it is, not a program in the
sense of computer code, rather a program in the sense of military logistics,
growing out of G. Dantzig's war experience at the Pentagon \cite{Dan91}.
G. Dantzig proposed the simplex method to solve such problems in 
1947.  The system of constraints $A x \le b$ defines a polyhedron,
the vector $c$ gives a preferred direction in
space, and the simplex method walks along edges of the polyhedron,
from vertex to vertex, until reaching a vertex where no further
progress can be made.  The simplex method has been named
one of the top ten algorithms of the twentieth century \cite{Cip00}.

Later in 1947, within minutes of first hearing about
linear programming, von Neumann introduced linear programming
duality, as an outgrowth of his theory of games.
In 1979, L. Khachiyan found a way to solve linear programs in
polynomial time \cite{Kha79}.  Still later, 
N. Karmarkar found a polynomial
time algorithm of practical value \cite{Kar84}.  
M. Wright gives a recent survey
of interior-point algorithms (algorithms that search for the 
optimal solution by moving through the interior of the polyhedron
rather than following the simplex method's strategy of searching
along the boundary of the polyhedron) in \cite{Wri05}. 
M. Todd also gives an excellent survey of algorithms \cite{Tod02}.
These algorithms have been implemented in various
software packages.
Because of this extensive and well-developed literature on the
subject of linear programming, we will take it as given that we
can solve large-scale linear programs.


The basic linear program admits many variations.  We can minimize
the objective function
$$x \mapsto c x$$ rather than maximize it.
Any linear equation
    $a x = b$ can be written as a system of inequalities
        $$a  x \le b \text{ and } -a x \le -b.$$
This allows us to reduce constrained linear optimization with
equality constraints to a constrained linear optimization with
inequality constraints.

\subsection{duality}

A linear program is feasible if there exists an $x$ satisfying
all the constraints $A x \le b$.  A linear program is bounded
if the set $c x$ is bounded from above for all $x$ satisfying the
constraints.  

Given the linear program of Equations~\ref{eqn:lp1},\ref{eqn:lp2}
defined from $A,b,c$, there is another linear program defined by
the same data, but in dual form
    \begin{equation}
     \min_y {y b}
    \end{equation}
such that
    \begin{equation}
      y A = c \text{ and } y \ge 0,
      \end{equation}
where $y\in\ring{R}^m$ is a row vector.
To distinguish the two linear programs, the one given by
Equations~\ref{eqn:lp1} and \ref{eqn:lp2} is called the primal.
If $x$ is any feasible solution to the primal and  if
 $y$ is any feasible solution to the dual (meaning that they satisfies
the constraints but are not necessarily optimal), then by
the inequality constraints, we have trivially that
    $$y b \ge y A x = c x.$$
That is, any feasible solution $y$ to the dual linear program gives an
upper bound to the primal.  If the left-hand side is minimized
for $y=y^*$ and the right-hand side is maximized when $x=x^*$, then
    $$y^* b \ge c x^*.$$
The linear programming duality theorem asserts that if the primal
is feasible and bounded, then the dual is also feasible and bounded,
and moreover $y^* b = c x^*$.  In summary, we can bound
the primal with any feasible solution to the dual, and find
the optimal bound on the primal by minimizing the dual.

Linear programming duality produces certificates of a bound
on the primal.  Suppose we treat a linear programming package as
an untrusted 
black box without any knowledge of its internal algorithms.
To handle round-off errors, we now also assume that we have
a priori bounds $|x_i| \le m$ on the variables $x_i$.
The untrusted package produces a vector $y$, which it claims
is (approximately) a feasible solution to the dual problem.  
First, replacing
any negative coefficients in $y$ with $0$, we may assume that
it satisfies the constraint $y\ge 0$.  The quantity $\delta = c - y A$
should be exactly zero for a feasible solution to the dual program, but
because of round-off errors, it may be  off.  
Then for any feasible $x$ to the primal problem
   $$
   c x = (\delta + y A)  x \le \delta  x + y b.
   $$
Using the a priori bounds on $x$, we get $\delta x\le \sum 
m|\delta_i|=D$.  An upper bound on the primal is the value
$D + y b$, which is computed without any knowledge of the
algorithms used by the software package.  By the duality theorem
for linear programming, there exists a certificate for which this
computed value is exactly the maximum of the primal.

In the sphere packing problem, in practice we have a priori bounds
on the variables $x$, and this method works extremely well.  Note
that the a priori bound $m$ is allowed to be an extremely crude estimate,
because in producing the estimate $D$, it is multiplied by $\delta$,
which typically has the magnitude of a machine epsilon.

\subsection{infeasibility}


In practice, many of the primal linear programs that appear
in the sphere packing problem are infeasible.  This requires
some small adjustments to the previous discussion.   In the
linear programs that we encounter in this book, it is not
necessary to determine the exact upper bound of a the linear program.
We have an explicit constant $K$, and we wish to prove that
the maximum of the objective function is less than $K$.
Rather
than consider two separate
types of problems, feasible and infeasible, we recast all the linear
programs in this book in terms of infeasibility.
When the maximum of $c x$ is less than $K$, by
adding the constraint $c x\ge K$ to the system of constraints
$A x\le b$,  we get an infeasible linear program.
This is the vanishing box trick:  the counterexamples to a nonlinear
problem are constrained to lie inside an infeasible system of linear
inequalities.


We now drop the objective function, and work
with a system of inequalities  of the form
    \begin{equation}
    \label{eqn:lpsys}
    \begin{array}{lll}
    A x &\le c\\
    a &\le x \le b,\\
    \end{array}
    \end{equation}
with explicitly given lower and upper bounds $(a\le x\le b)$ on
the variables $x$.  The problem
is to verify that the system is infeasible; that is, there does
not exist an $x$ satisfying the system of inequalities.  By
translating dual linear program certificates into this new context
we are led to the following definition.

\begin{definition}[certificate~of~infeasibility]
  A certificate of infeasibility for the system \ref{eqn:lpsys},
  is a pair $(u,v)$ satisfying
    $$
    \begin{array}{lll}
    0&\le u\\
    0&\le v\\
    0&\le u A + v\\
    0& > u(c-Aa) + v(b-a). \\
    \end{array}
    $$
\end{definition}

\begin{example}
If there is an index $i$ for which $b_i < a_i$, then the system is
clearly infeasible.  In this case, we have an obvious certificate
of infeasibility given by $(u,v)=(0,e_i)$, where $e_i$ is the
standard basis vector at index $i$.
\end{example}

\begin{lemma}
  If a certificate of infeasibility $(u,v)$ exists for the system
  \ref{eqn:lpsys}, then the system is infeasible.
\end{lemma}

\begin{proof}
    Suppose for a contradiction that $x$ is a feasible solution.
    Then
    $$
    \begin{array}{lll}
    0 &\le (u A + v)(x-a) \\
      &= [u (c- A a) + v (b- a)] - [u (c - A x)] - [v (b - x)]\\
      &< 0.
    \end{array}
    $$
\end{proof}

As in the case of dual linear program certificates, 
to prove a system infeasible, it is not necessary to know how a
certificate of infeasibility is produced.  It can be produced 
by an untrusted computer algorithm.  All that is needed
to be known is that it is a certificate.

If an approximation $(u',v')$ to a certificate of infeasibility is
produced (say by computer), it can often be adjusted to give a
true certificate.  Given an approximation $(u',v')$, define
$(u,v)$ by
    $$
    \begin{array}{lll}
    u &= \max(0,u')\\
    v &= \max(0,v',-u A).
    \end{array}
    $$
If $(u,v)$ satisfies
    $$u( c-A a) + v(b-a) <0,$$
then it is a certificate of infeasibility.

The theory of duality for linear programming insures that if a
system is infeasible, then a certificate of infeasibility exists.
Since the equations defining a certificate of infeasibility are
linear in $u$ and $v$, linear programming algorithms may be used
to produce certificates:
    $$
    \begin{array}{rll}
        \min_{u,v} &u (c-A a) + v(b-a) \text{ such that }\\
        0 &\le u\\
        0 &\le v \\
        0& \le u A + v\\
    \end{array}
    $$
This has a feasible solution $u=v=0$.  If we add the constraint
that the objective function is at least $-1$, then there exists a
bounded feasible solution.

%\begin{remark}
%\label{rem:bounded}
%If explicit upper bounds $b_j$  are not known
%for particular variables $x_j:j\in J$, then we can search for a
%certificate $(u,v)$ that satisfies $v_j = 0:j\in J$.  This
%eliminates the dependence on $b_j$.  If explicit lower bounds
%$a_j$ are not known for particular variables $x_j:j\in J$, then we
%can search for a certificate that satisfies $v_j = 0:j\in J$ and
%$u A e_j = 0: j\in J$. This eliminates the dependence on the
%variables $x_j:j\in J$. Again, this augmented system has a
%feasible solution $u=v=0$, and we can impose a constraint to make
%the objective function bounded.
%\end{remark}
%

\subsection{linear relaxation}

Suppose that we have a continuous
nonlinear function $f$ that we wish to maximize
over a compact domain $X\subset \ring{R}^n$.    A linear relaxation
of the domain is a polyhedron defined by constraints $A x \le b$
such that the polyhedron contains the domain $X$.  That is,
$x\in X$ implies $A x \le b$.    A linear relaxation of the objective
function $f$ is a linear function $x\mapsto c x$ such that
$f(x) \le c x$ for all $x\in X$.  It then follows trivially
that the maximum of $f$ over $X$ is at most the value of 
the linear program that maximizes $c x$ such that $A x \le b$.

(A popular article about the proof of the sphere packing problem
illustrated the method by depicting the nonlinear function $f$
as the rolling hills in the countryside, cows grazing in the background.  
A helicopter,
carrying a large roof, was slowing
descending over the hilltops, bounding the height of a hilltop
with the piecewise linear roof.)

Relaxation discards information about the nonlinear behavior of the
optimization problem.  It should be expected that the bounds
obtained by this method will often be crude.  However, it has the
definite advantage that linear programs can be solved efficiently,
while general nonlinear programming problems cannot.

For the problem to be a true linear relaxation, there must be
rigorous proofs that each $x\in X$ satisfies each linear
constraint $A_i x\le b_i$.
This means that the maximum of $A_i x$ over $X$ must be at most $b_i$.
This again, is a nonlinear optimization problem on the domain $X$
with objective function $A_i x$.  For an arbitrary constraint $A_i x\le b$,
and complicated domain $X$, there is no reason to suspect that this
is any easier to solve than the original optimization problem.  Thus,
we are in danger of replacing one intractible problem with another.

The sphere packing problem has a special structure that avoids this
problem.  Even though the nonlinear optimization problem involves
around 150 variables, 
this special structure allows all the nonlinearities to 
be confined to small dimensions (around 6 variables).  In other words,
the sphere packing problem can be
described as a coupled system of small nonlinear subsystems,
and all the coupling comes through linear systems of equations. 
We give a toy model of this in a moment to describe how this works.

This special structure is one of the key points of the entire proof.
If you want to know why this hard nonlinear optimization problem has
been solved, but others have not, you now know the fundamental reason.
By partitioning the linear relaxation of the domain $X$ according
to the small nonlinear subsystems, we can arrange for each inequality
$A_i x\le b_i$ to be sparsely populated, and for each relaxation
constraint to be a statement about a nonlinear inequality on a domain
of low dimension.  This is a tractible problem:
we  summon interval arithmetic to prove these
low dimensional inequalities by computer.  In this way we are able
to give a rigorous proof that we have a true linear relaxation of a
difficult nonlinear optimization problem.  In practice, the linear
relaxation  bounds  are good enough to solve the sphere packing
problem.

\subsection{toy model}

We describe a toy model.  It is a nonlinear optimization problem in a large
number of variables with a special structure.  The problem is a coupled
system of small nonlinear subsystems, and all coupling comes through linear
systems of equations.

Consider any triangulation of a polygonal domain in the plane such as the one
in Figure~\ref{XX}. % not drawn.  
Let $T$ be the set of triangles.  Introduce seven variables 
  $$y^t_1,y^t_2,y^t_3,\beta_1^t,\beta_2^t,\beta_3^t,A^t$$
for each triangle $t\in T$.  The variables $y_i^t$ are the lengths of the three
edges of the triangles; the variables $\beta_i^t$ are the three angles (with
$\beta_i^t$ opposite the edge $y_i^t$), and $A^t$ the area of the triangle.
The variables $y_i^t,\beta_i^t,A^t$ satisfy various nonlinear relations separately for each $t\in T$.
For instance, we may insert superscript $t\in T$ and permute indices into the 
law of cosines and area formula to get a large collection of nonlinear constraints:
   \begin{equation}\label{eqn:loc}
   \begin{array}{lll}
   y_1^2 &= y_2^2 + y_3^2 - 2 y_2 y_3 \cos\beta_1,\\
   2 A &= y_2 y_3 \sin\beta_1,
   \end{array}
   \end{equation}
The triangles fit together into a triangulation, and this imposes additional constraints.
For each pair of adjacent triangles $s,t\in T$, we have an edge matching constraint
$y^s_i = y^t_j$, for appropriate $i,j$.  If $v$ is any vertex of the triangulation, let
   $$
   B(v) = \{(t,i)\in T\times \{1,2,3\} \mid \beta^t_i \text{ is an angle at } v \}.
   $$
Then we have the constraint for each interior vertex $v$
   \begin{equation}\label{eqn:toy1}
   \sum_{(t,i)\in B(v)} \beta^t_i = 2\pi,
   \end{equation}
and for each boundary vertex $v$
   \begin{equation}\label{eqn:toy2}
   \sum_{(t,i)\in B(v)}\beta^t_i \le 2\pi.
   \end{equation}

Suppose that we are given bounds
   \begin{equation}\label{eqn:toy3}
   a_i^t \le y_i^t \le b_i^t,\quad \alpha_i^t \le \beta_i^t \le \gamma_i^t,
   \end{equation}
for the variables for some collection of explicit constants $a_i^t,b_i^t,\alpha_i^t,\gamma_i^t$,
$i=1,2,3$, $t\in T$.  We then have the nonlinear optimization problem of maximizing the total area
   \begin{equation}\label{eqn:toy4}
   \sum_{t\in T} A^t
   \end{equation}
subject to these bounds and  nonlinear constraints.  This is our toy model.

Note the the objective function is linear in the variables $A^t$.  The constraints
of Inequalities~\ref{eqn:toy1}, \ref{eqn:toy2}, and \ref{eqn:toy3} are also linear in
the variables $y^t_i,\beta^t_i$.  This is the key point.  Linearity comes directly
from additivity of measure.
The only nonlinear constraints are those of 
Equations~\ref{eqn:loc}.  Each of these constraints is limited to the seven variables
$y_i^t,\beta_i^t,A^t$ on  a single triangle.  So the nonlinearities are limited to seven
dimensions or less, a constant independent of the number of triangles in $T$.  This is a linearly
coupled system of small nonlinear subsystems.

We define a linear relaxation as follows.  For each triangle $t\in T$, we have a function
$f^t$ with domain
  $$
   [a^t_1,b^t_1][a^t_2,b^t_2][a^t_3,b^t_3]
  $$
and range $\ring{R}^7$, sending
  $$
  (y_1,y_2,y_3)\mapsto (y_1,y_2,y_3,\beta_1(y_1,y_2,y_3),\beta_2(y_1,y_2,y_3),\beta_3(y_1,y_2,y_3),
       A(y_1,y_2,y_3)),
  $$
where now angles $\beta_i$ and area $A$ are viewed as nonlinear functions of the edge lengths $y_i$,
rather than independent variables.
Let $X^t\subset\ring{R}^7$ be the image of $f^t$.
Choose a polyhedron $P^t$ that contains $X^t$.  (Use any algorithm that approximates the
convex hull of $X^t$ with a polyhedron, and use interval arithmetic to justify the containment
$X^t\subset P^t$.)   If two images $X^t$ and $X^s$ are the same, 
we can choose the same polyhedron for them, so the number of different relaxation inequalities
can potentially be much smaller than the size of $T$.

We then have the linear programming problem
of maximizing Formula~\ref{eqn:toy4} subject to 
Inequalities~\ref{eqn:toy1}, \ref{eqn:toy2}, and \ref{eqn:toy3}, 
and $(y^t_1,\ldots,A^t)\in P^t$, for all $t\in T$.  This is the relaxation.

This completes our description of the toy model.  It gives a simple example of the
structural properties that make linear relaxation so convenient.
Note that Leech's proof of the problem of thirteen spheres has essentially the same
structure.  The estimates of the areas of spherical triangles is a small nonlinear subsystem
of the nonlinear problem of minimizing the total area of a triangulation.
The sphere packing problem has the same structure as well.



\subsection{syntactic relaxation}





\subsection{branch and bound}

\subsection{formal linear programs}




\clearpage




\section{Tarski Arithmetic}


\clearpage
