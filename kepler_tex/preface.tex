

In 1611, Kepler wrote a small booklet in which he asserted that the familiar cannonball arrangement of congruent balls in space achieves the highest possible density.  That is, no other arrangement can fill a larger fraction of space.  This statement has come to be known as the packing problem, or the Kepler conjecture.  In 1900, Hilbert made this part of his eighteenth problem.  This book presents a solution to the packing problem.

\section{Formalization Blueprint}

Ten years have passed since a proof was first
obtained. Why give a new presentation of the proof?

The original proof was long and complex.  The complexity was not
because of conceptual challenges.  In fact, the proof makes only
modest demands on the theoretical training of the reader.  It is
possible to read and understand the proof with a knowledge of a
limited body of mathematics, such as basic calculus and elementary
Euclidean geometry.

Nevertheless, the proof involves many  calculation that are routine
and yet sometimes long and tedious.  The proof relies on the results
of computer programs.  An error in any calculation or a bug in the
computer code has the potential of toppling the entire proof.

The referees were conscientious and checked many of the
calculations.  However, the computer code was never seriously
checked, and even the most conscientious auditor can make an
occasional slip.

After all is said and done, no proof is more reliable than the
reliability of the processes that are used to verify its
correctness.  These processes include the checking that the author
makes before releasing the proof for public scrutiny, the checking
of the referees, and the checking done by readers over time.

In recent years, I have been increasingly preoccupied by the
processes that we rely on to insure the correctness of complex
proofs. Researchers from Frege to G\"odel, who solved a problem of
rigor in mathematics, found a theoretical solution but did not
extinguish the burning fire at the foundations of mathematics,
because they omitted the practical implementation. Some, such as
Bourbaki, have even gone so far as to claim that ``formalized
mathematics cannot in practice be written down in full'' and call
such a project
``absolutely unrealizable'' \cite[p 10,11]{Bo}. % Theory of Sets, page 10,11.

While it is true that formal proofs may be too long to print,
computers -- which do not have the same limitations as paper -- have
become the natural host of formal mathematics. In recent decades,
logicians and computer scientists have reworked the foundations of
mathematics, putting them in an efficient form designed for real use
on real computers.

For the first time in history, it is possible to generate and verify
every single logical inference of major mathematical theorems.  This
has now been done for the four-color theorem, the prime number
theorem, the Jordan curve theorem, the Brouwer fixed point theorem,
the fundamental theorem of calculus, and many other theorems.  Freek
Wiedijk reports that 63\% of a list of 100 ``top'' theorems have now
been checked formally \cite{freek}.

Some mathematicians remain skeptical of the process because
computers have been used to generate and verify the logical
inferences.  Computers are notoriously imperfect, with flaws ranging
from software bugs to defective chips.  Even if a computer verifies
the inferences, who will verify the verifier, or then verify the
verifier of the verifier?  Indeed, it would be unscientific of us to
place an unmerited trust in computers.

As I said above, I believe that mathematical proofs are ultimately
no more reliable that the processes we use to verify them.  We have
two competing verification processes.  The first is the traditional
process of referees, which depends largely on the luck of the draw
-- some referees are meticulous, others are careless.   The second
process is formal computer verification. In this case, the process
is less dependent on the whims of a particular referee.

In my view, choosing between the conventional referee process and
computer verification is like choosing between choosing between an
hourglass and an atomic clock as the scientific standard of time. It
impedes science to hold to the hourglass because of imperfections in
the atomic clock.

My standard of proof has become the highest scientific standard
available by current technology.  Today the highest available
standard is formal verification by computer.  This standard will
continue to evolve with the advancement of technology.

My dream is to have some day a fully formally verified solution to
the packing problem.
This has not been done, but progress is being
made.  This book is an attempt to rearrange the proof
in such a way to make formal verification easier to do.

The proof style of formal proofs is different from that of
conventional proofs.  It is better to have a large number of short
snappy proofs, rather than a few ingenious ones.  Humans enjoy
surprising new perspectives, but computers prefer standardization.
Despite these differences, I have worked to make this a proof that
will be valuable both to humans and to computers.


\section{Structure of this Book}

Because of the complexity of the solution to this problem, it is particularly important for the reader to maintain a clear view of the structure and organization of this book. 

The book is divided into several major parts.
\begin{enumerate}
\item Essays,
\item Foundations,
\item Solution to the Sphere Packing Problem,
\item Minor Appendices
\item Appendix on Tarski Arithmetic
\end{enumerate}
The essays describe the major ideas,  methods, and organization of the proof.  There is an essay on each major computer component of the proof. The purpose is to provide a panoramic view of proof, to provide intuition about proof strategies.  After reading this part of the book, the reading should understand what the proof is all about, without yet dipping into technical details.  

The part on foundations provides background material about constructions in discrete geometry that have relevance beyond the packing problem.  There are four chapters in this part.  The first covers trigonometric identities and basic vector geometry.  The second treats volume from an elementary point of view.  The third chapter covers planar graph theory from a purely combinatorial point of view.  The fourth chapter covers fans, which gives planar graphs from a more geometric point of view.

The next part of the book gives the solution to the packing problem.  The first chapter in this part gives a top-level overview of the major steps of the proof.  It describes how the problem can be reduced from a problem in infinitely many variables to a problem in finitely many variables.  The remaining chapters in this part flesh out that skeleton.

The remaining parts of the book are appendices.
There are various arguments that are used repeatedly with minor variations.
These proofs are so similar to one another and routine (in fact sometimes algorithmic) that it seems
best to separate them from the main flow of the proof and place them in appendices.  The first appendix contains various lemmas about the signs of derivatives: a derivative has fixed sign and the function is monotonic.  The second chapter contains a collection of problems called {\it assembly problems}.  In these problems, there is a finite set of constants (say the volumes of three dimensional bodies) and the problem is to bound the sum of the constants.  

There is a long appendix on Tarski arithmetic.  This is a collection of about 150 problems that can be expressed in the elementary theory of real closed fields.  There are algorithms to solve such problems (due to Tarski, Collins, and others).  However, many of problems seem to be just beyond the practical capabilities of these algorithms.  The appendix gives conventional proofs for these results, yet we hope that the conventional proofs will someday be replaced by fully automated proofs.

Beyond the text,
the proof relies on three separate external bodies of computer code.  These will be described in much greater detail at various places in the book.  These external bodies of code are called
\begin{enumerate}
\item Tame Hypermap Generation,
\item Interval Arithmetic,
\item Linear Programming.
\end{enumerate}
The tame hypermap generation is an stand-alone program that is run once at a specific point of the proof.  It carries out a combinatorial classification of planar graphs satisfying a certain restrictive list of properties.  The reader can safely ignore this computer program until reaching the relevant point in the proof.   By contrast, the interval arithmetic is a collection of nearly one thousand inequalities that have been proved by computer.   These inequalities are spread throughout the proof and appear throughout this book.  On first reading, the reader is encouraged to accept these inequalities as axiomatically given facts.   Detailed documentation about these inequalities is available for those who wish to follow up later on the computer-generated proofs of these inequalities.  The final stage of the proof consists entirely of linear programming.  There are also several small linear programs that appear in scattered places in the book.



\section{An Unfinished Manuscript}

\uf{It is with deep regret that I approach the end of my sabbatical year in Hanoi and Nijmegen with an unfinished manuscript in hand.  The end of the project is in sight, yet the time has come for me to give my battered wrists a rest and turn my attention to other projects.  .}

\uf{The starting point for this manuscript was the 2006 proof published  in Discrete and Computational Geometry.   The proof described here is essentially the same as that proof.  Considerably more detail has been added, obvious mistakes have been corrected, background material has been added, and the presentation has been substantially reorganized.  This manuscript is sufficiently developed that it now carries more weight on almost every point than the 2006 publication.}

\uf{There is now enough documentation on the proof that all the steps of the proof can be reconstructed.  The errata to the 2006 proof gives details of all the issues that have emerged during the revisions leading to this manuscript \cite{errata}.  However, a few loose ends require particular attention: the non basic linear programs,  the zeroing estimates, and the new interval arithmetic calculations.}

\uf{Steven Obua has formally checked the basic linear programs in Isabelle.  This covers over 90\% of the graphs.  I have read his forthcoming thesis and find that it covers all the infrastructure needed to run the non basic cases as well.  In my view, the bulk of the linear program formalization is now complete.  The difficulty with the remaining part is not mathematical.  As the chapter on linear programming explains, there is a problem only because this part of the proof has been largely ignored.  The editors and referees showed absolutely no interest in the linear programs.  As far as I know, no other human has looked at the non-basic linear programs.  It is the only part of the proof that I have not revisited in the past decade.  All this worries me.  I understand my own documentation.  Will others?  This documentation is all available on my Pitt webpage.  Much of it also appears with the version 1 arXiv tar file from the 1998 submission ``The Kepler Conjecture.''  Much of it is also bundled with the permanent code repository held by the Annals of Mathematics.  It would relieve me to learn that someone else understands this material.}

\uf{What are the zeroing estimates?  In 1994, I proposed a five-step plan to prove the Kepler conjecture.  Step two of that program was to prove that the score is at most $0$ on standard regions other than triangular regions.  This is what I call the zeroing estimates.  This solution of this problem appeared in the 1998 preprint posted on the arXiv ``Sphere Packings, II'' and in slightly revised form as in the 2006 proof.   This argument has been entirely deleted from this book.  It is no longer needed and it is based on a partition of space that is not described in this book.  To include it would complicate the formalization project.  Much stronger bounds are available in the ``Main Estimate'' in this book (the bounds $\sigma(\Lambda,v_0,U)\le s_n$).  However, there are still some loose ends here that haven't been cleaned up;  specifically, a few arguments make use of the zeroing estimates before the proof of the main estimate is complete.  There are no serious circularities here; but the presentation is still somewhat tattered.  
}



\uf{There are a number of new interval arithmetic calculations that appear in this manuscript.  They are listed separately in a flyspeck project file.\footnote{http://flyspeck.googlecode.com/svn/trunk/inequalities/kep\_ineq\_bis.ml.}}










\section{Thanks}

I am particularly grateful to Sam Ferguson, for the years that
he spent working on this problem with me.  I also thank the early editors
R. MacPherson, G. Fejes T\'oth, and J. Lagarias for their work
to improve a technically challenging piece of mathematics.  

Many colleagues in the formal theorem proving community have helped me to learn the theory and the tools.  Others have made significant contributions to the flyspeck formalization project.  I wish to thank 
Nguyen Quang Truong % fix accents. (he found a mistake)
Nguyen Tat Thang % fix accents. (he found a mistake)
J. Avigad, M. Adams, F. Wiedijk,  T. Nipkow, J. Harrison, S. McLaughlin, G. Bauer, S. Obua, and R. Zumkeller.   Much of the material from this book was covered in a course on discrete geometry and computers at the University of Pittsburgh.  I would like to thank the members of that class for working through the organization of the text with me.

This book has been written during a sabbatical leave from the University
of Pittsburgh.  I wish to thank the many institutions that supported
me during this period: the Max Planck Institute in Bonn, the \'Ecole Normale Sup\'erieure,  the Institute of Math
in Hanoi, Radboud Univeristy in Nijmegen, and the University of Strasbourg.
I thank those that made these visits possible: G. Mints, F. Loeser, F. Lecomte, H. Barendregt, Ha Huy Khoai and Ng\^o Vi\d{\^e}t Trung.


\bigskip
\hbox{}

{
\parindent=0pt
\obeylines

Thomas C. Hales
Nijmegen, Netherlands
April 12, 2008

}

\bigskip
\hrule
\bigskip

My attention has turned to C. Marchal's formulation of the Kepler conjecture.  He has proposed a new objective function that is simpler than the {\it score function} that is used in the original 1998 proof.  If that optimization problem can be carried out, it would bring about a significant reduction in complexity of the proof.

In addition to this book, there are a few other documents to understand the original 1998 proof.  The preprint ``A Revision of the Proof of the Kepler Conjecture'' describes the current status of the Flyspeck project and gives a listing of all corrections that have been found to the original proof~\cite{revision}.  There is a separate errata for the computer code~\cite{errata}.  The appendix on Tarski arithmetic has been moved to a separate document ``Lemmas in Elementary Geometry''~\cite{leg}.


\bigskip
\hbox{}

{
\parindent=0pt
\obeylines

Thomas C. Hales
Pittsburgh, PA
April 10, 2009

}







