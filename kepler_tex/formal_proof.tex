% Formal Proof: A Mathematician's Perspective
% Author: Thomas C. Hales
% Affiliation: University of Pittsburgh
% email: hales@pitt.edu
%
% latex format

% History.  File started Feb 18, 2008
% artifact (in wider use), but -> artefact = Britsh and dictionary preferred.
%% 

%% TO FIXX
% rewrite paragraph on computer proofs.
% rewrite paragraph on JCT.

%% Fixed
% rewrite flyspeck par
% repeat 500 lines of code in 7.1 7.2
% Express idea of automation of transcription not solved.
% In the terms, f x, function applications, no type specialization is done.
% drop paragraph on peer review.
% rewrite moonshine sentence.
% rewrite paragraph on Leroy, compilers.

\documentclass{llncs}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{alltt}


% Math notation.
\def\op#1{{\hbox{#1}}} %operatorname XX
\def\tc{\hbox{:}}
\newcommand{\ring}[1]{\mathbb{#1}}

% Flags


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Formal Proof}%: A Mathematician's Perspective}
\author{Thomas C. Hales\thanks{The author's research on the Formal Foundations of Discrete Geometry has been supported by NSF grant 0503447.}}
\institute{University of Pittsburgh\\
\email{hales@pitt.edu}}
\maketitle

%\begin{abstract}  
%A mathematician gives his perspective on formal %proof.
%\end{abstract}

\centerline{\it dedicated to N.G. de Bruijn}
% See gmail correspondence with Henk.
%XX add thanks to Henk and group, where this article was written.


\bigskip

{\narrower\it 

There remains but one course 
for the recovery of a sound and healthy condition --
namely, that the entire work of the understanding be commenced afresh, and the mind itself be from the very outset not left to take its own course, but
guided at every step; and the business be done
as if by machinery.  F. Bacon, 1620, Novum Organun
% preface.

}

\bigskip


%%%%%%%%%%%%%%%%%%%%%%

\bigskip

\section{Bugs}

Daily, we confront the errors of computers.  They crash,
hang, succumb to viruses, run buggy software, and harbor
spyware.  Our  tabloids report bizarre computer glitches:
the library patron who is fined \$40 trillion for an overdue
book, because a  
barcode is scanned as the size of the fine; or the 
dentist in San Diego who was delivered over 16 thousand tax forms
to his doorstep when he abbreviated ``suite'' 
in his address as ``su.''

  On average,
a programmer introduces 1.5 bugs per line while typing.
Most are typing errors that are spotted at once.
About one bug per hundred lines of computer code ships  to
market without detection.  Bugs are an accepted
part of programming culture.
The book that describes itself as the ``bestselling software testing
book of all time'' states that ``testers shouldn't want to verify
that a program runs correctly.'' 
% Wiley,
%Testing Computer Software
%The bestselling software testing book of all time!
%Second Edition
%Cem Kaner, Jack Falk, Hung Quoc Nguyen
%1999
Another book on software
testing states ``Don't insist that every bug be fixed $\ldots$
When the programmer fixes a minor bug, he might create
a more serious one.''  Corporations may keep critical bugs
off the books to
limit legal liability.
 Only those bugs should be corrected
that affect corporate profit.
The tools designed to root out bugs are themselves
full of bugs. ``Indeed, test tools are often buggier than
comparable (but cheaper) development tools.''
% Lessons Learned in Software Testing, Kaner, Bach, Pettichord 2001.
As for hardware reliability, former 
Intel President Andy Grove himself said 
``I have come to the conclusion that no microprocessor is ever
perfect; they just come closer to perfection $\ldots$''
% quoted p221. MacKenzie, Mechanizing Proof.
%(at the time of the famous pentium bug) 


Bugs can be far-reaching.
The bug causing the 
explosion of the Ariane 5 rocket cost hundreds of millions
of dollars.  As long ago as 1854, Thoreau wrote that 
``by the error of some calculator
the vessel often splits upon a rock that should have reached
a friendly pier.''  % Walden, Economy, page 13.
Last year, the New York Times reported Shamir's warning that
even a small math error in a widely used computer chip could 
be exploited to defeat cryptography and would
place
``the security of the global electronic commerce system at risk
$\ldots$''
% Adding Math to List of Security Threats, New York Times, November 17, 2007.





\section{Mathematical Certainty}

By contrast, philosophers tell us that
mathematics consists of analytic truths,
free of all imperfection.  We prove that $1+1=2$ by
recalling the definition of $1$ as the successor of $0$,
$2$ as the successor of $1$, and then invoking twice the recursive
definition
of addition: 
  $$1+1 = 1 + S(0) = S(1 + 0) = S(1) = 2.$$

If only all proofs were so simple.  
Mathematical error is as old as mathematics itself.
Euclid's very first proposition asks, ``on a given straight line
to construct an equilateral triangle.''  Euclid's construction
makes the implicit assumption -- not justified by the axioms -- that
two circles, each passing through the other's center, must intersect.
%The third vertex of Euclid's triangle is a point of intersection.
We revere Euclid, not because he got everything right, but because
he set us on the right path.

We have entered a era of proofs of extraordinary complexity.
%%  My early experiences with Langlands theory 
Take, for example, F. Almgren's masterpiece in geometric measure
theory, called appropriately enough the ``Big Paper.'' 
%http://www.worldscibooks.com/mathematics/4253.html
The preprint is
1728 pages long. Each line is a chore. He spent over a decade writing it in the 70s and
early 80s.  It was not published until 2000.  Yet the theorem
is fundamental.  It establishes the regularity of minimizing
rectifiable currents, up to codimension two;  in basic terms, 
it shows that higher dimensional soap bubbles are smooth
rather than jagged -- just
as one would naturally expect.  How am I to develop enough confidence
in the proof that I am willing to cite it as result in my own research?
Do the stellar reputations of the author
and editors suffice, or should I try to understand the details of the
proof?  I
would consider myself very fortunate, if I could work through the proof
in a year.

Computer proofs 
compound the complexity. % that mathematicians face.
Today, there are many major theorems, starting with the proof
of the four-color theorem.
%% XX Lam had collaborators.
C. W. H. Lam made an 800-day computation on a VAX computer to
prove the non-existence of a projective plane of order 10.
 W. Tucker solved Smale's 14th problem by computer, establishing
that the Lorenz equations have a strange attractor. 
A. Kumar and H. Cohn have used computers to prove 
that the Leech lattice gives
the sphere packing of optimal density among all lattices in $24$
dimensions.
The isoperimetric inequality in three dimensions states the
sphere provides the surface-minimizing way to enclose a fixed volume.
The double bubble conjecture, which is the generalization of
the isoperimetric inequality to two equal volumes, was originally solved
by computer.  The solution is two congruent spheres, fused along
a flat disk meeting the spheres at $120$ degrees (as required
by Plateau's conditions).
Computers have even been used in the study of hyperbolic $3$-manifolds.
% 800 days at
% The Search for a Finite Projective Plane of Order 10
% http://www.cecm.sfu.ca/organics/papers/lam/paper/html/node5.html
%
%http://arxiv.org/abs/math/9609207 Homotopy 3-manifolds.
 


\section{Formal Proof}

A formal proof is a proof in which every logical inference has
been checked all the way back to the fundamental axioms of mathematics.
Traditional mathematical proofs are written in a way to make them easily understood by mathematicians. Routine logical steps are omitted. An enormous amount of context is assumed on the part of the reader. Proofs, especially in topology and geometry, rely on intuitive arguments in situations where a trained mathematician would be capable of translating those intuitive arguments into a more rigorous argument.

In a formal proof, all the intermediate logical steps are supplied. No appeal is made to intuition, even if the translation from intuition to logic is routine. Thus, a formal proof is less intuitive, and yet less susceptible to logical errors.

There is a wide gulf that separates traditional proof from formal proof.
For example, Bourbaki's Theory of Sets was designed as a purely theoretical
edifice that was never intended to be used in the proof of actual theorems.
Indeed, Bourbaki declares that ``formalized mathematics cannot
in practice be written down in full'' and calls such a project
``absolutely unrealizable''  % Bourbaki, Elements of Sets, Addison-Wesley, Reading, MA, 1968,  pp.10,11.
The basic trouble with various foundational systems is that meta-mathematical arguments (for
example, abbreviations that are external to the system, or
inductions over the syntactical form of an expression) 
are usually introduced early on, and without these simplifying meta-arguments,
the vehicle stalls, never making it up the steep incline from primitive notions to 
high-level concept.   The gulf can be extreme: Matthias has calculated
that to expand the definition of the number `$1$' fully in terms of Bourbaki primitives requires
over $4$ trillion symbols.
In Bourbaki's view, 
the foundations of mathematics are roped-off museum pieces
to be silently appreciated, 
but not handled directly.

There is an opposing view that regards the 
foundational enterprise
as unfinished until it is realized in practice and written down in full.
This article sketches the current state of this endeavor.
It has been necessary to commence afresh, and to retool the foundations
of mathematics for practical efficiency, while preserving
its reliability and austere beauty.  For anything beyond a trivial
proof, the number of logical inferences is so large that a computer is
used to ensure that no steps are omitted.   This raises basic questions
about trust in computers.

As the art is currently practised, each formal proof starts with a traditional
mathematical proof, which is rewritten in a greatly expanded form, where all the
assumptions are made explicit and all cases are treated in full.
For example, a traditional mathematical proof might show that a graph is
planar by drawing the graph on a sheet of paper.  The expanded form of
the proof  replaces the picture by careful argument.  From the
expanded text, a computer script is prepared, which generates all
the logical inferences of the proof.  The transcription of a single traditional
proof into a formal proof is a major undertaking.

\subsection{Examples}

Formal proofs have been under development for decades, but only recently
has it become a practical matter to prove major theorems formally.
The most spectacular
example is Gonthier's formal proof of the four-color theorem.  His
starting point is the second-generation 
proof by Robertson et al.  Although the traditional proof uses a computer
and Gonthier uses a computer,  the two computer processes
differ from one another in the same way that a traditional proof differs
from a formal proof.  They differ in the same way that adding $1+1=2$ on
a calculator differs from the mathematical
justification of $1+1=2$ by definitions,
recursion, and a rigorous construction of the natural numbers.  
In short, a large logical gulf separates them.
As a result of Gonthier's formalization,
the proof of the four-color theorem has become one of the most meticulously verified proofs
in history.

In the past few years, several other significant theorems have been
formally verified. See Table~\ref{table}.  The table lists the
theorems, which formal proof system was used (there are many to choose from), the person who
produced a formal proof, and the mathematicians who produced the
original proof.  The Prime Number Theorem, asserting that the
number of primes less than $n$ is asymptotic to $n/\log\,n$, has
two essentially different proofs: the elementary proof of
Selberg and Erd\"os and the analytic proof of Hadamard and
de la Vall\'ee Poussin.  Formal versions of both proofs have
been produced.   More ambitious projects are in store:
 the leading problem of the document {\it Ten Challenging
Research Problems for Computer Science} is the formalization
of the proof of Fermat's Last Theorem.
% J. Bergstra, 5 July 2005.





\smallskip

\begin{table}[ht]
\caption{Examples of Formal Proofs}
\centering
\begin{tabular}{l l l l l}
\hline
Year\hspace{0.5em} & Theorem\hspace{6em} & Proof System\hspace{2em}  & Formalizer\hspace{3em} & Traditional Proof\\ [0.5ex]
\hline \\
1994 & First Incompleteness & Boyer-Moore   & Shankar &  G\"odel \\
2004 & Prime Number & Isabelle & Avigad et al. & Selberg-Erd\"os\\
2004 & Four Color & COQ & Gonthier & Robertson et al.\\
2005 & Jordan Curve  & HOL Light & Hales & Thomassen \\
2005 & Brouwer Fixed Point & HOL Light & Harrison & Kuhn \\
2006 & Flyspeck I & Isabelle & Bauer-Nipkow & Hales \\
2007 & Cauchy Residue & HOL Light & Harrison & classical \\
2008 & Prime Number & HOL Light & Harrison & analytic proof \\
% 2008 & Flyspeck II & Isabelle & Obua & Hales \\
 [1ex]
\hline
\end{tabular}
\label{table}
\end{table}

%XX 130 defs, 80 million inferences, etc.

Box~\ref{XX} displays the statement of the Jordan Curve theorem, in computer
readable form, as it appears in the formal
proof.  The complete specification of the theorem should also list
all definitions, all the way back to the fundamentals.  In particular, {\it top2} refers
to the standard topology on the plane; {\it top2}~$A$ indicates that $A$ is an open set
in the plane;  {\it euclid} $2$ is the Euclidean plane; {\it connected top2}~$A$ means that
$A$ is a connected set in the plane.

\bigskip
\noindent
\framebox{\parbox{4.8in}{
\parindent=0pt

\centerline{\it The Formal Jordan Curve Theorem}
\smallskip

\vbox{
\def\hb{\hfill\break}
\def\h{\hbox{}}

\obeylines

  {\tt %let JORDAN\_CURVE\_THEOREM = prove\_by\_refinement(
  \h~~`$\forall C$. {\it simple\_closed\_curve} {\it top2} $C$ $\Rightarrow$\hb
  \h~~~~($\exists A\, B$.  {\it top2} $A$~$\wedge$  {\it top2} $B$ $\wedge$\hb
  \h~~~~~~{\it connected} {\it top2} $A$~$\wedge$  {\it connected top2} $B$ $\wedge$\hb
  \h~~~~~~($A \ne \emptyset$) $\wedge$ ($B \ne \emptyset$) $\wedge$\hb
  \h~~~~~~($A \cap B = \emptyset$) $\wedge$ ($A \cap C = \emptyset$) $\wedge$ ($B \cap C = \emptyset$) $\wedge$\hb
  \h~~~~~~($A \cup B \cup C =~${\it euclid}~$2$))`
  \h~~%$\cdots$);;

}

}


}}
\medskip

Although details differ, 
in a typical system, the primitive inference rules and axioms
become bits of data that are processed by other computer procedures.
For example, to give a formal proof that 
$$
%% CHECKED May 31, 2008
2682440^4 + 15365639^4 + 18796760^4 = 20615673^4
$$
a human is not required to type each primitive inference.  Instead,
a programmer writes one general procedure that takes any arithmetic
identity as input,  generates the inferences,
and produces the theorem as output.   A large number of such
small decision procedures are programmed into the system to handle
routine tasks such as polynomial simplification, basic tautologies in logic,
handheld calculator operations, 
and decidable fragments of arithmetic.  
Procedures that automatically search for steps in a proof
are also programmed into the computer.  
%( Sledgehammer tactic.)
New procedures may be contributed
by any user at any time to automate further tasks.
The design of the kernel of the system prevents a rogue
user from writing computer code that could compromise the soundness of the system.% (Section~\ref{}).


A large library is maintained
of all previously established proofs in the system, and anyone may
use any result that has been previously established.
 Although every step of every proof is
always checked,
as researchers contribute procedures and theorems to the system,
interaction with the system gradually moves
away from the primitive foundations towards something more closely
resembling the high-level practice of mathematicians.


%\subsection{Jordan Curve Theorem}
%
%C. Thomassen's proof of the Jordan Curve theorem is based
%on the non-planarity of the $K_{3,3}$ graph (Figure~\ref{XX}).
%The formal proof of the Jodan Curve theorem was based on 
%a traditional proof by C. Thomassen.  





\section{HOL Light}

Over the past several years, I have used a system called HOL Light  
(an acronym for a lightweight implementation of Higher Order Logic.)  There are several competing systems to choose from. %, with names like
%as HOL, Mizar, Isabelle, and COQ.  
People argue about the relative merits of the
different systems much in the same way that people argue about the relative merits
of operating systems, political loyalties, or programming languages.  To some extent, preferences
show a geographical bias:  HOL in the UK, Mizar in Poland, Isabelle in Germany, and
COQ in France.



These systems currently take considerable effort to learn.  The hope
is they will eventually become sufficiently friendly
to become a familiar part of the 
mathematical workplace, much as email, \TeX\relax, computer algebra systems, and
web browsers are today.
%I would say that the overall effort is comparable to the effort required to become a \TeX\relax-pert,
%or advanced user in a computer algebra system such as SAGE, although
%perhaps a larger initial investiment is required to get started in a
%formal system.

\subsection{Types}

Much day-to-day mathematics is written at a level of abstraction that is
indifferent to its exact representation  as sets.  For example, 
it does not matter how an ordered pair is encoded as a 
set, as long as the ordered pair has the characteristic property
 $$
 (x,y) = (x',y) \quad \Leftrightarrow\quad  x = x' \hbox{ and } y=y'.
 $$
It is bad style to break the abstraction to write $2\in(0,1)$.
%the published proof
%of Fermat's Last Theorem does not discuss whether it is a theorem
%in ZFC (Zermelo-Fraenkel with the axiom of choice), or whether Grothendieck's universes from EGA slip in. 
This layer of abstraction good news, because it allows us to shift
from ZFC to a different foundational system with equanimity and ease.  

Many
formal proof systems  are based on
types.  Types are familiar to computer programmers.  In a typed computer language,
$3$ is a integer and $[1.0;2.0;3.0]$ is an array of floating point numbers. An attempt
to add $3$ to this array results in a type mismatch error, and the computer program
will not compile.  The type checking mechanism of programming languages
conveniently detects many bugs at the time
of compilation.  %This is an extremely useful way to detect bugs in computer code.


Zermelo-Fraenkel set theory has no such type checking mechanism.  As de Bruijn
puts it,
``Theoretically, it seems perfectly legitimate
to ask whether the union of the cosine function
and the number $e$ (the basis of natural
logarithms) contains a finite geometry.''
%- N. G. de Bruijn, Types in Mathematics, page 29
Mathematicians have the good sense not to ask such questions.  However, when moving
mathematics to a computer, lacking in common sense, it is useful to introduce types
into the foundations
to prevent this kind of nonsense.  By convention,  a colon is written before the name of a type.  For instance, we write the type of the real number $e$ as $\tc\ring{R}$, or simply $e:\ring{R}$, to indicate that $e$ is a real number.
The cosine function has a different type $\tc\ring{R}\to\ring{R}$,
or $\cos:\ring{R}\to\ring{R}$.
The type of the union operator 
forces its two arguments to have the same type,
%the polymorphic type\footnote{More accurately, the type of the union operator is $(A\to bool)\to (A\to% bool)\to (A\to bool)$, where $ bool$ is the boolean type.} $A\to A\to A$ (that is it takes any %two objects of the same type and returns a object of that same type).
so that an attempt to take the union of the cosine function with $e$ is then flat out rejected.

HOL Light is a new axiomatic foundation with types, 
 different from the usual ZFC.
The types are presented in Box~\ref{XX}.  There are only two primitive
types, the boolean type {\it \tc bool} and an infinite type {\it \tc ind}. The rest are formed
with type variables and composition arrows.
%(the verification system that I use) are quite basic.  There is
%only one primitive type: the
%boolean type $ bool$.  There is
%an infinite collection of
%are type variables, $A,B,\ldots$ that can be named with arbitrary strings.
%There is one way to join together types to form new types: the arrow $\to$ (which takes two more types as arguments to produce a composite type, for example
%$A \to B$).    The HOL Light
%system also contains an axiom of infinity, which is used to create a type for the natural
%numbers $\ring{N}$.  The basic types are built up using the arrow $\to$, type variables,
%and the types $ bool$, $\ring{N}$.  For example, $(A\to  bool)\to (\ring{N}\to\ring{N})$
%is a valid type.  
A mechanism is also provided for creating a new type
that is in bijection with a nonempty subset of another type,
allowing the system
to be extended with types for ordered pairs, integers, rational numbers, real numbers,
and so forth.

\subsection{Terms}

Terms are the basic mathematical objects of the HOL Light system.  The syntax is based
on Church's $\lambda$-calculus, which uses the notation
   $$
   \lambda x.\ f (x)
   $$
to represent the function that takes $x$ to $f(x)$, what a mathematician would write
as $f:\ring{N}\to\ring{N}$, $x\mapsto f(x)$.  The name $\lambda$-calculus is derived
from the use of the letter $\lambda$ to represent functions.  Box~\ref{XX} lays out
the construction of terms.

%The description of terms in HOL Light is rather simple.  A term can be a variable,
%a constant, or built up from variables and constants through function applications
%and a process called $\lambda$-abstraction.
%A function application is a term of the form $f x$, where $f$ and $x$ are both
%terms.  The type of the function term $f$ must be compatible with the type of its
%argument, as explained in the following paragraph.  A $\lambda$-abstraction, written
%$\lambda x.\,P$, is
%created from a variable $x$ and a term $P$.  

%Each term has a type, forming what is called a typed $\lambda$-calculus.
%Each variable and constant is assigned a type at the moment
%it is first constructed.  The application $f x$ of a function $f:A\to B$ to a
%term $x$ of type $A$ has type $B$.  The $\lambda$-abstraction,  $\lambda x.\,P$,
%obtained by binding a variable $x$
%of type $A$ to a term $P$ of type $B$ has type $A\to B$.  

In Zermelo set theory, there is a bijection of sets
  $$
  Z^{X \times Y} \simeq (Z^Y)^X.
  $$
In other words, a function $(x,y)\mapsto f(x,y)$
from the Cartesian product of $X \times Y$ to $Z$, can be viewed as a function on
$X$ that maps $x$ to a function $f(x,\cdot):Y\to Z$.  The right-hand side of this
bijection is called the curried form of the function (named after the logician Haskell Curry).  
In typed systems, the curried form of multivariate functions is generally preferred.  If we
treat $X,Y,Z$ as types, then we would write the type of the curried function as
$f:X\to Y \to Z$.

The system has only one primitive constant,\footnote{Recall that every term that is
not a variable, a function application, or $\lambda$-abstraction is a constant.  `Constancy' is thus a broader notion here than in first-order logic, and includes terms
such as equality that take arguments.  Parantheses are drawn around the the
equality symbol $( = )$ to denote the prefixed curried form, with
$( = )\, x\, x$ an alternative syntax for $x = x$.}  
the equality symbol $( = )$
of type $\tc A\to A\to  bool$.  
That is, equality is a curried function that takes two arguments of the same type
and returns the boolean type.
%This completes the description of terms.  

%In set theory, the set is primitive and a function is secondary, identified with 
%a special kind of set of ordered pairs.
%In HOL, functions not sets are the primitive concept in the system;  a set is an afterthought,
%identified with special kind of function, its characteristic function.

%In HOL Light, all mathematial object are expressed as terms.
%Functions are the basic construct, and
%%For example, since in the $\lambda$-calculus, 
%%functions rather
%%t%han sets are the primitive construct, 
%a set is a secondary notion,
%defined indirectly as a characteristic function.

%For example, the forall
%symbol is defined as a constant of type $(A\to bool)\to bool$.  By definition,   
%$( \forall ) P$ means $P = \lambda x. \op{true}$; that is, a predicate $P$ is defined to be universally valid if it constantly returns true. 
%A bit of syntactic sugar then makes the familiar syntax
%$\forall x. f(x)$ stand for the more unwieldy $( \forall ) (\lambda x. f(x))$.  All the other notation of logic is built up in a similar way.




\bigskip
\noindent
\framebox{\parbox{4.8in}{
\parindent=0pt

\centerline{\it The HOL Light System\footnote{\tt Note to editor: typeset the entire HOL Light system on a single page.}} % XX
\smallskip

{\bf HOL Light} (Lightweight Higher Order Logic) is a foundational system designed for doing mathematical proofs on a computer.  The notation is based on a typed $\lambda$-calculus.  

\bigskip

{\bf 1. Types:}  The type system is built up freely from
{\it type variables} $\tc A, \tc B,\ldots$ and
{\it type constants}  $\tc bool$ (boolean), $\tc ind$ (infinite type),
using the 
{\it composition arrow}  $( \to )$.   The colon is used as a notational device to indicate a type.
For example, $\tc bool$, $\tc bool\to A$, and $\tc(bool\to A)\to (A\to B)$  are types. 

\bigskip
{\bf 2. Terms:}  The terms are built up inductively from
{\it variables} $x,y,\ldots$ and {\it constants} $0,\ldots$ using
{\it abstraction} ($\lambda x. t$ where $x$ is a variable and $t$ a term) and {\it application} 
($f(x)$ for compatibly typed terms $x$ and $f$).  
Each term has a type.  The notation $x\tc A$ indicates that
the type of term $x$ is $\tc A$.  Variables and constants are assigned a type at the moment of creation; the types of abstractions and applications are defined inductively: the type of $\lambda x. t$ is $\tc A\to B$ if
$x\tc A$ and $t\tc B$; the type of $f(x)$ is $\tc B$ if $f\tc A\to B$ and $x\tc A$.

\bigskip
{\bf 3. Theorems:} A theorem is a {\it sequent} $\{p_1,\ldots,p_k\} \vdash q$,
where $p_1,\ldots,p_k,q$ are terms of type $\tc bool$.
The terms $p_1,\ldots,p_k$ are called the assumptions and $q$
is called the conclusion of the sequent.
The design of the system prevents the construction of theorems except through inference rules, new definitions, and axioms.

}}

\bigskip
\noindent
\framebox{\parbox{4.8in}{
\parindent=0pt


\smallskip
{\bf 4. Inference Rules:}  The system has ten inference rules and a mechanism for defining new constants and types. Each inference rules is depicted as a fraction; the inputs to the rule are listed in the numerator, and the output in the denominator.  The inputs to the rules may be terms or other theorems.  In the following rules, we assume
that $q$ and $q'$ are equal, up to a renaming
of bound variables.    (Such $q$ and $q'$ are called $\alpha$-equivalent.)

\quad On first reading, ignore the assumption lists $\Gamma$ and $\Delta$. They propagate silently through the inference rules, but are really not what the rules are about.  When taking the union $\Gamma\cup\Delta$, 
$\alpha$-equivalent assumptions should
be considered as equal.
\smallskip

%\protect\twocolumn

%\framebox{
%\vbox{
\smallskip

Equality is reflexive:\footnote{\tt Note to editor:  Typeset rules in a double column format, five per column. } % XX
$$
\frac{t}{t=t}
$$

Equality is transitive:
$$
\frac{\Gamma \vdash p=q,\  \Delta\vdash q'=r}
{\Gamma\cup\Delta \vdash p=r}.
$$

Equals applied to equals are equal:
$$
\frac{\Gamma\vdash f=g,\ \Delta\vdash x=y}
{\Gamma\cup\Delta\vdash f(x) = g(y)}
$$

The rule of abstraction holds: validity
propagates from function body to function.
$$
\frac{x,\ \Gamma\vdash t}
{\Gamma \vdash \lambda x.\ t \hbox{\ (if $x$ is not free in $\Gamma$)}}
$$

The application of the function $x\mapsto t$ to
to $x$ gives $t$:
$$
\frac{(\lambda x. t) x}
{\vdash (\lambda x.\ t) x = t}
$$

%}}

%\pagebreak

%\framebox{\vbox{


%%ASSUME
Assume $t$, then conclude $t$.
$$
\frac{t\tc bool}
{t \vdash t}
$$

An `equality-based' version of modus ponens holds:
%% EQ_MP
$$
\frac{\Gamma\vdash q',\ \Delta \vdash q=p}
{\Gamma\cup \Delta \vdash p}
$$

If the assumption $p$ gives conclusion $q$ and the assumption $q$ gives $p$, then they
are equivalent:
$$
\frac{\Gamma \vdash p,\ \Delta\vdash q}
{(\Gamma\setminus q)\cup (\Delta\setminus p)
\vdash p=q}
$$

Type variable substitution holds.  If arbitrary types are substituted for type variables in a sequent, a theorem results.

Term variable substitution holds.  If arbitrary terms are substituted for term variables in a sequent, a theorem results.

%}}


%\protect\onecolumn
\bigskip
{\bf 5. Mathematical Axioms:} There are only three mathematical axioms.
$$\begin{array}{lll}
\hbox{Axiom of Extensionality:} &\quad\forall t.\, (\lambda x.\, t\, x) = t.\\
\hbox{Axiom of Infinity:} &\quad\exists f\tc ind\to ind.\ (\op{ONE\_ONE}\,f) \land \neg(\op{ONTO}\, f).\\
%\intertext{There is a function that is one-to-one but not onto.}
\hbox{Axiom of Choice:}&\quad  \forall P\,x.\, P x \Rightarrow  P((\hbox{@}) P).\\
\end{array}
$$
That is, every function is determined by its values. There exists a function that is one-to-one but not onto.  The Hilbert choice operator $\hbox{@}$ applied to a predicate $P$ chooses an element that satisfies the predicate, provided the
predicate is satisfiable.
% replace @ with epsilon.


}} % FRAMEBOX
\bigskip

\subsection{Theorems}


The system has ten simple rules of inference, as described in Box~\ref{XX}.
For example, the first two state that equality is reflexive and transitive.
The final two rules
of inference allow one to substitute new terms for the free
variables in a theorem and  allow one to substitute new types for the
type variables in a theorem.  Beyond these ten rules of inference are mechanisms for
defining new constants and  new types.
% (from a nonempty subset of an existing type).

There are three mathematical axioms: an axiom of extensionality that asserts
that a function is determined by the values that it takes on all imputs,
an axiom of infinity that asserts that the type $\tc ind$ is not finite, and an axiom of choice.

This primitive system 
lacks much of the customary logical notation.
There are no symbols for %logical operations such as
`and', `or', `not', and `implies.'  There are no universal or existential quantifiers.  There is no symbol for set membership.  
It is remarkable none of this is needed to express axioms.
%that the logic of the system has been stated without
%any of the traditional logical notation (such as conjunctions, implicat%ions, negations, and quantifiers).

Logical notation
is defined later.
%, through the syntax of terms as already presented.  
For example, the boolean constant $T$ (true) can be defined as the conclusion of
% grammar.
any theorem that has no assumptions.  The most accessible yet jarringly iconoclast
theorem comes from the reflexive law applied to equality itself:
$$
\vdash ( = ) ( = ) ( = ).
$$
So then $\vdash T = ((=) (=) (=))$.  Conjunction $( \land )$
is roundaboutly defined as the
curried function of type $\tc bool\to bool\to bool$ that on input $p$ and $q$
returns $(\lambda f.\ f\, p\, q) = (\lambda f.\ f\, T\, T)$; that is,
conjunction returns  true exactly when no curried function $f$ is able to
distinguish $(p,q)$ from
$(T,T)$.
The other logical operations are built with similar tricks.

All the basic theorems of mathematics up through the Fundamental Theorem of Calculus are proved from scratch in about two minutes every time the system loads,
%Fundamentals -- basic logic, properties of natural numbers, arithmetic,
%the construction of real numbers, sets,
%and basic real analysis -- are automatically generated whenever the system
%loads, 
so
that the causual user does not need to be concerned with the low-level details.
%of the primitive definitions of logical operations, the construction
%of the natural numbers, and so forth.    
Basic facts of logic and elementary mathematics are simply there in the system
to be used as needed.


\section{Soundness}

HOL Light is both an axiomatic system for doing mathematics and a computer program that implements the system.
How trustworthy is it?

If the computer is set aside for a moment, 
and the axiomatic system alone analyzed, it is known to be
consistent relative to ZFC.  That is, an inconsistency in
the HOL Light system would imply the inconsistency of ZFC. 
%?In the other direction, HOL Light is not much weaker;
% a model for ZFC can
%be constructed inside HOL augmented by an additional axiom
%of large cardinal. 



\subsection{Computer Implementation}

{\narrower\it  

You've got to prove the theorem-proving program correct. You're in a regression aren't you?
--A. Robinson.
%% page 288, Chapter 8, MacKenzie.

}

\smallskip

The more pressing question is the soundness and reliability of the computer program that implements the
logic.  An earlier section reported that a typical software program has approximately one
bug per 100 lines of computer code.  The most reliable software ever created, for example
mission-critical software written for the space shuttle,
has approximately one bug per 10,000 lines of computer
code.  Various formal proof systems vary widely in reliability, ranging
from some of the world's most carefully crafted code at the upper end, to
rubbish at the lower end.  I  confine my attention to the
upper-end.


%HOL types, terms, rules of inference and axioms have been programmed into a computer.
The computer code that implements the axioms and rules of inference is referred to as the kernel of the system.
It only takes about
500 lines of computer code to implement the kernel of HOL Light.  
(In contrast, a Linux distribution contains approximately
283 million lines of computer code.)
% http://en.wikipedia.org/wiki/Linux (Code size) 
% In a later study, the same analysis was performed for Debian GNU/Linux version 4.0.[55] This distribution contained over 283 million source lines of code.
An bug anywhere in the kernel of this system might have fatal consequences.  For example,
if one of the axioms  is incorrectly typed, it might lead to an inconsistent system.



%\subsection{Self Verification}



Yes, it is a regress; but a rather manageable regress.  The kernel
is a tiny amount of computer code, but hundreds of thousands of lines of code  are
verified by the kernel.  Eventually, there may be many millions of lines of lines that are
verified by this small kernel.  The same kernel verifies everything from
the prime number theorem to the correctness of hardware designs.

Since the kernel is so small, it can be checked on many different levels.  The
code has been written in a literate programming style for the benefit
of a human audience.  The source code is available for public scrutiny.   Indeed, the code
has been studied by emminent logicians.  
%The kernel implements types, terms,  ten rules
%of inference,  and three mathematical axioms.  
By design, the mathematical system is spartan
and clean.  The computer code has these same attributes.   A powerful type-checking mechanism within the programming language prevents a user from creating a theorem by any means except through this small fixed kernel.  Through type-checking, soundness is ensured, even after
a large community of users contributes further theorems and computer code. 
I wish to see a poster of the lines of kernel, to be taught in undergraduate courses, and published throughout the world, as
the bedrock of mathematics.  It is math commenced afresh
as executable code.

Experience from other top-tier theorem-proving systems has been that about 3 to 5 bugs have been found
in these systems over a period of 15-20 years of use.  After decades of use on many different
systems, to my knowledge, only one proof
has ever had to be retracted as a result of bug in a theorem-proving system,
and this in a system that I do not rank in the top-tier:  in 1995
a heap overflow error led to false claim that the theorem-prover REVEAL had solved the Robbins
conjecture. %% page 289, MacKenzie.
We can assert with utmost confidence that 
the error rates of top-tier theorem-proving systems are an
order of magnitude lower than
error rates in the most prestigious mathematical journals.  Indeed, since a formal proof starts with
a traditional proof, then does strictly more checking even at the human level, 
it would be hard for the outcome to be otherwise.

As an extra check, J. Harrison gave what can almost be described as 
``a formal proof in HOL Light of its own soundness.''   To get around the self-referential limitations
imposed by G\"odel, he gave two separate proofs.  In the first proof, a weakened
version of HOL Light is created, without the axiom of infinity.  The standard version is
used to give a formal proof of the soundness of the weakened version.  In the second proof, a strengthened
version of HOL Light is created, with an additional axiom giving a large cardinal.  The strengthened
version then proves the standard version sound.  These proofs go beyond traditional relative consistency
proofs in logic in two respects.  First of all, they are formal proofs, rather than conventional proofs.
Second, the proofs establish not only the soundness of the logic, but also the underlying soundness
of the computer code implementing the logic.\footnote{The soundness of the computer code is considered
relative to a semantic model of the underlying programming language.  This model may differ from
the `real-world' behavior of the programming language, a reminder that the task of verification
is never complete.}

\subsection{Export}

In the past few years, a number of
programs have been written to automatically translate a proof written in one system into
a proof in another system.  If a proof in one system is incorrect because of an underlying
flaw in the theorem-proving program itself, then the export to
a different system fails, and the underlying flaw is exposed.  (Except of course,
unless the second theorem-proving program also has a bug that is perfectly aligned with the
bug in the first system.  Since these systems are largely independently designed and implemented, 
the events of failure in different systems are
treated as nearly independent, so that the probability of a perfect
storm of failures across $n$ systems, goes to zero roughly as $p^n$, where $p$ is the individual
failure rate.)

Consider what happens when J. Harrison's proof of the soundness of HOL Light is exported.
(This has not happened yet, but should happen soon.)
The exported proof is a formal proof within a second theorem-prover that the HOL Light logic and implementation are sound.  
It will soon be within reach for several systems to give proofs
of one anothers' soundness.  When this is achieved, the probability of a false certification
of a pseudo-proof is pushed an order of magnitude closer to zero.  With a computer -- indeed with any physical artefact, whether a codex, transistor, or a flash drive made of proteins from salt-marsh bacteria --
% bug proteins 
% http://www.getusb.info/50-terabyte-flash-drive-made-of-bug-protein/
% http://www.tomshardware.com/news/bacteria-drives-store-terabytes,3125.html
it is never a matter of achieving philosophical certainty.
It is a scientific knowledge of the regularity of nature and human technology, akin to the scientific evidence
that Planck's constant $\hbar$ lies reliably within its experimental range.
Technology can push the probability of a false certification
ever closer to zero: $10^{-6}$, $10^{-9}$, $10^{-12},\ldots$. The intent
is 
that one day a system will store a million proofs without so much as a misplaced
semicolon.

A bug in the compiler, operating sytem,
or underlying hardware has the potential to compromise a formal proof. 
%% 
On the other hand, verification of hardware design, 
compilers, and computer
languages has long been one of the principal aims of formal methods.
HOL itself was initially created for hardware verification.
As early as 1989, a simple computer system from high-level language down to microprocessor was ``formally specified and mechanically
verified'' \cite{BHMY}.
%% quoted in MacKenzie page 243, ref 77.
Today, the semantics of various
high-level programming languages have been defined with complete mathematical
rigor \cite{Harper}.  
%Once the semantics are rigorously captured, mathematical theorems can be proved about the behavior of the computer code.
%  C++ example, .
In recent work that is nothing short of spectacular, X. Leroy has
developed a formally verified compiler for the C programming
language
%A compiler for a language in widespread use (a version of {\it C}) has
%been formally verified, all the way down to a widely used microprocessor 
\cite{CC}.


\section{Flyspeck}

A few years ago, I launched a project called {\it Flyspeck} to
give a formal proof of the Kepler conjecture, which asserts that
no packing of congruent balls in three-dimensional Euclidean space
can have density greater than the density of the face-centered cubic
packing (also known as the cannonball arrangement).  The name
Flyspeck is derived from the acronym FPK, for the Formal Proof of the Kepler
conjecture.  The word flyspeck is 
 appropriate for a formalization project, because it 
can mean to scrutinize.

The original proof of this theorem was unusually difficult
to check.  As an editor expressed it, ``The referees put a level of energy into this that is, in my experience, unprecedented. They ran a seminar on it for a long time. A number of people were involved, and they worked hard. They checked many local statements in the proof, and each time they found that what you claimed was in fact correct. Some of these local checks were highly non-obvious at first, and required weeks to see that they worked out\ldots
They have not been able to certify the correctness of the proof, 
and will not be able to certify it in the future, because they 
have run out of energy to devote to the problem.''  In addition to
a $300$ page text, the proof relies
on about forty thousand lines of custom computer code.  To the best of my knowledge,
the computer code was
never carefully examined by the referees.
The policy of the {\it Annals of Mathematics}
states, ``The human part of the proof, which reduces the original mathematical problem to one tractable by the computer, will be refereed for correctness in the traditional manner. The computer part may not be checked line-by-line, but will be examined for the methods by which the authors have eliminated or minimized possible sources of error$\ldots$''

Ultimately, the mathematical corpus is no more reliable than the processes
that assure its quality.  A formal proof attains a much
higher level of quality control than can be achieved by ``local checks''
and an ``examination of methods.''


Flyspeck may take as many as twenty work-years to complete. S. Obua and G. Bauer have already defended Ph.D. theses 
on the  project.  
Together with the work of
their advisor T. Nipkow (one of the principal architects of the Isabelle proof system), nearly half of the computer code
used in the proof of the Kepler conjecture is now 
certified.
% under the direction of T. Nipkow in
%Munich.  G. Bauer's thesis, together with subsequent work by Nipkow,
%give a complete formal verification of one of the three  pieces of computer
%code that are used in the proof of the Kepler conjecture. 
%A second thesis by S. Obua certifies has verified most (but still
%not all) of the second piece of computer code to be correct.
(When
the target of a formal verification is a piece of computer code, rather
than a standard mathematical text, the formalization checks that the
computer code conforms to a precise specification of the behavior
of the algorithm; certifying that the computer code is bug free.)
%The level of certification of these pieces of code is at the
%s%ame level of the most critical software... aircraft control,
%cryptography algorithms, etc.

%% Certification at this level, aircraft control systems, crypt algorithms
% Back and forth with Bauer.



\section{Full Automation}

%Many mathematicians are curious about what efforts have been made to
%produce a fully automated theorem prover with artificial intelligence.
% (not quite true)
Fifty years ago, it was famously predicted that
within a decade ``a digital computer will discover and prove
an important new mathematical theorem.''
%  MacKenzie, page 89. % H. Simon and A. Newell 
This did not happen.


Most success has been with
the development of specialized algorithms to solve special classes
of problems.  The WZ algorithm  gives  automated proofs of identities
hypergeometric sums.   Gr\"obner basis
methods solve ideal membership problems.
Wu's geometry algorithm proves theorems such as
%proves many elementary theorems in geometry such as 
Pappus' theorem
and Pascal's theorem on the ellipse.
% XX (ref).  
Tarski's algorithm  solves
problems that can be formulated in the first-order language of the real numbers.
The list of specialized algorithms is in fact enormous.

The most widely acclaimed example of a fully automated computer proof
was the solution of the Robbins conjecture in 1996.  
The conjecture asserts that an alternative definition is equivalent
to the usual definition of a Boolean algebra.
It is remarkable
because the solution did not involve any human assistance,
specialized algorithms, or software
designed with this particular problem in mind.
Just type the problem into W. McCune's general purpose
theorem prover {\it EQN}, hit return, and wait
eight days for the solution to appear  \cite{Mc1},\cite{Mc2}.

Yet the story is only a qualified success.  It has
remained almost an isolated example, rather than the first in a stream
of results.  The conjecture itself has the rather special form of a
word problem in an
abstractly defined algebraic system; a type of
problem particularly amenable to computer search.
%that only rarely involves deep mathematical insight. 
The proof that was found by
computer can be expressed as a short yet non-obvious sequence
of substitutions. (See box.)


\bigskip
\noindent
\framebox{\parbox{4.8in}{
\centerline{\it Full Automation of the Robbins Conjecture}
\smallskip
Let $S$ be a nonempty set with an associative commutative binary operation $(x,y)\mapsto xy$ and a unary operation $x\mapsto[x]$ (which, for convenience, we write synonymously as $x\mapsto \bar x$).
The Robbins conjecture (in Winker form) asserts that the general Robbins identity
   %\begin{eqn}\label{eqn:winker}
   $$
   [[ab][a\bar b]] = a
   $$
   %\end{eqn}
implies the existence of $c,d\in S$ such that $[cd]=\bar c$.  Here is the original proof that
EQN discovered, as reconstructed in \cite{fit}.
\begin{proof}  A solution is $c=x^3u$, $d=x u$, where $u=[x\bar x]$ and $x$ is arbitrary.
Abbreviate $j=[cd]$,  $e=u[x^2]\bar c$.  Over the equality sign, 
a prime indicates a direct application of the Robbins identity; a superscript
indicates a substitution of the numbered line; no superscript indicates a rewriting of abbreviations $c,d,e,j,u$.
$$
\begin{array}{lll}
%\hbox to 8in{\hss}\\
%zero
 0: [u [x^2]] &= [[x\bar x][xx]] =' x.\\
%two
 1: [x u [x u [x^2]\bar c]] &=' [   [[x u x^2] [x u [x^2]]]  [x u [x^2]\bar c]] = 
       [  [\bar c [x u [x^2]]]   [\bar c x u [x^2]] ] =' \bar c.\\ 
%four
 2: [u\bar c]&= [u[x^2 u x]]=^0 [u[x^2 u[u[x^2]]]] =' [ [[u x^2][u [x^2]]] [x^2 u [u [x^2]]] ] 
  \\&='
   [u [x^2]] =^0 x.\\ 
%seven:
 3: [j u]&= [[xcu]u]=' [[xcu][[uc][u\bar c]]] =^2 [[xcu][x[cu]]] =' x\\
%eight
 4: [x[x[x^2]u\bar c]] &=' [  [[x[u\bar c]][x u \bar c]] [x [x^2]u \bar c]] =^2 [ [[x^2][x u \bar c]] [[x^2] x u\bar c]] =' [x^2]\\
%ten:
5: [x\bar c] &=^1 [x [x u [x u [x^2]\bar c]]] =^0  [[u [x^2] ] [x u [x u [x^2]\bar c]]]  
  \\&= [[u [x^2]] [u x[ x e]]] =^4 [  [u[x[xe]]][u x[xe]] ] ='     u\\
%thirteen:
6: [j x]&=' [j[[xc][x\bar c]]] =^{5} [j[[xc]u]] =[[uxc][u[xc]]]=' u\\
[1ex]
%
7:  [cd]&= j =' [[j[x\bar c]][jx\bar c]] =^{5} [[ju][jx\bar c]] =^3 [x[j x \bar c]] 
  =^2[[\bar c u][\bar c j x]]
  \\& =^{6} [ [\bar c [jx]][\bar c j x]] =' \bar c.
 \end{array}
$$
\end{proof}
}} % parbox%framebox

\bigskip


Overall, the level today of fully automated computer proof (lying outside
special purpose algorithms) remains that of undergraduate
homework exercises: a group in which every element has order two is necessarily abelian; Cantor's theorem asserting that set is not in bijection with its powerset; 
%
%JSTOR: 2004 Annual Meeting of the Association for Symbolic Logic
%E-mail: cebrown(andrew. cmu. edu. The Theorem Proving System TPS can be ... TPS can prove automatically are: THM 1 5B: If some iterate of function f has a ...
%links.jstor.org/ sici?sici=1079-8986(200503)11%3A1%3C92%3A2AMOTA%3E2.0.CO%3B2-V - Similar pages
if some iterate of a function has a unique fixed point, then
the function has a fixed point \cite{TPS}.
Because of current limitations, fully automated proof tools
generally serve to fill in small intermediate steps of a
larger formal proof.  They are not ready to tackle major conjectures
in mathematics.

\section{Automated Discovery}

What happens if one sets aside rigour, and lets a computer explore?
A groundbreaking project was D. Lenat's 1976 Stanford thesis.
His computer program AM (for Automated Mathematician) was
designed to discover new mathematical concepts.  When AM was set loose to explore in the wild, it discovered the concepts of natural number, addition, multiplication, prime numbers, Pythagorean triples, and even the fundamental theorem of arithmetic.
% and even rediscovered the Goldbach conjecture.  (% Bundy disputes Goldbach)
The thesis touched off a firestorm of criticism and praise.

To put AM in context, consider a hypothetical program that is instructed to discover new concepts by deleting conditions
from the list of axioms defining a finite abelian group.  The computer will then immediately discover the concepts
of infinite  group,  nonabelian group,  monoid, and so forth because these concepts all
arise as subsets of the axioms.  These discoveries could be sensationalized:
{\it ``A program in Artificial Intelligence 
has made the ultimate leap from the finite to infinite, and from the abelian to the nonabelian,
rediscovering fundamental concepts in seconds that mathematicians have grappled with for centuries.''}
There are nagging questions about the shallowness of AM's discoveries; a
suggestive representation of the problem gives the answer away.

%The response in the Artificial Intelligence community to this program was a flurry of praise and criticism.
%At first pass, these
%discoveries
%sound extraordinary. On closer inspection, AM receives
%considerable coaxing
%% However, as it turns out, the discoveries of
%% AM (Automated Marionnette) were guided by a human puppeteer's strings.
%through a combination of explicitly programmed heuristics, mathematical
%structures embedded in the programming language, and direct human
%intervention. (An exegesis of AM appears in \cite{Haase}.)  For example,
%the concept of addition is already implicit in the concatenation of lists, which
%is built into the system; multiplication is a special case of the concept of operator iteration (built in); and then
%factorization follows from multiplication by operator inversion (also built in).
%The restriction of addition $X+Y=Z$ to squares gives Pythagorean triples, and
%so on.
%Although it explores a large search space,
%AM never breaks free of its ingrained structures.
%%For now, mathematicians are secure in their posts -- the tenured ones at least.
%% Clever Hans.
%% The horse Clever Hans 
%

More recent projects stir the imagination, even if the field
is still young.
The computer program  Graffiti.pc has made over one thousand
 conjectures in graph theory, expressing numerical relationships
between different graph invariants.
One open conjecture is described in Box~\ref{XX}.
% asserts that the total domination number is at least twice the
%path covering number, for every finite connected simple regular graph $G$ .  The box gives details.
% Written on the Wall II.
%% http://cms.dt.uh.edu/faculty/delavinae/research/wowII/  
%For example, Conjecture 2 from 1996 states that if $G$ is
%a simple connected finite graph, then $L_s(G) \ge 2(\bar\ell-1)$.\footnote{Here
%$G$ is simple if ;
%$L_s(G)$ is the maximum number of leaves of a spanning tree;
%for any vertex $v$, $\ell(v)$ is the independence number of the
%induced subgraph
%and $\bar\ell$ is the average of $\ell(v)$ over all vertices,
%}
%Another conjecture generating program is HR (named in honor
%of Hardy and Ramanujan). \cite{}. 
% Conjecture 4 
% http://www.lacim.uqam.ca/~plouffe/OEIS/citations/G-2002-46.pdf
%  need permission to quote.
%Another program HR (named in honor of Hardy and Ramanujan) 
%It conjectures for example (the elementary fact)
%that every refactorable number is congruent to $0,1,2$ or $4$ modulo $8$.
%(A number is refactorable if the number of divisors of $n$ divides
%$n$.).
% HR example mentioned on page 12 of C. Larson's Discovery.
No technological barriers prevent us from unleashing conjecturing
machines in all branches of mathematics,
%One can imagine the day that conjecturing in Artificial Intelligence farms many branches of mathematics  %fertile soil. % Unleash an AI program on
% regulators, conductors, ranks, height pairings, periods, and special values of $L$-functions,
to see what  moonshine they reveal.


\bigskip
\noindent
\framebox{\parbox{4.8in}{
\centerline{\it An example of an open computer-generated conjecture}
\smallskip
Let $G$ be a finite graph with the following properties:
  \begin{enumerate}
  \item It has at least two vertices.
  \item The graph is simple; that is, there are no loops or multiple joins.
  \item It is regular; that is, every vertex has the same degree.
  \item The graph is connected.
  \end{enumerate}
For example, the complete graph (the graph with an edge between every two vertices)  on $n$ vertices has these properties, when $n\ge 2$.
Define the {\it total domination number} of $G$ to be the size of the smallest
subset of vertices such that every vertex of $G$ is adjacent to some vertex in the
subset.    The {\it path covering number} is the size of the smallest partition of the vertices
into subsets, such that there exists a path confined to each subset $S$ that steps through
each vertex of S exactly once (that is, the induced graph on $S$ has a Hamiltonian path).  
\smallskip

The computer program Graffiti.pc conjectures that {\it the total domination number of $G$ is at least
twice the path covering number of $G$}  \cite{DLPWW}.
For example, the complete graph on $n$ vertices has path covering number one,
because it has a Hamiltonian path.  Its total domination number is two (take any two vertices).
The conjecture is sharp in this case by these direct observations.
}} % parbox

%%  Peterson graph.
% http://en.wikipedia.org/wiki/Petersen_graph graphic source, creative commons license.
% Peterson_graph_blue.



\section{QED}

{

\narrower\it  As a kind of dream I played (in 1968)
with the idea of a future where every
mathematician would have a machine on his desk,
all for himself, on which he would write
mathematics and which would verify his work.
But, by lack of experience in such matters, I 
expected that such machines would be available
within 5 years from then.  But now, 23 years
later, we are not that far yet.  - N.G. de Bruijn~\cite{dBRA}
% Reflections on Automath

}

\smallskip

In the Notices in 1991, de Bruijn proposed an assembly line
to turn mathematical ideas into formally verified proofs~\cite{dB91}.
The standard benchmark for the
human labor to transcribe one printed page
of textbook mathematics into machine verified formal text is one week, or \$100 per page at an outsourced wage. To undertake the formalization of just $100,000$ pages
of core mathematics 
% take %$10$ years and
%an outsourcing budget of well over \$10 million -- in the most wildly o%ptimistic of scenarios.  It would 
would be one of the most ambitious collaborative projects ever undertaken in pure mathematics, the sequencing of a `mathematical genome.'
%, a mathematical counterpart of the human genome project.  
Before it gets off the ground, a massive wiki collaboration is needed to
 settle  a % $100,000$ pages
text of the most significant theorems in contemporary mathematics, from 
Poincar\'e to Sato-Tate.  %Others would train a corps of $400$ technicians
%in developing countries in the art of formalization.
%During the final phase, lasting $250$ weeks, the technicians would perform the formalization.

Outsourcing is the brute force solution to the Q.E.D. manifesto
(an anonymous document declaring that all significant mathematical results should be preserved in a vast library of formal proofs).  Most researchers, however, prefer beauty over brute force;
we may hope for advances in our 
understanding that will permit us someday to convert a printed page of textbook mathematics into machine verified formal text in a matter of hours, rather than after a full week's labor.  As long as
transcription from traditional
proof into formal proof is based on human labor rather than
automation, formalization remains an art rather than a science.
%by human labor rather than automation, 
%What we cannot automate is art, but not science.
Until that day of automation, % transcription is largely automated,
we fall short of a practical understanding of the foundations of mathematics.
%the foundation problem is left unfinished.
%This I see as the primary technological challenge in the field of formal
%proof.  
%There has been steady progress on this technological problem over the past forty years.  
%When this technological problem is solved, we will at last have constructed de Bruijn's dream machine.




% old teacher Wilbur Knorr.

\section{Recommended Reading and Software}

By far the best overview of the subject is the book {\it Mechanizing Proof,}  winner of the 2003 Merton Book Award of
the American Sociological Association \cite{Mac}.
% Review by Hayes: http://www.americanscientist.org/template/BookReviewTypeDetail/assetid/12866.
The Q.E.D. Manifesto can be found at \cite{QED}.
Historical surveys include \cite{Bled},
%``Automated Theorem Proving after 25 Years,'' 
\cite{Ha07}, and
% A short survey of automated reasoning.
\cite{Mu}.
% Present State of Mechanical Deduction
For something more comprehensive, see \cite{Ha08}.
% Harrison's book


Several theorem proving systems are extensively documented and are available for free download,
including HOL-Light \cite{HOLL}, Isabelle \cite{Isa}, COQ \cite{COQ},
and Mizar \cite{Mizar}.  There is also a web-browser based version
of COQ, which allows one to experiment with the system without
downloading any software \cite{PW}.


\begin{thebibliography}{A}

\bibitem{TPS} P. B. Andrews and C. Brown, ``Proving theorems and teaching logic with 
TPS and ETPS,'' The Bulletin of Symbolic Logic 
Volume 11, Number 1, March 2005.
% http://www.math.ucla.edu/~asl/bsl/1101/1101-005.ps

\bibitem{BHMY} W. R. Bevier, W. A. Hunt, Jr. J Strother Moore, W. D. Young, ``An aproach to Systems Verification''  Journal of Automated Reasoning 5 (1989); 411-428, at pp. 422-423.

\bibitem{Bled} W. W. Bledsoe and D. W. Loveland (eds.), Automated
Theorem Proving: After 25 Years, Contemporary Mathematics, Vol. 29,
AMS, Providence, RI, 1984.

\bibitem{dBRA} N. G. de Bruijn, Reflections on Automath.

\bibitem{dbXY} N. G. de Bruijn, Types in Mathematics.

\bibitem{dB91}  N.G. de Bruijn, Checking Mathematics with Computer Assistance, Notices of the AMS,
Vol 38, (1), Jan. 1991.

\bibitem{COQ} The COQ proof assistant, http://coq.inria.fr/.

%\bibitem{Col} S. Colton, Automated Theory Formation in Pure
%Mathemaics,  Springer, London, 2002.

%\bibitem{ED} E. Devalina, Some History of the Development of Graffiti,

\bibitem{DLPWW} E. DeLaVina, Q. Liu, R. Pepper, B. Waller and D. B. West, On some conjectures of
Graffiti.pc on total domination, Congressus Numerantium, (2007), Vol. 185, 81-95.

\bibitem{fit}  B. Fitelson, ``Using Mathematica to Understand the Computer Proof of the Robbins Conjecture''
Mathematica in Education and Research (Winter 1998, Volume 7, No. 1).

%\bibitem{Haase} K.W. Haase, Jr., Invention and Exploration in Discovery,
%Ph.D. thesis, MIT, 1990. 


\bibitem{HOLL} J. Harrison, The HOL Light theorem prover,
http://www.cl.cam.ac.uk/\~~jrh13/hol-light/index.html

\bibitem{Ha07} J. Harrison, A short survey of automated reasoning,
Proceedings of AB 2007, the second international conference on Algebraic Biology, Springer LNCS vol. 4545, pp. x--x, 2007.

\bibitem{Ha08} J. Harrison, Introduction to Logic and Automated
Theorem Proving, 637pp. to appear.

\bibitem{Isa} Isabelle, http://isabelle.in.tum.de/.

%\bibitem{Kol} G. Kolata, Computer Math Proof
%Shows Reasoning Power, New York Times, Dec 10, 1996.
%http://www.nytimes.com/library/cyber/week/1210math.html

\bibitem{CC} X. Leroy et al.  Compcert, http://compcert.inria.fr/.

\bibitem{Mac} D. MacKenzie, Mechanizing Proof, MIT Press, Cambridge, MA,
2001.

\bibitem{Mc1} W. McCune, Robbins Algebras are Boolean, http://www.cs.unm.edu/~mccune/papers/robbins/

\bibitem{Mc2} W. McCune, Solution of the Robbins Problem, JAR 19(3), 263--276 (1997)

\bibitem{Harper} R. Milner, M. Tofte, R. Harper, The Definition of Standard ML, MIT Press, 1990.

\bibitem{Mizar} Mizar Home Page, http://mizar.org/.

\bibitem{Mu} R. Murawski, The Present State of Mechanized
Deduction, and the Present Knowledge of Limitations,
Studies in Logic, Grammar and Rhetoric 
year: 2006, vol: 9, number: 22, pages: 31-60.

%\bibitem{Nor} M. Norrish, 

\bibitem{PW} ProofWeb, http://prover.cs.ru.nl/login.php.

\bibitem{QED} The QED Manifesto, ``Automated Deduction - CADE 12'', Springer-Verlag, Lecture Notes in Artificial Intelligence, Vol. 814, pp. 238-251, 1994.
http://www.cs.ru.nl/\~~freek/qed/qed.html.

\end{thebibliography}


\end{document}

