% Formal Proof: A Mathematician's Perspective
% Author: Thomas C. Hales
% Affiliation: University of Pittsburgh
% email: hales@pitt.edu
%
% latex format

% History.  File started Feb 18, 2008

%% 


\documentclass{llncs}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{alltt}


% Math notation.
\def\op#1{{\hbox{#1}}} %operatorname XX
\def\tc{\hbox{:}}
\newcommand{\ring}[1]{\mathbb{#1}}

% Flags


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Formal Proof}%: A Mathematician's Perspective}
\author{Thomas C. Hales\thanks{The author's research on the Formal Foundations of Discrete Geometry has been supported by NSF grant 0503447.}}
\institute{University of Pittsburgh\\
\email{hales@pitt.edu}}
\maketitle

%\begin{abstract}  
%A mathematician gives his perspective on formal %proof.
%\end{abstract}

\centerline{\it dedicated to N.G. de Bruijn}
% See gmail correspondence with Henk.
%XX add thanks to Henk and group, where this article was written.


\bigskip

{\narrower\it 

There remains but one course 
for the recovery of a sound and healthy condition --
namely, that the entire work of the understanding be commenced afresh, and the mind itself be from the very outset not left to take its own course, but
guided at every step; and the business be done
as if by machinery.  F. Bacon, 1620, Novum Organun
% preface.

}

\bigskip


%%%%%%%%%%%%%%%%%%%%%%

\bigskip

\section{Bugs}

Daily, we confront the errors of computers.  They crash,
hang, succumb to viruses, run buggy software, and harbor
spyware.  Our  tabloids report bizarre computer glitches:
the library patron who is fined \$40 trillion for an overdue
book, because a  
barcode is scanned as the size of the fine; or the 
dentist in San Diego who was delivered over 16 thousand tax forms
to his doorstep because he abbreviated ``suite'' 
in his address as ``su.''

  On average,
a programmer introduces 1.5 bugs per line while typing.
Most are typing errors that are spotted at once.
About one bug per hundred lines of computer code ships  to
market without detection.  Bugs are an accepted
part of programming culture.
The book that describes itself as the ``bestselling software testing
book of all time'' states that ``testers shouldn't want to verify
that a program runs correctly.'' 
% Wiley,
%Testing Computer Software
%The bestselling software testing book of all time!
%Second Edition
%Cem Kaner, Jack Falk, Hung Quoc Nguyen
%1999
Another book on software
testing states ``Don't insist that every bug be fixed $\ldots$
When the programmer fixes a minor bug, he might create
a more serious one.''  Corporations may keep critical bugs
off the books to
limit legal liability.
 Only those bugs should be corrected
that affect corporate profit.
The tools designed to root out bugs are themselves
full of bugs. ``Indeed, test tools are often buggier than
comparable (but cheaper) development tools.''
% Lessons Learned in Software Testing, Kaner, Bach, Pettichord 2001.
As for hardware reliability, former 
Intel President Andy Grove himself said 
``I have come to the conclusion that no microprocessor is ever
perfect; they just come closer to perfection $\ldots$''
% quoted p221. MacKenzie, Mechanizing Proof.
%(at the time of the famous pentium bug) 


Bugs can be far-reaching.
The bug causing the 
explosion of the Ariane 5 rocket cost hundreds of millions
of dollars.  As long ago as 1854, Thoreau wrote that 
``by the error of some calculator
the vessel often splits upon a rock that should have reached
a friendly pier.''  % Walden, Economy, page 13.
Last year, the New York Times reported Shamir's warning that
even a small math error in a widely used computer chip could 
be exploited to defeat cryptography and would
place
``the security of the global electronic commerce system at risk
$\ldots$''
% Adding Math to List of Security Threats, New York Times, November 17, 2007.

As D. Knuth has said, 
{

\narrower\it

There is no royal road, and you can't make any complex program totally failsafe. You can test a program for a year and then someone or something will create a condition that nobody ever anticipated and then very subtle errors become very visible. The layers that you place on a program to make it failsafe can themselves fail. Computer programs are the most complicated things that humans have ever created. 
% http://www.literateprogramming.com/byte1996.html

}



\section{Mathematical Certainty}

By contrast, philosophers tell us that
mathematics consists of analytic truths,
free of all imperfection.  We prove that $1+1=2$ by
recalling the definition of $1$ as the successor of $0$,
$2$ as the successor of $1$, and then invoking twice the recursive
definition
of addition: 
  $$1+1 = 1 + S(0) = S(1 + 0) = S(1) = 2.$$

If only all mathematical proofs were so impeccable.  
The history of mathematical error is as old as mathematics itself.
Euclid's very first proposition asks, ``on a given straight line
to construct an equilateral triangle.''  Euclid's construction
makes the implicit assumption -- not justified by the axioms -- that
two circles, each passing through the other's center, must intersect.
We revere Euclid, not because he got everything right, but because
he set us on the right path.

We have entered a era of proofs of extraordinary complexity.
%% XX My early experiences with Langlands theory 
Take, for example, F. Almgren's masterpiece in geometric measure
theory, called appropriately enough the ``Big Paper.'' 
%http://www.worldscibooks.com/mathematics/4253.html
The preprint is
1728 pages long. Each line is a chore. He spent over a decade writing it in the 70s and
early 80s.  It was not published until 2000.  Yet the theorem
is fundamental.  It establishes the regularity of minimizing
rectifiable currents, up to codimension two;  that is, 
it shows that higher dimensional soap bubbles are smooth
rather than jagged -- just
as one would naturally expect.  How am I to develop enough confidence
in the proof that I am willing to cite it as result in my own research?
%(I do need to cite it in \cite{XX}.) 
Do the stellar reputations of the author
and editors suffice, or should I try to understand the details of the
proof?  I
would consider myself very fortunate, if I could work through the proof
in a year.

Computer proofs 
compound the complexity. % that mathematicians face.
Today, there are many major theorems, starting with the proof
of the four-color theorem.
%% XX Lam had collaborators.
C. W. H. Lam made an 800-day computation on a VAX computer to
prove the non-existence of a projective plane of order 10.
 W. Tucker solved Smale's 14th problem by computer, establishing
that the Lorenz equations have a strange attractor. 
A. Kumar and H. Cohn have used computers to prove 
that the Leech lattice gives
the sphere packing of optimal density among all lattices in $24$
dimensions.
The isoperimetric inequality in three dimensions states the
sphere provides the surface-minimizing way to enclose a fixed volume.
The double bubble conjecture, which is the generalization of
the isoperimetric inequality to two equal volumes, was originally solved
by computer.  The solution is two congruent spheres, fused along
a flat disk meeting the spheres at $120$ degrees (as required
by Plateau's conditions).
Computers have even been used in the study of hyperbolic $3$-manifolds.
% 800 days at
% The Search for a Finite Projective Plane of Order 10
% http://www.cecm.sfu.ca/organics/papers/lam/paper/html/node5.html
%
%http://arxiv.org/abs/math/9609207 Homotopy 3-manifolds.
 


\section{Formal Proof}

A formal proof is a proof in which every logical inference has
been checked all the way back to the fundamental axioms of mathematics.
Traditional mathematical proofs are written in a way to make them easily understood by mathematicians. Routine logical steps are omitted. An enormous amount of context is assumed on the part of the reader. Proofs, especially in topology and geometry, rely on intuitive arguments in situations where a trained mathematician would be capable of translating those intuitive arguments into a more rigorous argument.

In a formal proof, all the intermediate logical steps are supplied. No appeal is made to intuition, even if the translation from intuition to logic is routine. Thus, a formal proof is less intuitive, and yet less susceptible to logical errors.

There is a wide gulf that separates traditional proof from formal proof.
For example, Bourbaki's Theory of Sets was designed as a purely theoretical
edifice that was never intended to be used in the proof of actual theorems.
Indeed, Bourbaki declares that ``formalized mathematics cannot
in practice be written down in full'' and calls such a project
``absolutely unrealizable''  % Bourbaki, Elements of Sets, Addison-Wesley, Reading, MA, 1968,  pp.10,11.
The basic trouble with these systems is that meta-mathematical arguments (for
example, abbreviations that are external to the system, or
inductions over the syntactical form of an expression) 
are usually introduced early on, and without these simplifying meta-arguments,
the vehicle stalls, never making it up the steep incline from primitive notions to 
high-level concept.   The gulf can be extreme: Matthias has calculated
that to expand the definition of the number `$1$' fully in terms of Bourbaki primitives requires
over $4$ trillion symbols.
In Bourbaki's view, 
the foundations of mathematics are roped-off museum pieces
to be silently observed, 
but not handled directly.

There is an opposing view that regards the 
foundational enterprise
as unfinished until it is realized in practice and written down in full.
This article sketches the current state of this endeavor.
It has been necessary to commence afresh, and to retool the foundations
of mathematics for practical efficiency, while preserving
its reliability and austere beauty.  For anything beyond a trivial
proof, the number of logical inferences is so large that a computer is
used to ensure that no steps are omitted.   This raises basic questions
about trust in computers.

As the art is currently practised, each formal proof starts with a traditional
mathematical proof, which is rewritten in a greatly expanded form, where all the
assumptions are made explicit and all cases are treated in full.
For example, a traditional mathematical proof might show that a graph is
planar by drawing the graph on a sheet of paper.  The expanded form of
the proof  replaces the picture by careful argument.  From the
expanded text, a computer script is prepared, which generates all
the logical inferences of the proof.  The transcription of a traditional
proof into a formal proof is a major undertaking.

What theorems, then, have been proved formally?  The most spectacular
example is Gonthier's formal proof of the four-color theorem.  His
starting point is the 1996XX? ``traditional''
proof by Robertson et al.  Although the traditional proof uses a computer
and Gonthier uses a computer,  the two computer calculations
differ from one another in the same way that a traditional proof differs
from a formal proof.  They differ in the same way that adding $1+1=2$ on
a calculator differs from the mathematical
justification of $1+1=2$ by definitions
and recursion.  That is, a large gulf separates them.
As a result of Gonthier's formalization,
the proof of the four-color theorem has become one of the most meticulously verified proofs
in history.

In the past few years, several other significant theorems have been
formally verified. See Table~\ref{table}.  The table lists the
theorems, which formal proof system was used, the person who
produced a formal proof, and the mathematicians who produced the
original proof.  The Prime Number Theorem, asserting that the
number of primes less than $n$ is asymptotic to $n/\log\,n$, has
two essentially different proofs: the elementary proof of
Selberg and Erd\"os and the analytic proof of Hadamard and
de la Vall\'ee Poussin.  Formal versions of both proofs have
been produced.



\smallskip

\begin{table}[ht]
\caption{Examples of Formal Proofs}
\centering
\begin{tabular}{l l l l l}
\hline
Year\hspace{0.5em} & Theorem\hspace{6em} & Proof System\hspace{2em}  & Formalizer\hspace{3em} & Traditional Proof\\ [0.5ex]
\hline \\
1994 & First Incompleteness & Boyer-Moore   & Shankar &  G\"odel \\
2004 & Prime Number & Isabelle & Avigad et al. & Selberg-Erd\"os\\
2004 & Four Color & COQ & Gonthier & Robertson et al.\\
2005 & Jordan Curve  & HOL Light & Hales & Thomassen \\
2005 & Brouwer Fixed Point & HOL Light & Harrison & Kuhn \\
2006 & Flyspeck I & Isabelle & Bauer-Nipkow & Hales \\
2007 & Cauchy Residue & HOL Light & Harrison & classical \\
2008 & Prime Number & HOL Light & Harrison & analytic proof \\
% 2008 & Flyspeck II & Isabelle & Obua & Hales \\
 [1ex]
\hline
\end{tabular}
\label{table}
\end{table}


Although details differ, 
in a typical system, the primitive inference rules and axioms
become bits of data that are processed by computer programs.
For example, to give a formal proof that 
$$
2682440^4 + 15365639^4 + 18796760^4 = 20615673^4
$$
a human is not required to type each primitive inference.  Instead,
a programmer writes one general procedure that takes any arithmetic
identity as input,  generates the inferences,
and produces the theorem as output.   A large number of such
small decision procedures are programmed into the system to handle
routine tasks such as polynomial simplification, basic tautologies in logic,
handheld calculator operations, 
and decidable fragments of arithmetic.  
Procedures that automatically search for steps in a proof
are also programmed into the computer.  
%(XX Sledgehammer tactic.)
New procedures may be contributed
by any user at any time to automate further tasks.
The design of the kernel of the system prevents a rogue
user from writing code that could compromise the soundness of the system (Section~\ref{XX}).


A large library is maintained
of all previously established proofs in the system, and anyone may
use any result that has been previously established.
 Although every step of every proof is
always checked,
as others contribute procedures and theorems to the system,
interaction with the system gradually moves
away from the primitive foundations towards something more closely
resembling the high-level practice of mathematicians.


\section{Jordan Curve Theorem}

C. Thomassen's proof of the Jordan Curve theorem is based
on the non-planarity of the $K_{3,3}$ graph (Figure~\ref{XX}).
The formal proof of the Jodan Curve theorem was based on 
a traditional proof by C. Thomassen.  

XX 130 defs, 80 million inferences, etc.

Box~\ref{XX} displays the statement of the Jordan Curve theorem, in computer
readable form, as it appears in the formal
proof.  The complete specification of the theorem should also list
all definitions, all the way back to the fundamentals.  In particular, {\it top2} refers
to the standard topology on the plane; {\it top2}~$A$ indicates that $A$ is an open set
in the plane;  {\it euclid} $2$ is the Euclidean plane; {\it connected top2}~$A$ means that
$A$ is a connected set in the plane.

\bigskip
\noindent
\framebox{\parbox{4.8in}{
\parindent=0pt

\centerline{\it The Formal Jordan Curve Theorem}
\smallskip

\vbox{
\def\hb{\hfill\break}
\def\h{\hbox{}}

\obeylines

  {\tt %let JORDAN\_CURVE\_THEOREM = prove\_by\_refinement(
  \h~~`$\forall C$. {\it simple\_closed\_curve} {\it top2} $C$ $\Rightarrow$\hb
  \h~~~~($\exists A\, B$.  {\it top2} $A$~$\wedge$  {\it top2} $B$ $\wedge$\hb
  \h~~~~~~{\it connected} {\it top2} $A$~$\wedge$  {\it connected top2} $B$ $\wedge$\hb
  \h~~~~~~($A \ne \emptyset$) $\wedge$ ($B \ne \emptyset$) $\wedge$\hb
  \h~~~~~~($A \cap B = \emptyset$) $\wedge$ ($A \cap C = \emptyset$) $\wedge$ ($B \cap C = \emptyset$) $\wedge$\hb
  \h~~~~~~($A \cup B \cup C =~${\it euclid}~$2$))`
  \h~~%$\cdots$);;

}

}


}}




\section{Flyspeck}

A few years ago, I launched a project called {\it Flyspeck} to
give a formal proof of the Kepler conjecture, which asserts that
no packing of congruent balls in three-dimensional Euclidean space
can have density greater than the density of the face-centered cubic
packing (also known as the cannonball arrangement).  The name
Flyspeck is derived from the acronym FPK, for the Formal Proof of the Kepler
conjecture.  The word flyspeck can mean to scrutinize or examine in
detail, which is quite appropriate for a formalization project.

I estimate that it may take as many as twenty work-years to complete
the project. Two Ph.D. theses in computer science have already been
defended on the Flyspeck project, under the direction of T. Nipkow in
Munich.  G. Bauer's thesis, together with subsequent work by Nipkow,
give a complete formal verification of one of the three  pieces of computer
code that are used in the proof of the Kepler conjecture.  When
the target of a formal verification is a piece of computer code, rather
than a standard mathematical text, the formalization checks that the
computer code conforms to a precise specification of the behavior
of the algorithm; certifying that the computer code is bug free.

%% Certification at this level, aircraft control systems, crypt algorithms
% XX Back and forth with Bauer.

How do we know that the computer software performing the formalization
is not itself full of bugs?  To answer this question, I will first discuss
the design of a particular theorem proving system, HOL Light.

\section{HOL Light}

Over the past several years, I have used a system called HOL Light  
(an acronym for a lightweight implementation of Higher Order Logic.)  There are several competing systems to choose from. %, with names like
%as HOL, Mizar, Isabelle, and COQ.  
People argue about the relative merits of the
different systems much in the same way that people argue about the relative merits
of operating systems, political loyalties, or programming languages.  To some extent, preferences
show a geographical bias:  HOL in the UK, Mizar in Poland, Isabelle in Germany, and
COQ in France.



These systems currently take a considerable effort to learn.  The hope
is they will eventually become sufficiently friendly
to become part of the 
mathematical workplace, much as email, \TeX\relax, computer algebra systems, and
web browsers are today.
I would say that the overall effort is comparable to the effort required to become a \TeX\relax-pert,
or advanced user in a computer algebra system such as SAGE, although
perhaps a larger initial investiment is required to get started in a
formal system.

\subsection{Types}

Most day to day mathematics is done at a sufficiently high level of abstraction that is
indifferent to the foundations.  For example, the published proof
of Fermat's Last Theorem does not discuss whether it is a theorem
in ZFC, or whether Grothendieck's universes from EGA slip in. In some sense, widespread independence of foundations is good news, because it allows us to shift
away from ZFC to a different foundational system with equanimity.  

Many
formal proof systems that have been implemented on a computer are based on
types.  Types are familiar to computer programmers.  In a typed computer language,
$3$ is a integer and $[1.0;2.0;3.0]$ is an array of floating point numbers. An attempt
to add $3$ to this array results in a type mismatch error, and the computer program
will not compile.  The type checking mechanism of programming languages
conveniently detects many bugs at the time
of compilation.  %This is an extremely useful way to detect bugs in computer code.


Zermelo-Fraenkel set theory has no such type checking mechanism.  As de Bruijn
puts it,
``Theoretically, it seems perfectly legitimate
to ask whether the union of the cosine function
and the number $e$ (the basis of natural
logarithms) contains a finite geometry.''
%- N. G. de Bruijn, Types in Mathematics, page 29
Mathematicians have the good sense not to ask such questions.  However, when moving
mathematics to a computer, lacking in common sense, it is useful to introduce types
into the foundations
to prevent this kind of nonsense.  By convention,  a colon is written before the name of a type.  For instance, we write the type of the real number $e$ as $\tc\ring{R}$, or simply $e:\ring{R}$.
The cosine function has a different type $\tc\ring{R}\to\ring{R}$,
or $\cos:\ring{R}\to\ring{R}$.
The type of the union operator 
forces its two arguments to have the same type,
%the polymorphic type\footnote{More accurately, the type of the union operator is $(A\to bool)\to (A\to% bool)\to (A\to bool)$, where $ bool$ is the boolean type.} $A\to A\to A$ (that is it takes any %two objects of the same type and returns a object of that same type).
so that an attempt to take the union of the cosine function with $e$ is then flat out rejected.

The types in HOL Light are presented in Box~\ref{XX}.  There are only two primitive
types, the boolean type {\it \tc bool} and an infinite type {\it \tc ind}, the rest are formed
with type variables and arrow compositions.
%(the verification system that I use) are quite basic.  There is
%only one primitive type: the
%boolean type $ bool$.  There is
%an infinite collection of
%are type variables, $A,B,\ldots$ that can be named with arbitrary strings.
%There is one way to join together types to form new types: the arrow $\to$ (which takes two more types as arguments to produce a composite type, for example
%$A \to B$).    The HOL Light
%system also contains an axiom of infinity, which is used to create a type for the natural
%numbers $\ring{N}$.  The basic types are built up using the arrow $\to$, type variables,
%and the types $ bool$, $\ring{N}$.  For example, $(A\to  bool)\to (\ring{N}\to\ring{N})$
%is a valid type.  
The axioms of HOL Light also provide a mechanism for creating a new type
that is in bijection with a nonempty subset of another type.  This mechanism allows the system
to be extended with types for ordered pairs, integers, rational numbers, real numbers,
and so forth.

\subsection{Terms}

Terms are the basic mathematical objects of the HOL Light system.  The syntax is based
on Church's $\lambda$-calculus, which uses, for example, the notation
   $$
   \lambda x.\ f (x)
   $$
to represent the function that takes $x$ to $f(x)$, what a mathematician would write
as $f:\ring{N}\to\ring{N}$, $x\mapsto x+1$.  The name $\lambda$-calculus is derived
from the use of the letter $\lambda$ to represent functions.  Box~\ref{XX} lays out
the construction of terms in HOL Light.

%The description of terms in HOL Light is rather simple.  A term can be a variable,
%a constant, or built up from variables and constants through function applications
%and a process called $\lambda$-abstraction.
%A function application is a term of the form $f x$, where $f$ and $x$ are both
%terms.  The type of the function term $f$ must be compatible with the type of its
%argument, as explained in the following paragraph.  A $\lambda$-abstraction, written
%$\lambda x.\,P$, is
%created from a variable $x$ and a term $P$.  

%Each term has a type, forming what is called a typed $\lambda$-calculus.
%Each variable and constant is assigned a type at the moment
%it is first constructed.  The application $f x$ of a function $f:A\to B$ to a
%term $x$ of type $A$ has type $B$.  The $\lambda$-abstraction,  $\lambda x.\,P$,
%obtained by binding a variable $x$
%of type $A$ to a term $P$ of type $B$ has type $A\to B$.  

In standard set theory, there is a bijection of sets
  $$
  Z^{X \times Y} \simeq (Z^Y)^X.
  $$
In other words a function $(x,y)\mapsto f(x,y)$
from the Cartesian product of $X \times Y$ to $Z$, can be viewed as a function on
$X$ that maps $x$ to a function $f(x,\cdot):Y\to Z$.  The right-hand side of this
bijection is called the curried form of the function (named after the logician Haskell Curry).  
In a typed system, such
as HOL, the curried form of multivariate functions is generally preferred.  If we
treat $X,Y,Z$ as types, then we would write the type of the curried function as
$f:X\to Y \to Z$.

The system has only one primitive constant,\footnote{Recall every term that is
not a variable, a function application, or $\lambda$-abstraction is a constant.  `Constancy' is thus a broader notion here than in first-order logic, and includes terms
such as equality that take arguments.}  
the equality symbol $( = )$
of type $\tc A\to A\to  bool$.  
That is, equality is a curried function that takes two arguments of the same type
and returns the boolean type.
%This completes the description of terms.  

%In set theory, the set is primitive and a function is secondary, identified with 
%a special kind of set of ordered pairs.
%In HOL, functions not sets are the primitive concept in the system;  a set is an afterthought,
%identified with special kind of function, its characteristic function.


The specification of terms lacks much of the customary logical notation.
There are no symbols for logical operations such as
`and', `or', `not', and `implies.'  There are no univerasal or existential quantifiers.  There are no sets, or symbol for set membership.  All of this
is defined later, through the syntax of terms as already presented.  
For example, since in the $\lambda$-calculus, functions rather
than sets are the primitive construct, a set is a secondary notion,
defined indirectly as a characteristic function.
%For example, the forall
%symbol is defined as a constant of type $(A\to bool)\to bool$.  By definition,   
%$( \forall ) P$ means $P = \lambda x. \op{true}$; that is, a predicate $P$ is defined to be universally valid if it constantly returns true. 
%A bit of syntactic sugar then makes the familiar syntax
%$\forall x. f(x)$ stand for the more unwieldy $( \forall ) (\lambda x. f(x))$.  All the other notation of logic is built up in a similar way.


{\tt XX Typeset the entire HOL Light system on a single page.}

\bigskip
\noindent
\framebox{\parbox{4.8in}{
\parindent=0pt

\centerline{\it The HOL Light System}
\smallskip

{\bf HOL Light} (Lightweight Higher Order Logic) is a foundational system designed for doing mathematical proofs on a computer.  The notation is based on a typed $\lambda$-calculus.  

\bigskip

{\bf 1. Types:}  The type system is built up freely from
{\it type variables} $\tc A, \tc B,\ldots$ and
{\it type constants}  $\tc bool$ (boolean), $\tc ind$ (infinite type),
using the 
{\it composition arrow}  $( \to )$.   The colon is used as a notational device to indicate a type.
For example, $\tc bool$, $\tc bool\to A$, and $\tc(bool\to A)\to (A\to B)$  are types. 

\bigskip
{\bf 2. Terms:}  The terms are built up inductively from
{\it variables} $x,y,\ldots$ and {\it constants} $0,\ldots$ using
{\it abstraction} ($\lambda x. t$ where $x$ is a variable and $t$ a term) and {\it application} 
($f(x)$ for compatibly typed terms $x$ and $f$).  
Each term has a type.  The notation $x\tc A$ indicates that
the type of term $x$ is $\tc A$.  Variables and constants are assigned a type at the moment of creation; the types of abstractions and applications are defined inductively: the type of $\lambda x. t$ is $\tc A\to B$ if
$x\tc A$ and $t\tc B$; the type of $f(x)$ is $\tc B$ if $f\tc A\to B$ and $x\tc A$.

\bigskip
{\bf 3. Theorems:} A theorem is a sequent $\{p_1,\ldots,p_k\} \vdash q$,
where $p_1,\ldots,p_k,q$ are terms of type $\tc bool$.
The terms $p_1,\ldots,p_k$ are called the assumptions and $q$
is called the conclusion of the sequent.
The design of the system prevents the construction of theorems except through inference rules, new definitions, and axioms.

}}

\bigskip
\noindent
\framebox{\parbox{4.8in}{
\parindent=0pt


\smallskip
{\bf 4. Inference Rules:}  The system has ten inference rules and a mechanism for defining new constants and types. Each inference rules is depicted as a fraction; the inputs to the rule are listed in the numerator, and the output in the denominator.  Some inputs to the rules are terms; other inputs are theorems.  In the following rules, we assume
that $q$ and $q'$ are equal, up to a renaming
of bound variables.    (Such $q$ and $q'$ are called $\alpha$-equivalent.)

\quad On first reading, ignore the assumption lists $\Gamma$ and $\Delta$. They propagate silently through the inference rules, but are really not what the rules are about.  When taking the union $\Gamma\cup\Delta$, 
$\alpha$-equivalent assumptions should
be considered as equal.
\smallskip

%\protect\twocolumn

%\framebox{
%\vbox{
{\tt XX Typeset rules in a double column format, five per column.}
\smallskip

Equality is reflexive:
$$
\frac{t}{t=t}
$$

Equality is transitive:
$$
\frac{\Gamma \vdash p=q,\  \Delta\vdash q'=r}
{\Gamma\cup\Delta \vdash p=r}.
$$

Equals applied to equals are equal:
$$
\frac{\Gamma\vdash f=g,\ \Delta\vdash x=y}
{\Gamma\cup\Delta\vdash f(x) = g(y)}
$$

The rule of abstraction holds: validity
propagates from function body to function.
$$
\frac{x,\ \Gamma\vdash t}
{\Gamma \vdash \lambda x.\ t \hbox{\ (if $x$ is not free in $\Gamma$)}}
$$

The application of the function $x\mapsto t$ to
to $x$ gives $t$:
$$
\frac{(\lambda x. t) x}
{\vdash (\lambda x.\ t) x = t}
$$

%}}

%\pagebreak

%\framebox{\vbox{


%% ASSUME
Assume $t$, then conclude $t$.
$$
\frac{t\tc bool}
{t \vdash t}
$$

An `equality-based' version of modus ponens holds:
%% EQ_MP
$$
\frac{\Gamma \vdash q=p,\ \Delta\vdash q'}
{\Gamma\cup \Delta \vdash p}
$$

If the assumption $p$ gives $q$ and the assumption $q$ gives $p$, then they
are equivalent:
$$
\frac{\Gamma \vdash p,\ \Delta\vdash q}
{(\Gamma\setminus q)\cup (\Delta\setminus p)
\vdash p=q}
$$

Type variable substitution holds.  If arbitrary types are substituted for type variables in a sequent, a theorem results.

Term variable substitution holds.  If arbitrary terms are substituted for term variables in a sequent, a theorem results.

%}}


%\protect\onecolumn
\bigskip
{\bf 5. Mathematical Axioms:} There are only three mathematical axioms.
$$\begin{array}{lll}
\hbox{Axiom of Extensionality:} &\quad\forall t.\, (\lambda x.\, t\, x) = t.\\
\hbox{Axiom of Infinity:} &\quad\exists f\tc ind\to ind.\ (\op{ONE\_ONE}\,f) \land \neg(\op{ONTO}\, f).\\
%\intertext{There is a function that is one-to-one but not onto.}
\hbox{Axiom of Choice:}&\quad  \forall P\,x.\, P x \Rightarrow  P((\hbox{@}) P).\\
\end{array}
$$
That is, every function is determined by its values. There exists a function that is one-to-one but not onto.  The Hilbert choice operator $\hbox{@}$ applied to a predicate $P$ chooses an element that satisfies the predicate, provided the
predicate is satisfiable.
% replace @ with epsilon.


}} % FRAMEBOX
\bigskip

\subsection{Theorems}

The theorems of the system are described in Box~\ref{XX}.
%A theorem is constructed from a pair $([p_1;\ldots;p_k], q)$, called a {\it seq%uent}, where $p_1,\ldots,p_k,q$
%are terms of boolean type.  The terms $p_1,\ldots,p_k$ are the hypotheses and $%q$ is the conclusion.
%The theorem is written in the form
%$$%%
%	p_1,\ldots,p_k \vdash    q.
%$$
The system has ten simple rules of inference, as described in Box~\ref{XX}.
For example, the first two state that equality is reflexive and transitive.
It is remarkable that the logic of the system has been stated without
any of the traditional logical notation (such as conjunctions, implications, negations, and quantifiers).
%
%The system is designed in such a way that theorems can only be constructed from
%the ten rules of logical inference and three mathematical axioms.
%I give a somewhat simplified account of the axioms and logic here.  See~\cite{XX} for details.
%The first rule of inference is the reflexive law of equality.  For any term $t$,
%we may infer the theorem
%$$
%   \vdash t = t.
%$$
%The second is the law of transitivity of equality, if we are given theorems $\vdash p=q$
%and $\vdash q=r$, we may infer
%$
%   \vdash p=r
%$.
%The third rule governs function application, if two functions are equal $\vdash f=g$
%and two arguments are equal $\vdash x = y$, then a rule of inference affirms the
%equality of the functions applied to their arguments:
%$$
%  \vdash f x = g y
%$$
%Similarly, for $\lambda$-abstraction: if $\vdash p = q$, then we may infer that
%$$
%  \vdash \lambda x.\ p = \lambda x.\ q.
%$$
%The fifth rule of inference relates $\lambda$-abstraction to function application. 
%For any term $t$, the function that sends $x$ to $t$, when applied to $x$, gives $t$:
%$$
%  \vdash (\lambda x. t) x = t.
%$$
%
%There are three inference rules that govern the turnstile symbol $(\vdash)$.  One is that for any term $t$, we may infer
%$
%  t \vdash t.
%$  That is, with $t$ as an assumption, we have $t$ as the conclusion.
%Another is that from theorems $\vdash p=q$ and $\vdash r$, we may infer $\vdash p=r$.
%The final turnstile rule, allows one to conclude $\vdash p=q$ from $p \vdash q$ and $q \vdash p$.
%
%XX Rewrite, with summary sheet of axioms.
%The first four logical rules govern equality.  Equality is reflexive and transitive, and equals
%can be substituted for equals into functions and
%$\lambda$-abstractions.  The fifth axiom ties
%function creation (that is $\lambda$-abstraction) with function application: the function $\lambda x. P$ applied to $x$ is $P$.
%
The final two rules
of inference allow one to substitute new terms for the free
variables in a theorem and  allow one to substitute new types for the
type variables in a theorem.  Beyond these ten rules of inference are mechanisms for
defining new constants and creating a new type (from a nonempty subset of an existing type).

There are three mathematical axioms: an axiom of extensionality that asserts
that a function is determined by the values that it takes on all imputs,
an axiom of infinity that asserts that the type $\tc ind$ is not finite, and an axiom of choice.



\section{Soundness}

HOL Light is both an axiomatic system for doing mathematics and a computer program that implements the system.
We may ask how trustworthy it is.

If the computer program is set aside for a moment, 
and the axiomatic system analyzed, it has been shown that
it  is consistent relative to ZFC.  That is, an inconsistency in
the HOL Light system would imply the inconsistency of ZFC. 
In the other direction, XX a model for ZFC can
be constructed inside HOL augmented by an additional axiom
of large cardinal. 

The more serious question is the soundness and reliability of the computer program that implements the
logic.  An earlier section reported that a typical software program has approximately one
bug per 100 lines of computer code.  The most reliable software ever created, for example
mission-critical software written for the space shuttle,
has approximately one bug per 10,000 lines of computer
code.  

\subsection{Computer Implementation}




HOL types, terms, rules of inference and axioms have been programmed into a computer.
The computer code that implements this is referred to as the kernel of the system.
It only takes about
500 lines of computer code to implement the kernel.  
(In contrast, a Linux distribution contains approximately
283 million lines of computer code.)
% http://en.wikipedia.org/wiki/Linux (Code size) 
% In a later study, the same analysis was performed for Debian GNU/Linux version 4.0.[55] This distribution contained over 283 million source lines of code.
An bug anywhere in the kernel of this system might have fatal consequences.  For example,
if one of the axioms  is encoded incorrectly, it might lead to an inconsistent system.



XX Users Cannot subvert

\subsection{Self Verification}

{\narrower\it  

You've got to prove the theorem-proving program correct. You're in a regression aren't you?
--A. Robinson.
%% page 288, Chapter 8, MacKenzie.

}

\smallskip

Yes, it is a regress; but a rather manageable regress.  The kernel of HOL Light
is only about 500 lines of code, but hundreds of thousands of lines of code  are
verified by HOL Light.  Eventually, there may be many millions of lines of lines that are
verified by this tiny kernel.  The same small kernel verifies everything from
the prime number theorem to the correctness of hardware designs.

Since the kernel is so small, it can be checked on many different levels.  The
code has been written in a literate programming style for the benefit
of a human audience.  The source code is available for the public to study.   Indeed, the code
has been studied by emminent logicians.  The kernel implements types, terms,  ten rules
of inference,  and three mathematical axioms.  By design, the system is spartan
and clean.  The computer code has these same attributes.  I wish that somebody would
make a poster of these lines of code to be published throughout the world as
the bedrock of mathematics.

Experience from other theorem-proving systems has been that about 3 to 5 bugs have been found
in these systems over a period of 15-20 years of use.  After decades of use on many different
systems, to my knowledge, only one proof
has ever had to be retracted as a result of bug in a theorem-proving system:  in 1995
a heap overflow error led to false claim that the theorem-prover REVEAL had solved the Robbins
conjecture. %% page 289, MacKenzie.
To date, no bug has ever compromised a
proof in the HOL Light system.

As an extra check, J. Harrison gave what can almost be described as 
``a formal proof in HOL Light of its own soundness.''   To get around the self-referential limitations
imposed by G\"odel, he gave two separate proofs.  In the first proof, a weakened
version of HOL Light is created, without the axiom of infinity.  The standard version is
used to give a formal proof of the soundness of the weakened version.  In the second proof, a strengthened
version of HOL Light is created, with an additional axiom giving a large cardinal.  The strengthened
version then proves the standard version sound.  These proofs go beyond traditional relative consistency
proofs in logic in two respects.  First of all, they are formal proofs, rather than conventional proofs.
Second, the proofs establish not only the soundness of the logic, but also the underlying soundness
of the computer code implementing the logic.\footnote{The soundness of the computer code is considered
relative to a semantic model of the underlying programming language.  This model may differ from
the `real-world' behavior of the programming language, a reminder that the task of verification
is never complete.}

\subsection{Export}

In the past few years, a number of
programs have been written to automatically translate a proof written in one system into
a proof in another system.  If a proof in one system is incorrect because of an underlying
flaw in the theorem-proving program itself, then the export to
a different system fails, and the underlying flaw is exposed.  (Except of course,
unless the second theorem-proving program also has a bug that is perfectly aligned with the
bug in the first system.  Since these systems are independently designed and implemented, 
most researchers feel that the probability of such a perfect storm of events is negligeable.)

Consider what happens when J. Harrison's proof of the soundness of HOL Light is exported.
(This has not happened yet, but should happen soon.)
The exported proof is a proof within a second theorem-prover that HOL Light is sound.  
It will soon be within the reach of current technology for many systems to give proofs
of the others' soundness.  When this is achieved, the probability of a false certification
of a proof is an order of magnitude closer to zero.  With a computer, it 
is never a matter of achieving philosophical certainty.
Rather, the hope is that the probability of a false certification
can drop ever closer to zero: $10^{-6}$, $10^{-9}$, $10^{-12},\ldots$

To make a realistic assessment of probabilities of failure, it is necessary to consider the
larger environment in which the program executes.  A bug in the compiler, operating sytem,
or underlying hardware has the potential to compromise a formal proof.  The current solution
is to run a proof in different operating systems and on different hardware, to lessen
the likelihood of faults caused by the external environment.  The dream, however, is to have
a formal proof of the correctness of the computer environment, relative to models of how
they should behave, from top to bottom.  (The
bottom means all the way down to the gate level -- at the lowest level, 
the physical world and engineering trump logical reasoning.)  This dream might not be as far off
as it sounds.  An early example of top-to-bottom verification was achieved for simple
systems in XXXX.   More recently, X. Leroy has proved the correctness of a C compiler for the
XXX computer chip.  Recently, researchers have succeeded in given precise definitions of the
semantics of various computer languages (such as ML, C++, etc.).  This makes it possible (for the
first time ever) to reason with mathematical precision about the expected behavior of computer
programs written in these languages.

%Overall,  far more effort has gone in recent years into making computers
%reliable than has gone into making published journal articles in mathematics reliable.  
Nobody has good estimates of  the rate of false certification
of proofs  under the current system of peer review in mathematics.  I have heard mathematicians estimate that
5 to 10 percent of all mathematical publications contain serious errors.  Some say the percentage
is even higher.  I know of no empirical studies.
Medical research journals have made empirical studies, where
articles were intentionally peppered with mistakes and sent out for peer review.  These studies
show that most mistakes were
not detected. % http://www.gsr.gov.uk/new_research/archive/2006/articles/060201.asp
Are mathematicians more conscientious or are better able
to spot errors in mathematics than medical researchers are in medicine?




\section{Full Automation}

Many mathematicians are curious about what efforts have been made to
produce a fully automated theorem prover with artificial intelligence.
With a few early successes in automated theorem proving, 
there were predictions in the 1950s, that the computer would rapidly
displace the mathematician.  These predictions were far off the mark.
Some fifty years later, there are relatively few significant examples
of general success in this area.  

Most success has been with
the development of specialized algorithms to solve special classes
of problems.  The WZ algorithm  gives  automated proofs of identities
hypergeometric sums.   Gr\"obner basis
methods can be used to solve a large variety of polynomial problems.  
Wu's method 
can prove many elementary theorems in geometry such as Pappus' theorem
and Pascal's theorem on ellipses XX (ref).  Tarski's algorithm can solve
problems that can be formulated in the elementary theory of the reals.
The list of specialized algorithms is in fact enormous.

The most widely acclaimed example of a fully automated computer proof
was the solution of the Robbins conjecture in 1996 \cite{Mc1},\cite{Mc2},\cite{Kol}.  
The conjecture asserts that alternative definition is equivalent
to the usual definition of a Boolean algebra.
It is remarkable
because the solution did not involve any human assistance,
specialized algorithms, or software
designed with this particular problem in mind.
Just type the problem into a general purpose
theorem prover {\it EQN}, hit return, and wait
eight days for the solution to appear.

Yet the story is only a qualified success.  It has
remained almost an isolated example, rather than the first in a stream
of results.  The conjecture itself has the rather special form of a
word problem in an
abstractly defined algebraic system; a type of
problem that only rarely involves deep mathematical insight. The proof that was found by
computer can be expressed as a short yet non-obvious sequence
of substitutions. (See box.)


\bigskip
\noindent
\framebox{\parbox{4.8in}{
\centerline{\it Full Automation of the Robbins Conjecture}
\smallskip
Let $S$ be a nonempty set with an associative commutative binary operation $(x,y)\mapsto xy$ and a unary operation $x\mapsto[x]$ (which we write synonymously as $x\mapsto \bar x$).
The Robbins conjecture (in Winker form) asserts that the general Robbins identity
   %\begin{eqn}\label{eqn:winker}
   $$
   [[ab][a\bar b]] = a
   $$
   %\end{eqn}
implies the existence of $c,d\in S$ such that $[cd]=\bar c$.  Here is the original proof that
W. McCune discovered by computer, as reconstructed in \cite{fit}.
\begin{proof}  A solution is $c=x^3u$, $d=x u$, where $u=[x\bar x]$ and $x$ is arbitrary.
Abbreviate $j=[cd]$,  $e=u[x^2]\bar c$.  Over the equality sign, 
a prime indicates a direct application of the Robbins identity; a superscript
indicates a substitution of the numbered line; no superscript indicates a rewriting of abbreviations $c,d,e,j,u$.
$$
\begin{array}{lll}
%\hbox to 8in{\hss}\\
[cd]&= j =' [[j[x\bar c]][jx\bar c]] =^{10} [[ju][jx\bar c]] =^7 [x[j x \bar c]] 
  =^4[[\bar c u][\bar c j x]]
  \\& =^{13} [ [\bar c [jx]][\bar c j x]] =' \bar c.\\
13: [j x]&=' [j[[xc][x\bar c]]] =^{10} [j[[xc]u]] =[[uxc][u[xc]]]=' u\\
10: [x\bar c] &=^2 [x [x u [x u [x^2]\bar c]]] =^0  [[u [x^2] ] [x u [x u [x^2]\bar c]]]  
  \\&= [[u [x^2]] [u x[ x e]]] =^8 [  [u[x[xe]]][u x[xe]] ] ='     u\\
 8: [x[x[x^2]u\bar c]] &=' [  [[x[u\bar c]][x u \bar c]] [x [x^2]u \bar c]] =^4 [ [[x^2][x u \bar c]] [[x^2] x u\bar c]] =' [x^2]\\
 7: [j u]&= [[xcu]u]=' [[xcu][[uc][u\bar c]]] =^4 [[xcu][x[cu]]] =' x\\
 4: [u\bar c]&= [u[x^2 u x]]=^0 [u[x^2 u[u[x^2]]]] =' [ [[u x^2][u [x^2]]] [x^2 u [u [x^2]]] ] 
  \\&='
   [u [x^2]] =^0 x.\\
 2: [x u [x u [x^2]\bar c]] &=' [   [[x u x^2] [x u [x^2]]]  [x u [x^2]\bar c]] = 
       [  [\bar c [x u [x^2]]]   [\bar c x u [x^2]] ] =' \bar c.\\
 0: [u [x^2]] &= [[x\bar x][xx]] =' x.
 \end{array}
$$
\end{proof}
}} % parbox

\bigskip

A similar example is S. Wolfram's fully automated proof that XX.
Overall, the level today of fully automated computer proof (lying outside
special purpose algorithms) remains that of undergraduate
homework exercises: a group in which every element has order two is necessarily abelian; Cantor's theorem asserting that set is not in bijection with its powerset; 
%
%JSTOR: 2004 Annual Meeting of the Association for Symbolic Logic
%E-mail: cebrown(andrew. cmu. edu. The Theorem Proving System TPS can be ... TPS can prove automatically are: THM 1 5B: If some iterate of function f has a ...
%links.jstor.org/ sici?sici=1079-8986(200503)11%3A1%3C92%3A2AMOTA%3E2.0.CO%3B2-V - Similar pages
if some iterate of a function has a unique fixed point, then
the function has a fixed point.\cite{TPS}


\section{Automated Discovery}

What happens if one sets aside rigour, and lets a computer explore?
A groundbreaking project was D. Lenat's 1976 Stanford thesis.
His computer program AM (for Automated Mathematician) was
designed to discover new mathematical concepts.  When AM was set loose to explore in the wild, it discovered the concepts of natural number, addition, multiplication, prime numbers, Pythagorean triples, and even the fundamental theorem of arithmetic.
% and even rediscovered the Goldbach conjecture.  (% Bundy disputes Goldbach)
The thesis touched off a firestorm of criticism and praise.

To put AM in context, consider a hypothetical program that is instructed to discover new concepts by deleting conditions
from the list of axioms defining a finite abelian group.  The computer will then immediately discover the concepts
of infinite  group,  nonabelian group,  monoid, and so forth because these concepts all
arise as subsets of the axioms.  These discoveries could be sensationalized:
{\it ``A program in Artificial Intelligence 
has made the ultimate leap from the finite to infinite, and from the abelian to the nonabelian,
rediscovering fundamental concepts in seconds that mathematicians have grappled with for centuries.''}
There are serious questions about the shallowness of AM's discoveries; a
suggestive representation of the problem gives the answer away.

%The response in the Artificial Intelligence community to this program was a flurry of praise and criticism.
%At first pass, these
%discoveries
%sound extraordinary. On closer inspection, AM receives
%considerable coaxing
%% However, as it turns out, the discoveries of
%% AM (Automated Marionnette) were guided by a human puppeteer's strings.
%through a combination of explicitly programmed heuristics, mathematical
%structures embedded in the programming language, and direct human
%intervention. (An exegesis of AM appears in \cite{Haase}.)  For example,
%the concept of addition is already implicit in the concatenation of lists, which
%is built into the system; multiplication is a special case of the concept of operator iteration (built in); and then
%factorization follows from multiplication by operator inversion (also built in).
%The restriction of addition $X+Y=Z$ to squares gives Pythagorean triples, and
%so on.
%Although it explores a large search space,
%AM never breaks free of its ingrained structures.
%%For now, mathematicians are secure in their posts -- the tenured ones at least.
%%XX Clever Hans.
%% The horse Clever Hans 
%

More recent projects stir the imagination, even if the field
is still young.
The computer program  Graffiti.pc has made over one thousand
 conjectures in graph theory, expressing numerical relationships
between different graph invariants.
One open conjecture asserts that the total domination number is at least twice the
path covering number, for every finite connected simple regular graph $G$ \cite{DLPWW}.  The box gives details.
% Written on the Wall II.
%% http://cms.dt.uh.edu/faculty/delavinae/research/wowII/  
%For example, Conjecture 2 from 1996 states that if $G$ is
%a simple connected finite graph, then $L_s(G) \ge 2(\bar\ell-1)$.\footnote{Here
%$G$ is simple if XX;
%$L_s(G)$ is the maximum number of leaves of a spanning tree;
%for any vertex $v$, $\ell(v)$ is the independence number of the
%induced subgraph
%and $\bar\ell$ is the average of $\ell(v)$ over all vertices,
%}
%Another conjecture generating program is HR (named in honor
%of Hardy and Ramanujan). \cite{XX}. 
% Conjecture 4 
% http://www.lacim.uqam.ca/~plouffe/OEIS/citations/G-2002-46.pdf
% XX need permission to quote.
%Another program HR (named in honor of Hardy and Ramanujan) 
%It conjectures for example (the elementary fact)
%that every refactorable number is congruent to $0,1,2$ or $4$ modulo $8$.
%(A number is refactorable if the number of divisors of $n$ divides
%$n$.).
% HR example mentioned on page 12 of C. Larson's Discovery.
One can imagine the day that conjecturing in Artificial Intelligence farms more fertile soil.  Unleash an AI program on
 regulators, conductors, ranks, height pairings, periods, and special values of $L$-functions,
to see what  moonshine it reveals.


\bigskip
\noindent
\framebox{\parbox{4.8in}{
\centerline{\it An example of an open computer-generated conjecture}
\smallskip
Let $G$ be a finite graph with the following properties:
  \begin{enumerate}
  \item It has at least two vertices.
  \item The graph is simple, that is, there are no loops or multiple joins.
  \item It is regular; that is, every vertex has the same degree.
  \item The graph is connected.
  \end{enumerate}
For example, the Peterson graph (Figure~\ref{XX}) has these properties.
Define the {\it total domination number} of $G$ to be the size of the smallest
subset of vertices such that every vertex of $G$ is adjacent to some vertex in the
subset.    The {\it path covering number} is the size of the smallest partition of the vertices
into subsets, such that there exists a path confined to each subset $S$ that steps through
each vertex of S exactly once.  
\smallskip

The computer program Graffiti.pc conjectures that {\it the total domination number of $G$ is at least
twice the path covering number of $G$}.  
For example, the Peterson graph has path covering number one,
because it has a Hamiltonian path.  The total domination number of the Peterson graph is four (take any vertex and the three adjacent to it).
The conjecture holds in this case by these direct calculations.
The conjecture is sharp for the graph with two vertices connected by
one edge.
}} % parbox

%% XX Peterson graph.
% http://en.wikipedia.org/wiki/Petersen_graph graphic source, creative commons license.
% Peterson_graph_blue.



\section{QED}

{

\narrower\it  As a kind of dream I played (in 1968)
with the idea of a future where every
mathematician would have a machine on his desk,
all for himself, on which he would write
mathematics and which would verify his work.
But, by lack of experience in such matters, I 
expected that such machines would be available
within 5 years from then.  But now, 23 years
later, we are not that far yet.  - N.G. de Bruijn
% Reflections on Automath

}

In the Notices in 1991, de Bruijn proposed an assembly line
to turn mathematical ideas into formally verified proofs~\cite{dB91}. [XX I say we outsource geometry.]
Today, we would rather outsource his business plan. % with outsourcing to do
%the same.  
The standard benchmark for the
human labor to transcribe one printed page
of textbook mathematics into machine verified formal text is one week, or \$100 per page at an outsourced wage. To undertake the formalization of just $100,000$ pages
of core mathematics might take %$10$ years and
a budget of well over \$10 million -- in the most wildly optimistic of scenarios.  It would represent one of the most ambitious collaborative projects ever undertaken in pure mathematics, a mathematical counterpart of the human genome project.  Before it gets off the ground, a massive wiki collaboration is needed to
 settle on the $100,000$ pages
of text covering the most significant theorems in contemporary mathematics, from 
Poincar\'e to Sato-Tate.  Others would train a corps of $400$ technicians
in developing countries in the art of formalization.
%During the final phase, lasting $250$ weeks, the technicians would perform the formalization.

Outsourcing is the brute force solution to the Q.E.D. manifesto
(an anonymous document declaring that all significant mathematical results should be preserved in a vast library of formal proofs).  Most researchers, however, prefer beauty over brute force;
we may hope for advances in our 
understanding that will permit us someday to convert a printed page of textbook mathematics into machine verified formal text in a matter of hours, rather than after a full week's labor.  This I see as the primary technological challenge in the field of formal
proof.  There has been steady progress on this technological problem over the past forty years.  When this technological problem is solved, we will at last have constructed de Bruijn's dream machine:




% old teacher Wilbur Knorr.

\section{Recommended Reading and Software}

``Mechanizing Proof''  (winner of the 2003 Merton Book Award of
the American Sociological Association).
% Review by Hayes: http://www.americanscientist.org/template/BookReviewTypeDetail/assetid/12866.


Early History ``Automated Theorem Proving after 25 Years''

Harrison's forthcoming book, ``XX''

The systems are readily available.
The easiest way to get started
is with web-browser based version of COQ, which you can use without downloading any software.  XX In the next version of Mathematica, you will be able to
try out XX.  HOL Light is available at \cite{XX} and Isabelle at \cite{XX}.




\section{Old Introduction}

And now, 40 years later, where does this dream stand?

It is sometimes a challenge to convince a mathematician that process and
reality matter. Some mathematicians have declared the independence
of their creations from
the physical world.  Hardy argues that it is nonsense to suppose
a connection between a proof and the physical world:
% writes, ``Suppose now a $\ldots$ massive
%gravitating body is introduced into the room.  Then the physicists
%tell us that the geometry of the room is changed, its whole physical
%pattern slightly but definitely distorted.  Do the theorems which
%I have proved become false?  Surely it would be nonsense to suppose
%that the proofs of them which I have given are affected in any way.
``It would be like supposing that a play of Shakespeare is changed when
a reader spills his tea over a page.  The play is independent of the pages
on which it is printed, and `pure geometries' are independent of lecture
rooms, or of any other detail of the physical world.''

History gnaws at Hardy's example.
Scholars have argued for centuries about differences between the folio
and quarto versions of Shakespeare's plays.  Who recites the line
`Break, heart, I prithee break' as King Lear dies?  Was it Kent (folio version)
or Lear (quarto version)?  This play is not truly ``independent of the pages
on which it is printed.'' Likewise, 
our understanding of  Archimedes  is
shaped by the codices, tangible artefacts in the physical world.  

In this essay,
I avoid the
philosophical question of whether or not mathematics exists in a Platonic realm.
I have no quarrel with Hardy's philosophical point.
Rather, I am making the mundane point that the preservation
of mathematics relies on human artefacts, and that the quality of those
artefacts affects the transmission of mathematics
from one generation to the next.  I also hope to make the less obvious
point that the computer has become the preferred medium for the long-term
preservation of mathematical thought.  I argue further that the demands of
mathematical consistency require us to reexamine our practice of mathematics.

Accepting that all human artefacts are subject to corruption and
decay, what tangible record should we make of a proof?
S. Mac Lane suggested a dialectic
starting with the published paper,  expanded by the
author in response to each challenge, until all objections are satisfied.
XX QUOTE MAC LANE.
Anyone who has been subjected to more than three rewrites of a paper has reason to dread this dialectic.

Conventional printed text encodes natural language.  
XML encodes superficial structures
(This is a theorem; that is a definition.)  Formal systems capture logic and
semantics.



Did Jordan have or not have a proof of the Jordan curve theorem?
Did Leech have or not have a proof of the problem of thirteen spheres?
And more to the heart of my concern, 
did Ferguson and Hales have or not have a proof of the Kepler conjecture?
Even when we examine the papers, mathematicians continue to disagree.
Is Jordan's  outdated use of infinitessimals and
missing detail of significance in his proof of the Jordan curve theorem?

\section{Trust in Computers}

One of the most common question that mathematicians have about formal proofs concerns
the infallibility of math and the fallibility of the computer.  This section will
describe why I have a high degree of confidence in formal proofs.

Often I hear arguments that pit the ethereal perfection of math against the physical
imperfections of a computing device.
This is not a valid comparison.  I accept arguments that compare the ethereal with
ethereal (that is, that compare abstract ZFC with an abstract Turing machine),
or physical artefacts with physical artefacts (that compare paper-based
systems including typos and printer errors with formal systems including compiler bugs and hardware defects).  But it is not a fair fight to mix the two.  To those that argue the perfection of math, I say show me your artefacts!  Let's examine the evidence.

I am not interested in abstract arguments of other-worldly consistency.  I am a
scientist, not a philosopher.  Show me your artefacts!  Let's examine the evidence.



When I was a Masters student in a course on Bayesian statistics, I ran an experiment on myself.   I took a series of practice multiple choice exams and made a record of which ones I was sure I had marked correctly.  When I corrected the exams, I found that I had answered correctly only 95\% of those I was sure I had answered correctly.  Psychological certainty is not a proof of correctness.\footnote{``When someone is honestly 55\% right, that's very good and there's no use wrangling.  And if someone is 60\% right, it's wonderful, it's great luck, and let him thank God.  But what's to be said about 75\% right?  Wise people say this is suspicious.  Well, and what about 100\% right?  Whoever says he's 100\% right is a fanatic, a thug, and the worst kind of rascal.'' -- An Old Jew of Galicia, epigraph to ``The Captive Mind'' by Czeslaw Milosz.}  In my published papers, I am relieved to have so far been able to avoid the embarrassment of a major error.  It has required an elaborate and time-consuming system of self-checks to be as consistent as I have been.  I find errors in other mathematical publications all the time.  They are generally fixable, but occasionally fatal.  Even Euclid has its errors. 

We can compare two processes.  The first is the traditional system of peer review.  
I largely distrust the peer review system.  I have had too many incomplete and even incompetent 
reviews, I find too many errors in print, I know of too many papers that mathematicians should have retracted but have not, and I have had too many of my reports ignored
to have much confidence in the system.  The second process is a machine verification in a 
formal system.  The question for
each process is what is the  false positive rate:  statements accepted as correct theorems by the review process yet that are later discovered to be false?

There is almost universal agreement, among those that use formal theorem proving systems
regularly, that the false positive rate is several orders of magnitude larger for
the traditional peer review system than for formal proof systems.  Each formal proof
starts with a traditionally published proof and then goes much farther.  Thus, we can be
nearly certain that a formal proof has a strictly smaller false positive rate than the original publication.



\section{Misc Notes}

The foundational system is designed for practical use, and not just for theoretical purposes.  It is for those who wish to prove theorems to the core, and not just reason abstractly about theorem proving.  Contrast this with the Bourbaki system, which was never intended for practical use.  According to the authors of that system, implementing an actual theorem in Bourbaki's set theory is `absolutely unrealizable.' XX.
Contrast this with the way most mathematicians invoke ZFC to justify how something may be expressed  foundationally in principle but never in practice.

Theorem proving software is some of the most reliable software ever
produced.  The normal standard for software is rather low.  Give details...
This section explains some of the precautions that have been used
to produce extremely reliable packages.  Typically, these systems
have had 3-5 noteworthy bugs over a lifespan of 10-20 years. 
These bugs have been sufficiently obscure that they have not affected
the correctness of any of the formal proofs in their systems.

Various ongoing research projects are working to automatic a broader range
of mathematical activities, such as formulating conjectures, judging the significance of mathematical statements, theory building, and finding counterexamples [XX Simon Colton and guy I wrote the letter for.]

---

Where Trust is important, aircraft control, 
security protocals for the internet.  Difficult time-consuming
to do this kind of analysis of software.
There is no higher standard of software reliability in existence.

---

Huge communication problem in CS and math.
CS wonder why mathematicians don't care.
Math wonder why CS doesn't tell them about these wonderful tools.

---

Code: esthethique works here too.  We don't give up beauty.

\begin{thebibliography}{ABCDE}

\bibitem{Bled} W. W. Bledsoe and D. W. Loveland (eds.), Automated
Theorem Proving: After 25 Years, Contemporary Mathematics, Vol. 29,
AMS, Providence, RI, 1984.

\bibitem{dBXX} N. G. de Bruijn, Reflections on Automath.

\bibitem{dbXY} N. G. de Bruijn, Types in Mathematics.

\bibitem{dB91}  N.G. de Bruijn, Checking Mathematics with Computer Assistance, Notices of the AMS,
Vol 38, (1), Jan. 1991.

\bibitem{Col} S. Colton, Automated Theory Formation in Pure
Mathemaics,  Springer, London, 2002.

%\bibitem{ED} E. Devalina, Some History of the Development of Graffiti,

\bibitem{DLPWW} E. DeLaVina, Q. Liu, R. Pepper, B. Waller and D. B. West, On some conjectures of
Graffiti.pc on total domination, Congressus Numerantium, (2007), Vol. 185, 81-95.

\bibitem{fit}  B. Fitelson, ``Using Mathematica to Understand the Computer Proof of the Robbins Conjecture''
Mathematica in Education and Research (Winter 1998, Volume 7, No. 1).

\bibitem{Haase} K.W. Haase, Jr., Invention and Exploration in Discovery,
Ph.D. thesis, MIT, 1990. 

\bibitem{Ha07} J. Harrison, A short survey of automated reasoning,
Proceedings of AB 2007, the second international conference on Algebraic Biology, Springer LNCS vol. 4545, pp. x--x, 2007.

\bibitem{Ha08} J. Harrison, Introduction to Logic and Automated
Theorem Proving, 637pp. to appear.

\bibitem{Kol} G. Kolata, Computer Math Proof
Shows Reasoning Power, New York Times, Dec 10, 1996.
http://www.nytimes.com/library/cyber/week/1210math.html


\bibitem{Mac} D. MacKenzie, Mechanizing Proof, MIT Press, Cambridge, MA,
2001.

\bibitem{Mc1} W. McCune, Robbins Algebras are Boolean, http://www.cs.unm.edu/~mccune/papers/robbins/

\bibitem{Mc2} W. McCune, Solution of the Robbins Problem, JAR 19(3), 263--276 (1997)

\bibitem{Mu} R. Murawski, The Present State of Mechanized
Deduction, and the Present Knowledge of Limitations,
Studies in Logic, Grammar and Rhetoric 
year: 2006, vol: 9, number: 22, pages: 31-60.


\end{thebibliography}


\end{document}

