\chapter{Appendix}

\begin{note}%XX
The appendix will be rewritten.  What follows is only a rough draft.
\end{note}

Beyond the text, the proof relies on three separate external bodies of
computer code.  These are described in much greater detail at various
places in the book.  These external bodies of code are called
\begin{enumerate}
\item Tame Hypermap Generation,
\item Interval Arithmetic,
\item Linear Programming.
\end{enumerate}
The tame hypermap generation is an stand-alone program that is run
once at a specific point of the proof.  It carries out a combinatorial
classification of planar graphs satisfying a certain restrictive list
of properties.  The reader can safely ignore this computer program
until reaching the relevant point in the proof.  By contrast, the
interval arithmetic is a collection of nearly one thousand
inequalities that have been proved by computer.  These inequalities
are spread throughout the proof and appear throughout this book.  On
first reading, the reader is encouraged to accept these inequalities
as axiomatically given facts.  Detailed documentation about these
inequalities is available for those who wish to follow up later on the
computer-generated proofs of these inequalities.  The final stage of
the proof consists entirely of linear programming.  There are also
several small linear programs that appear in scattered places in the
book.

\section{Computation}

As this project  progressed, the computer  replaced conventional
mathematical arguments more and more, until now
nearly every aspect of the proof relies on
computer verifications.  Many assertions in this book
are results of computer calculations.
To make the solution more accessible, I have
posted extensive resources \cite{web}.

There are three major pieces of computer code that enter into the proof.

\begin{itemize}
\item {\it Combinatorics}.  A computer program classifies all of the
  planar hypermaps that are relevant to the packing problem.
\item {\it  Interval analysis}.  ``Sphere Packings
I'' describes a method of proving various inequalities in a small number
of variables by computer by interval arithmetic.
\item {\it  Linear programming}.  Many of the nonlinear optimization
problems for the scores of centered packings are replaced by linear
problems that dominate the original score.  They are solved
by linear programming methods by computer.  A typical problem has
between 100 and 200 variables and 1000 and 2000 constraints.  Nearly
100000
such problems enter into the proof.
\end{itemize}

Computers are used in various other ways.  


\begin{itemize}
\item {\it Formal proof}.
Various parts of the solution of the packing problem have now been
formally verified.
\item  {\it Numerical optimization}.  The exploration of the problem
has been substantially
aided by nonlinear optimization and symbolic math packages.
\item {\it Branch and bound methods}.  When linear programming methods
  do not give sufficiently good bounds, they have been combined with
  branch and bound methods from global optimization.
\item {\it Computer Algebra Systems} Mathematica was used for many
  minor calculations, such as the calculation of exact explicit
  formulas for derivatives.
\item {\it Organization of output}.
The organization of the few gigabytes of code and data that
enter into the proof is in itself a nontrivial undertaking.
\end{itemize}



\section{Formal Proof}


\section{Tame Hypermaps}

\clearpage

\section{Interval Analysis}%DCG 8.3, p75
\label{sec:bounds-simplex}

Interval analysis is a method to obtain trustworthy results from a
computer.  This essay gives a basic introduction to this method.

\subsection{deliberate error}

Interval arithmetic can trace its origins to the method of deliberate
error.  This is an ancient method of navigation with imperfect
instruments.  When William the Conqueror crossed the English Channel
in 1066, he deliberately steered to the north of Hastings.

\begin{quote}
  % ``There is no direct evidence of how a twelfth-century pilot found
  % his way across the English Channel $\ldots$
  ``Any pilot even now who had to make that crossing without a chart
  or compass, as William's pilots did, would use the ancient method of
  Deliberate Error: he would not steer directly towards his objective
  but to one side of it, so that when he saw the coast he would know
  which way to turn.'' \cite[p81]{How81}
  % 1066: The Year of the Conquest, Penguin paperback edition
  % 1981. page 148.
\end{quote}

Deliberate error implements ``better safe than sorry.''
To find an address on a one-way street,  a  driver enters the
street too early, to point in the right direction.  
When searching for a familiar landmark
on a two-way street, it is more efficient  
to start the search safely to one side of the landmark, 
rather than search in ever
expanding zigzags in both directions.  
Deliberate error 
is the idea of trapping a target inside a large enough net, in the
way that
Khachiyan traps the optimal solutions of a linear program inside
an ellipsoid.


The method of deliberate error cushions the adverse effects of
imperfect technology.  The method does not aim to minimize the
imperfections.  It works with a faulty chart and compass.
% The method of deliberate error seeks not to minimize the error of
% imperfect technolo

\subsection{arithmetic}

The method of deliberate error, implemented to control for round-off
errors on a computer, leads to interval arithmetic.  To approximate
the circumference of a circle, Archimedes inscribes a polygon and
circumscribes a second polygon, trapping the unknown within a known
interval.  This is interval arithmetic.


Fixed precision floating point numbers on a computer exactly represent
a finite number of rational numbers.  The remaining uncountable set of
real numbers cannot be precisely represented.  For example, 64-bit
floating point numbers can encode at most $2^{64}$ different real
numbers.  This finite set of representable numbers is not closed under
addition, multiplication, subtraction, or division.  For example, the
input $2.0 + 2.0$ returns $4.0$, because all the numbers involved are
precisely representable.  However, through round-off error, my
computer returns
\begin{verbatim}0.0
\end{verbatim} 
in response to
\begin{verbatim}
-1.0 + (1.0 + 1.0e-16)
\end{verbatim}
and 
\begin{verbatim}1.0e-16
\end{verbatim} 
in response to 
\begin{verbatim}
(-1.0 + 1.0) + 1.0e-16.
\end{verbatim}
As this example shows, machine addition is not even associative.  One
is quickly led to absurd conclusions, if one tries to reason about
floating point operations as if they formed a group, a ring, or a
field.  To reason correctly about floating point, one must accept
these operations on their own terms.  To distinguish the imprecise
machine (floating point) operations from standard operations on the
field of real numbers, in the rest of this section (except in computer
code listings), place a dot over the floating point operations $(\dot
+)$, $(\dot -)$, and so forth.

The precise behavior of floating point operations on a computer is
governed by the IEEE-754 floating point standard \cite{Gol}.  By
giving a precise specification of properties of floating point
arithmetic, the standard makes it possible to reason about the
behavior of floating point.  It is possible to prove theorems about
the behavior of IEEE floating point.  For example, it is possible to
prove that a computer that correctly implements the standard should
return the values in reponse to the inputs given above (in the nearest
rounding mode).

Let $F$ be the set of machine-representable floating point numbers.
Assume that $F$ contains two special symbols $\pm\infty$.  The total
order on the real numbers is extended to $\ring{R}\cup\{\pm\infty\}$,
with $-\infty < x < \infty$ for all $x\in\ring{R}$.  Because of these
two special symbols, the floating point floor $x\to\floor{x}_F\in F$
and ceiling $x\mapsto\ceil{x}_F\in F$ functions, with domain
$\ring{R}$ can be defined.  In this section, we drop the subscript $F$
on the floor and ceiling functions.  We map the set of real numbers to
$F^2$, by sending $x$ to $[a,b]$, where $a = \floor{x}$ and
$b=\ceil{x}$.  If $x$ is Hastings, then $b$ is a point on the shore
north of Hastings.  It is the deliberate error to one side of the
target.





On most modern processors, the rounding mode can be set to directed
rounding, as described in the standard.  When the rounding mode is set
upward, the result of any basic arithmetic floating-point operation
$(\dot +)$, $(\dot -)$, $(\dot *)$, $(\dot /)$ applied to two floating
point numbers $(x,y)\mapsto x\dot\diamond y$ is defined by standard to
be the $\ceil{x\diamond y}$.  That is, make the calculation in the
field of real numbers and round up to the next floating point.  When
the rounding mode is set downward, set $x\dot\diamond y =
\floor{x\diamond y}$.  The notation $\dot\diamond$ does not show
rounding mode, but it should, because it is mode dependent.


Interval arithmetic, like the method of deliberate error, does not
seek to eliminate the sources of floating point round off error.
Rather it brings it under scientific control.


Let $I_F = \{[a,b] \in F^2 \mid a \le b\}$ be the set of floating
point intervals.  Basic machine operations extend to intervals.  The
sum $[a_3,b_3]$ of $[a_1,b_1]$ and $[a_2,b_2]$, is defined as
$a_3=\floor{a_1+a_2}$ and $b_3 = \ceil{b_1+b_2}$.  Write $[a_3,b_3] =
[a_1,b_1] \dot+ [a_2,b_2]$.  This addition of intervals is not
associative.  However, addition of intervals satisfy a crucial
inclusion property.  If $x\in[a_1,b_1]$ and $y\in [a_2,b_2]$, and $z =
x+y$, then $z\in [a_3,b_3]$.  The other arithmetic operations can be
extended to machine interval operations in a similar way, so as to
satisfy the corresponding inclusion properties.  Division requires
special treatment when $0$ belongs to an interval denominator.  We
will not go into those details here.

These operations are easily implemented in code.  For example, here is
the actual snippet of {\tt C++} code that implements the addition of
intervals.  The functions {\tt interMath::up()} and {\tt
  interMath::down()} set the rounding modes on the computer.
\begin{verbatim}
inline interval interval::operator+(interval t2) const
{
interval t;
interMath::up(); t.hi = hi+t2.hi;
interMath::down(); t.lo = lo+ t2.lo;
return t;
}
\end{verbatim}

By implementing interval operations on a computer, we can develop a
procedure that takes as input an arbitrary arithmetic expression over
the rational numbers and returns an interval $[a,b]$ with floating
point endpoints (augmented by $\pm\infty$ as usual) that contains the
rational value of that expression.  The true answer, still unknown,
lies trapped between the interval endpoints.  Since the associative
and distributive laws fail, the returned interval $[a,b]$ depends on
the actual syntax of the expresssion, on the placement of parentheses,
and so forth.



\subsection{analysis}

This essay is not a treatise on interval arithmetic.  Its purpose is
to give an introduction, to demonstrate the possibility of rigorously
bounding the error of floating point performed according to IEEE
standards.  We become increasingly sketchy.

The theory of interval analysis progresses step by step, starting with
basic arithmetic and developing through higher levels of analysis.  If
$p$ is a polynomial with rational coefficents
$p\in\ring{Q}[x_1,\ldots,x_n]$, then it has an interval extension
$\bar p : I_F^n \to I_F$.  Just as in the case of rational numbers,
the interval extension $\bar p$ depends on the syntax used to
expressed to polynomial $p$.  The polynomial extension is {\it
  monotonic}: if $z_i\in [a_i,b_i]$ for all $i$, then
$p(z_1,\ldots,z_n) \in \bar p([a_1,b_1],\ldots,[a_n,b_n])$.  Interval
extensions of rational functions is similar.

Consider a function $f:[a,b]\to\ring{R}$ that
has a rational function approximation $r$ with known
error bound:  $|f(x) - r(x)|<\epsilon$ for $x\in[a_1,b_1]$,
with $\pm\epsilon\in F$.
Define a monotonic interval extension $\bar f$ of $f$ by
$\bar f([c,d]) = \bar r([c,d]) \dot + [-\epsilon,\epsilon]$, for
$[c,d]\subset [a,b]$.  
Multivariate functions are similar.   
%
A large class of analytic functions fall within this framework.
We have interval extensions of trigonometric functions, logs, and
exponentials.  Interval extensions of real-valued functions can be
added, multiplied and composed.  

To prove that a function $f$ is positive on a rectangular domain
$[a_1,b_1]\cdots[a_n,b_n]$, compute the
value of an interval extension $[c,d]=\bar f([a_1,b_1]\cdots[a_n,b_n])$.
By the monotonic property of interval extensions, if $c>0$, then $f$
is positive on the domain.  If this is done naively, with the
first interval extension $\bar f$ that comes to mind, the image interval
$[c,d]$ may be so large that no worthwhile information results.
If we naively 
compute $x+(-x)$ by interval arithmetic for $x$ in the interval $[-1,1]$,
we find that $x+(-x)$ lies in the interval sum
$[-1,1]\dot + [-1,1] = [-2,2]$.  Useless!
There is a large mathematical literature describing various 
efficient algorithms
to compute image intervals $[c,d]$ with accuracy.
Kearfott's book is a recommended starting point, because it comes
close to describing the types of algorithms implemented for the
solution to the packing problem \cite{Kea96}. 



\subsection{archive}

Although this book is long, it represents only a fraction of the
solution of the packing problem.  The other resources that are needed
to understand the full solution are available on the internet.

There is an archive of several hundred inequalities that have been
proved by computer.  The list of inequalities can be found at
\cite{web}.\footnote{The archive of interval arithmetic inequalities
  appears at \url{http://flyspeck.googlecode.com/svn/trunk/}} The list
of inequalities are in computer readable form in the rigorous
mathematical syntax of HOL-Light (for Higher Order Logic).

For example, one of the inequalities from that file reads as follows.
\begin{verbatim}
let I_572068135=
all_forall `ineq 
[((square (#2.3)), x1, (#6.3001));
((#4.0), x2, (#6.3001));
((#4.0), x3, (#6.3001));
((#4.0), x4, (#6.3001));
((#4.0), x5, (#6.3001));
((#4.0), x6, (#6.3001))
]
((((tau_sigma_x x1 x2 x3 x4 x5 x6) -.  ((#0.2529) *.  
(dih_x x1 x2 x3 x4 x5 x6))) >. (--. (#0.3442))) \/ 
((dih_x x1 x2 x3 x4 x5 x6) <.  (#1.51)))`;;
\end{verbatim}
The first part of this snippet of code gives lower and upper bounds on
each of the six variables $x_1,\ldots,x_6$.  The final part of this
code gives an inequality (or rather a disjunction of two inequalities)
of nonlinear real-valued functions that holds on the given domain.
The $\#$ symbol marks exact decimal constants.  The functions {\tt
  dih\_x}, {\tt tau\_sigma\_x} are defined rigorously in a separate
file.  They correspond to the functions $\dih$ and $\tau$ in this
book.  The symbols for arithmetic operations are followed by periods
($*.$ and so forth) to distinguish them from the corresponding
operations on natural numbers.

The inequality carries a nine-digit identifier {\tt 572068135}.  This
number is a tracking number that can be used in a search engine to
locate everything known about a given inequality.  For example, if one
googles this number, the search engine returns nine matches related to
this inequality, including a preprint on the arXiv, the website of the
Annals of Mathematics, a Springer Link to the relevant issue of the
journal Discrete and Computational Geometry, a flyspeck discussion
group, the {\it C++} computer code proving the inequality, and the
output file from running that code.  A search on the pdf file of this
book links this inequality to Lemma~\ref{lemma:11.16}.  Every interval
arithmetic calculation in this book carries a nine-digit identifier,
for easy tracking.

Currently, the most convenient way to access the code that proves the
inequality is through the Google's Code Search, a custom search engine
for computer code.\footnote{The interval arithmetic code that proves
  the inequalities is available at
  \url{http://code.google.com/p/flyspeck/}.}


Further information about proving these inequalities by interval
arithmetic can be found in \cite{algorithm} and \cite{part1}.
%\indy{Index}{calc@\calc{123456789}}%

\subsection{code verification}

S. Ferguson and I put considerable effort into developing trustworthy
code.  The two of us made entirely independent implementations of
the code. %automated inequality proving by interval arithmetic. 
(We shared algorithms
but not source code.)  This allowed us to check our answers against each
other to ensure mutual consistency, 
and to eliminate certain sources of bugs.  We also made independent
implementations in Mathematica and {\tt C} of traditional floating point
versions of the functions used for the nonlinear
optimizations.  By requiring these different implementations to give
compatible answers, we eliminated further sources of bugs.

Each nonlinear inequality was also checked independently with the
nonlinear optimization package {\tt cfsqp}.  This is a collection of
{\tt C} routines that searches for the minimum of a smooth function on
a domain described by a system of constraints.  As is the case with
many nonlinear optimization packages, there is no guarantee that the
search will converge to the true global minimum of the function.  By
repeating the search with a large collection of initial values for the
search, it becomes more probable that the true global minimum will be
found.  In practice it works remarkably well.

This package was not used in any proofs.  The numerical testing with
{\tt cfsqp} was used to discover false inequalities before they were
shipped to the interval arithmetic prover for verification.  Those
that failed never shipped.  This extra level of testing adds an extra
level of robustness to this part of the proof.  Testing gives us
(nonrigorous) reasons to believe the inequalities, even if a bug
should appear in our interval arithmetic code.  This gives us some
hope that an undetected bug would be unlikely to affect the overall
design of the solution to the packing problem.


There are certain types of bugs that would be very difficult to
detect.  For example, since floating point arithmetic is not
associative, a misplaced pair of parentheses might throw a calculation
off by a machine epsilon.  This potential source of bugs is evidence
that the entire project was not sufficiently automated: the
parentheses should all have been worked out automatically.  To make
the calculations more robust, we have tried to design the collection
of inequalities so that they hold with a considerable margin of error,
rather than just squeeze by.  (There are only a few inequalities that
are sharp, and they are sharp for clear mathematical reasons related
to the theory of packings.)  In a bug-free environment, such
precautions would not be necessary.  Nonetheless, we take precautions.

As far as I know, no comprehensive efforts were made by the referees
and editors to check the correctness of the computer code before the
publication of the 1998 solution to the packing problem.  The editors'
preface to \cite{DCG} states that during the review process ``some
computer experiments were done in a detailed check.''


The flyspeck project is a long-term project intended to make the
solution to the packing problems one of the most thoroughly checked
computer proofs of all times.  Part of this project calls for a formal
proof of correctness of the computer code used in the interval
verification of inequalities.

Flyspeck is still far from completion in 2008.  Nevertheless, there
are various ongoing projects related to this second-generation
verification of the interval code.  S. McLaughlin has an independent
implementation of the interval arithmetic code used in the packing
problem~\cite{McL08}.  His work has exposed some data-entry bugs.
They are reported in the comprehensive errata to the 1998 proof, which
is maintained at \cite{errata} with additional discussion
at~\cite{flydis}.  Fortunately, no bugs have surfaced over the past
decade in the underlying interval arithmetic inequality proving
algorithms.  The reported errors have been at the data-entry level: a
mismatch between the data as typed into the preprint and data in
computer code.

One of the formal proof assistants under most active development is
the COQ system~\cite{COQ}.  R. Zumkeller has implemented automated
inequality proving with interval arithmetic inside the theorem prover
COQ, with the flyspeck project in mind; although (as of 2008) the
inequalities that are used in this book have not yet been checked in
this way \cite{Zu}.


\subsection{interval analysis and proof}


The editors of the Annals of Mathematics have posted a statement on
computer-assisted proof.  At first, the editors planned to make a
disclaimer directed at the computer solution of the packing problem.
Eventually, they formulated a general policy on computer-assisted
proofs.  The policy mentions interval arithmetic as one way to control
sources of computer error.


\begin{quote}
%Statement by the Editors on Computer-Assisted Proofs

  ``Computer-assisted proofs of exceptionally important mathematical
  theorems will be considered by the Annals.

  ``The human part of the proof, which reduces the original
  mathematical problem to one tractable by the computer, will be
  refereed for correctness in the traditional manner. The computer
  part may not be checked line-by-line, but will be examined for the
  methods by which the authors have eliminated or minimized possible
  sources of error: (e.g., round-off error eliminated by interval
  artihmetic, programming error minimized by transparent surveyable
  code and consistency checks, computer error minimized by redundant
  calculations, etc. [Surveyable means that an interested person can
  readily check that the code is essentially operating as claimed]).

  ``We will print the human part of the paper in an issue of the
  Annals. The authors will provide the computer code, documentation
  necessary to understand it, and the computer output, all of which
  will be maintained on the Annals of Mathematics website online.''
  \cite{Ann06}

%http://annals.princeton.edu/EditorsStatement.html
\end{quote}

A number of proofs in pure and applied mathematics have been based on
interval analysis.  W. Tucker implemented a rigorous ODE solver with
interval arithmetic and used it to prove that the Lorenz equations
have a strange attractor \cite{Tuc02}. The existence of strange
attractors is problem 14 on Smale's list of 18 Centennial Problems
\cite{Sma98}.  Another prominent problem solved by interval methods is
the double bubble conjecture, a generalization of the isoperimetric
problem in three dimensional Euclidean space.  A sphere gives the
solution to the classical isoperimetric problem.  The work of J. Hass,
M. Hutchings, and R. Schlafly shows that the surface area minimizing
way to enclose two regions of equal volume is the double bubble, which
consists of two partial spheres, separated by a flat circular disk
\cite{HHS95}.

Interval arithmetic has also yielded a number of new results on the
problem of packing circles in a square. M. Cs. Mark\'ot and T. Csendes
have obtained optimality proofs for packings of $28$, $29$, and $30$
circles in a square.  See Figure~\ref{fig:optimal-circles}.  This is
an area of active research. See, for example, \cite{Sza07} and
\cite{Mark07}.

%% WW not yet done.
\begin{figure}[htb]
\centering
\myincludegraphics{noimage.eps}
\caption{Optimal circle packings in a square}
\label{fig:optimal-circles}
\end{figure}



\subsection{note}

There are those who have tried to downplay the role of computers and
interval methods in the solution to the packing problem.  In fact,
they play an absolutely central role.  To segregate the computation
destroys the proof.  After writing the paper {\it The Sphere Packing
  Problem}, I had all but given up on solving the problem.  I had an
extremely difficult nonlinear optimization problem on my hands and no
rigorous mathematical method to solve it.  In the summer 1993, I
happened upon book on Pascal-XSC (a language extension of Pascal for
interval analysis) at the Seminary Coop Bookstore next to the
University of Chicago.  This book described the method I had lacked.
With fresh hope, in January 1994, I set aside all else and devoted
full effort to the packing problem.  The interval code was the most
difficult part of the computer code to implement because its speed was
crucial.  Thanks to the improvements of S. Ferguson, eventually the
code could run from beginning to end in about three months.  The
interval verifications were the last part of the proof to be completed
in August 1998.


\clearpage
\section{Linear Programming}

The solution of the packing problem is based on the {\it vanishing
box trick.}  At the outset, the set of 
counterexamples to the problem has an unknown size and structure.
These counterexamples are all placed in a large box.   
The width of the box is measured and is found to be negative.
Therefore the box is empty, it contains no counterexamples, and
the problem is solved.

It may seem that such a strategy is too hopelessly naive to work.
In fact, precisely this strategy is pursued, and it works beautifully.
The act of placing a counterexample in a box is called linear 
relaxation.  The counterexamples form an unknown nonlinear set.
The box that contains them is defined by linear constraints.  
Relaxation refers to a relaxation of constraints, so that the
box contains the counterexamples -- it is not merely a linear
approximation to the set of counterexamples.  The box needn't be
rectangular.  Any polyhedron will do.  To determine that a box
has negative width is to say that it gives an {\it infeasible} linear
program.  With the strategy, there is no need to limit ourselves
to a single box, we can use a finite collection of boxes that
contain the set of counterexamples.  This leads to branch and bound
methods.


In this appendix, we will discuss these methods in further detail,
including linear programming, linear relaxation,  infeasibility,
and branch and bound methods.

The introduction of linear programming methods to the packing
problem was gradual and tranquil.  
Many of the
inequalities in the packing problem have an obvious linear form.
As an engineering student
at Stanford, I had studied
linear and nonlinear optimization.
Given my background, 
it was natural for me to express them as a linear program.  
Once linear programming had made an appearance, the 
repeated successes of the method led me to rely on it more and more.
In the end, approximately $10^5$ linear programs appear in the solution,
each involving some $200$ variables and around $2000$ constraints.
This is a inconsequential task for modern algorithms.  

The consequential part is to organize the output of this many linear
programs into a comprehensible proof narrative.  How do I convince you
the reader that this vast collection of linear programs constitutes
the proof of a theorem?  What is the precise statement of that
theorem, expressed in a way that does not refer to $10^5$ separate
problems? There is an entire chapter of this book devoted to these
questions.  This introductory essay gives enough background to carry
us through to that chapter.


\subsection{preliminaries}

There is a vast mathematical literature describing algorithms to
solve the constrained optimization problem of finding a column vector
$x\in\ring{R}^n$ that maximizes the objective function $ x$
\begin{equation}\label{eqn:lp1}
\max_{x}  c x
\end{equation}
where $c\in\ring{R}^n$ is a fixed row vector, subject to a system of
linear inequalities,
\begin{equation}\label{eqn:lp2}
A x\le b
\end{equation} 
for some matrix $A$ and vector $b\in \ring{R}^m$.  (An inequality $A
x\le b$ of vectors means that each component of vectors satisfy the
inequality.)  Such a maximization problem is called a {\it linear
  program}, an unusual name for what it is, not a program in the sense
of computer code, rather a program in the sense of military logistics,
growing out of G. Dantzig's war experience at the Pentagon
\cite{Dan91}.  G. Dantzig proposed the simplex method to solve such
problems in 1947.  The system of constraints $A x \le b$ defines a
polyhedron, the vector $c$ gives a preferred direction in space, and
the simplex method walks along edges of the polyhedron, from vertex to
vertex, until reaching a vertex where no further progress can be made.
The simplex method has been named one of the top ten algorithms of the
twentieth century \cite{Cip00}.

Later in 1947, within minutes of first hearing about
linear programming, von Neumann introduced linear programming
duality, as an outgrowth of his theory of games.
In 1979, L. Khachiyan found a way to solve linear programs in
polynomial time \cite{Kha79}.  Still later, 
N. Karmarkar found a polynomial
time algorithm of practical value \cite{Kar84}.  
M. Wright gives a recent survey
of interior-point algorithms (algorithms that search for the 
optimal solution by moving through the interior of the polyhedron
rather than following the simplex method's strategy of searching
along the boundary of the polyhedron) in \cite{Wri05}. 
M. Todd also gives an excellent survey of algorithms \cite{Tod02}.
These algorithms have been implemented in various
software packages.
Because of this extensive and well-developed literature on the
subject of linear programming, we will take it as given that we
can solve large-scale linear programs.


The basic linear program admits many variations.  We can minimize
the objective function
$$x \mapsto c x$$ rather than maximize it.
Any linear equation
$a x = b$ can be written as a system of inequalities
$$a  x \le b \text{ and } -a x \le -b.$$
This allows us to reduce constrained linear optimization with
equality constraints to a constrained linear optimization with
inequality constraints.

\subsection{duality}

A linear program is feasible if there exists an $x$ satisfying
all the constraints $A x \le b$.  A linear program is bounded
if the set $c x$ is bounded from above for all $x$ satisfying the
constraints.  

Given the linear program of Equations~\ref{eqn:lp1},\ref{eqn:lp2}
defined from $A,b,c$, there is another linear program defined by
the same data, but in dual form
\begin{equation}
\min_y {y b}
\end{equation}
such that
\begin{equation}
y A = c \text{ and } y \ge 0,
\end{equation}
where $y\in\ring{R}^m$ is a row vector.
To distinguish the two linear programs, the one given by
Equations~\ref{eqn:lp1} and \ref{eqn:lp2} is called the primal.
If $x$ is any feasible solution to the primal and  if
$y$ is any feasible solution to the dual (meaning that they satisfies
the constraints but are not necessarily optimal), then by
the inequality constraints, we have trivially that
$$y b \ge y A x = c x.$$
That is, any feasible solution $y$ to the dual linear program gives an
upper bound to the primal.  If the left-hand side is minimized
for $y=y^*$ and the right-hand side is maximized when $x=x^*$, then
$$y^* b \ge c x^*.$$
The linear programming duality theorem asserts that if the primal
is feasible and bounded, then the dual is also feasible and bounded,
and moreover $y^* b = c x^*$.  In summary, we can bound
the primal with any feasible solution to the dual, and find
the optimal bound on the primal by minimizing the dual.

Linear programming duality produces certificates of a bound
on the primal.  Suppose we treat a linear programming package as
an untrusted 
black box without any knowledge of its internal algorithms.
To handle round-off errors, we now also assume that we have
a priori bounds $|x_i| \le m$ on the variables $x_i$.
The untrusted package produces a vector $y$, which it claims
is (approximately) a feasible solution to the dual problem.  
First, replacing
any negative coefficients in $y$ with $0$, we may assume that
it satisfies the constraint $y\ge 0$.  The quantity $\delta = c - y A$
should be exactly zero for a feasible solution to the dual program, but
because of round-off errors, it may be  off.  
Then for any feasible $x$ to the primal problem
$$
c x = (\delta + y A)  x \le \delta  x + y b.
$$
Using the a priori bounds on $x$, we get $\delta x\le \sum 
m|\delta_i|=D$.  An upper bound on the primal is the value
$D + y b$, which is computed without any knowledge of the
algorithms used by the software package.  By the duality theorem
for linear programming, there exists a certificate for which this
computed value is exactly the maximum of the primal.

In the packing problem, in practice we have a priori bounds
on the variables $x$, and this method works extremely well.  Note
that the a priori bound $m$ is allowed to be an extremely crude estimate,
because in producing the estimate $D$, it is multiplied by $\delta$,
which typically has the magnitude of a machine epsilon.

\subsection{infeasibility}


In practice, many of the primal linear programs that appear
in the packing problem are infeasible.  This requires
some small adjustments to the previous discussion.   In the
linear programs that we encounter in this book, it is not
necessary to determine the exact upper bound of a the linear program.
We have an explicit constant $K$, and we wish to prove that
the maximum of the objective function is less than $K$.
Rather
than consider two separate
types of problems, feasible and infeasible, we recast all the linear
programs in this book in terms of infeasibility.
When the maximum of $c x$ is less than $K$, by
adding the constraint $c x\ge K$ to the system of constraints
$A x\le b$,  we get an infeasible linear program.
This is the vanishing box trick:  the counterexamples to a nonlinear
problem are constrained to lie inside an infeasible system of linear
inequalities.


We now drop the objective function, and work
with a system of inequalities  of the form
\begin{equation}
\label{eqn:lpsys}
\begin{array}{lll}
A x &\le c\\
a &\le x \le b,\\
\end{array}
\end{equation}
with explicitly given lower and upper bounds $(a\le x\le b)$ on
the variables $x$.  The problem
is to verify that the system is infeasible; that is, there does
not exist an $x$ satisfying the system of inequalities.  By
translating dual linear program certificates into this new context
we are led to the following definition.

\begin{definition}[certificate~of~infeasibility]
A certificate of infeasibility for the system \ref{eqn:lpsys},
is a pair $(u,v)$ satisfying
$$
\begin{array}{lll}
0&\le u\\
0&\le v\\
0&\le u A + v\\
0& > u(c-Aa) + v(b-a). \\
\end{array}
$$
\end{definition}

\begin{example}
If there is an index $i$ for which $b_i < a_i$, then the system is
clearly infeasible.  In this case, we have an obvious certificate
of infeasibility given by $(u,v)=(0,e_i)$, where $e_i$ is the
standard basis vector at index $i$.
\end{example}

\begin{lemma}
If a certificate of infeasibility $(u,v)$ exists for the system
\ref{eqn:lpsys}, then the system is infeasible.
\end{lemma}

\begin{proof}
Suppose for a contradiction that $x$ is a feasible solution.
Then
$$
\begin{array}{lll}
0 &\le (u A + v)(x-a) \\
&= [u (c- A a) + v (b- a)] - [u (c - A x)] - [v (b - x)]\\
&< 0.
\end{array}
$$
\end{proof}

As in the case of dual linear program certificates, 
to prove a system infeasible, it is not necessary to know how a
certificate of infeasibility is produced.  It can be produced 
by an untrusted computer algorithm.  All that is needed
to be known is that it is a certificate.

If an approximation $(u',v')$ to a certificate of infeasibility is
produced (say by computer), it can often be adjusted to give a
true certificate.  Given an approximation $(u',v')$, define
$(u,v)$ by
$$
\begin{array}{lll}
u &= \max(0,u')\\
v &= \max(0,v',-u A).
\end{array}
$$
If $(u,v)$ satisfies
$$u( c-A a) + v(b-a) <0,$$
then it is a certificate of infeasibility.

The theory of duality for linear programming insures that if a
system is infeasible, then a certificate of infeasibility exists.
Since the equations defining a certificate of infeasibility are
linear in $u$ and $v$, linear programming algorithms may be used
to produce certificates:
$$
\begin{array}{rll}
\min_{u,v} &u (c-A a) + v(b-a) \text{ such that }\\
0 &\le u\\
0 &\le v \\
0& \le u A + v\\
\end{array}
$$
This has a feasible solution $u=v=0$.  If we add the constraint
that the objective function is at least $-1$, then there exists a
bounded feasible solution.


\subsection{linear relaxation}

Suppose that we have a continuous
nonlinear function $f$ that we wish to maximize
over a compact domain $X\subset \ring{R}^n$.    A linear relaxation
of the domain is a polyhedron defined by constraints $A x \le b$
such that the polyhedron contains the domain $X$.  That is,
$x\in X$ implies $A x \le b$.    A linear relaxation of the objective
function $f$ is a linear function $x\mapsto c x$ such that
$f(x) \le c x$ for all $x\in X$.  It then follows trivially
that the maximum of $f$ over $X$ is at most the value of 
the linear program that maximizes $c x$ such that $A x \le b$.

(A popular article about the proof of the packing problem
illustrated the method by depicting the nonlinear function $f$
as the rolling hills in the countryside, cows grazing in the background.  
A helicopter,
carrying a large roof, was slowing
descending over the hilltops, bounding the height of a hilltop
with the piecewise linear roof.)

Relaxation discards information about the nonlinear behavior of the
optimization problem.  It should be expected that the bounds
obtained by this method will often be crude.  However, it has the
definite advantage that linear programs can be solved efficiently,
while general nonlinear programming problems cannot.

For the problem to be a true linear relaxation, there must be rigorous
proofs that each $x\in X$ satisfies each linear constraint $A_i x\le
b_i$.  This means that the maximum of $A_i x$ over $X$ must be at most
$b_i$.  This again, is a nonlinear optimization problem on the domain
$X$ with objective function $A_i x$.  For an arbitrary constraint $A_i
x\le b$, and complicated domain $X$, there is no reason to suspect
that this is any easier to solve than the original optimization
problem.  Thus, we are in danger of replacing one intractible problem
with another.

The packing problem has a special structure that avoids this
problem.  Even though the nonlinear optimization problem involves
around 150 variables, 
this special structure allows all the nonlinearities to 
be confined to small dimensions (around 6 variables).  In other words,
the packing problem can be
described as a coupled system of small nonlinear subsystems,
and all the coupling comes through linear systems of equations. 
We give a toy model of this in a moment to describe how this works.

This special structure is one of the key points of the entire proof.
If you want to know why this hard nonlinear optimization problem has
been solved, but others have not, you now know the fundamental reason.
By partitioning the linear relaxation of the domain $X$ according
to the small nonlinear subsystems, we can arrange for each inequality
$A_i x\le b_i$ to be sparsely populated, and for each relaxation
constraint to be a statement about a nonlinear inequality on a domain
of low dimension.  This is a tractible problem:
we  summon interval arithmetic to prove these
low dimensional inequalities by computer.  In this way we are able
to give a rigorous proof that we have a true linear relaxation of a
difficult nonlinear optimization problem.  In practice, the linear
relaxation  bounds  are good enough to solve the packing
problem.





\clearpage
