\chapter{Appendix}

\begin{note}%XX
The appendix is not for publication.  It contains some further notes about the
proof that may be of interest to a few experts.
\end{note}

Beyond the text, the proof relies on three separate external bodies of
computer code.  These are described in much greater detail at various
places in the book.  These external bodies of code are called
\begin{enumerate}
\item Tame Hypermap Generation,
\item Interval Arithmetic,
\item Linear Programming.
\end{enumerate}
The tame hypermap generation is an stand-alone program that is run
once at a specific point of the proof.  It carries out a combinatorial
classification of planar graphs satisfying a certain restrictive list
of properties.  The reader can safely ignore this computer program
until reaching the relevant point in the proof.  By contrast, the
interval arithmetic is a collection of nearly one thousand
inequalities that have been proved by computer.  These inequalities
are spread throughout the proof and appear throughout this book.  On
first reading, the reader is encouraged to accept these inequalities
as axiomatically given facts.  Detailed documentation about these
inequalities is available for those who wish to follow up later on the
computer-generated proofs of these inequalities.  The final stage of
the proof consists entirely of linear programming.  There are also
several small linear programs that appear in scattered places in the
book.

\section{Computation}

As this project  progressed, the computer  replaced conventional
mathematical arguments more and more, until now
nearly every aspect of the proof relies on
computer verifications.  Many assertions in this book
are results of computer calculations.
To make the solution more accessible, I have
posted extensive resources \cite{web}.

There are three major pieces of computer code that enter into the proof.

\begin{itemize}
\item {\it Combinatorics}.  A computer program classifies all of the
  planar hypermaps that are relevant to the packing problem.
\item {\it  Interval analysis}.  ``Sphere Packings
I'' describes a method of proving various inequalities in a small number
of variables by computer by interval arithmetic.
\item {\it  Linear programming}.  Many of the nonlinear optimization
problems for the scores of sphere packings are replaced by linear
problems that dominate the original score.  They are solved
by linear programming methods by computer.  A typical problem has
between 100 and 200 variables and 1000 and 2000 constraints.  Nearly
100000
such problems enter into the proof.
\end{itemize}

Computers are used in various other ways.  


\begin{itemize}
\item {\it Formal proof}.
Various parts of the solution of the packing problem have now been
formally verified.
\item  {\it Numerical optimization}.  The exploration of the problem
has been substantially
aided by nonlinear optimization and symbolic math packages.
\item {\it Branch and bound methods}.  When linear programming methods
  do not give sufficiently good bounds, they have been combined with
  branch and bound methods from global optimization.
\item {\it Computer Algebra Systems} Mathematica was used for many
  minor calculations, such as the calculation of exact explicit
  formulas for derivatives.
\item {\it Organization of output}.
The organization of the few gigabytes of code and data that
enter into the proof is in itself a nontrivial undertaking.
\end{itemize}



\section{Formal Proof}


\section{Tame Hypermaps}

\clearpage

\section{Interval Analysis}%DCG 8.3, p75
\label{sec:bounds-simplex}

Interval analysis is a method to obtain trustworthy results from a
computer.  This essay gives a basic introduction to this method.

\subsection{deliberate error}

Interval arithmetic can trace its origins to the method of deliberate
error.  This is an ancient method of navigation with imperfect
instruments.  When William the Conqueror crossed the English Channel
in 1066, he deliberately steered to the north of Hastings.

\begin{quote}
  % ``There is no direct evidence of how a twelfth-century pilot found
  % his way across the English Channel $\ldots$
  ``Any pilot even now who had to make that crossing without a chart
  or compass, as William's pilots did, would use the ancient method of
  Deliberate Error: he would not steer directly towards his objective
  but to one side of it, so that when he saw the coast he would know
  which way to turn.'' \cite[p81]{How81}
  % 1066: The Year of the Conquest, Penguin paperback edition
  % 1981. page 148.
\end{quote}

Deliberate error implements ``better safe than sorry.''
To find an address on a one-way street,  a  driver enters the
street too early, to point in the right direction.  
When searching for a familiar landmark
on a two-way street, it is more efficient  
to start the search safely to one side of the landmark, 
rather than search in ever
expanding zigzags in both directions.  
Deliberate error 
is the idea of trapping a target inside a large enough net, in the
way that
Khachiyan traps the optimal solutions of a linear program inside
an ellipsoid.


The method of deliberate error cushions the adverse effects of
imperfect technology.  The method does not aim to minimize the
imperfections.  It works with a faulty chart and compass.
% The method of deliberate error seeks not to minimize the error of
% imperfect technolo

\subsection{arithmetic}

The method of deliberate error, implemented to control for round-off
errors on a computer, leads to interval arithmetic.  To approximate
the circumference of a circle, Archimedes inscribes a polygon and
circumscribes a second polygon, trapping the unknown within a known
interval.  This is interval arithmetic.


Fixed precision floating point numbers on a computer exactly represent
a finite number of rational numbers.  The remaining uncountable set of
real numbers cannot be precisely represented.  For example, 64-bit
floating point numbers can encode at most $2^{64}$ different real
numbers.  This finite set of representable numbers is not closed under
addition, multiplication, subtraction, or division.  For example, the
input $2.0 + 2.0$ returns $4.0$, because all the numbers involved are
precisely representable.  However, through round-off error, my
computer returns
\begin{verbatim}0.0
\end{verbatim} 
in response to
\begin{verbatim}
-1.0 + (1.0 + 1.0e-16)
\end{verbatim}
and 
\begin{verbatim}1.0e-16
\end{verbatim} 
in response to 
\begin{verbatim}
(-1.0 + 1.0) + 1.0e-16.
\end{verbatim}
As this example shows, machine addition is not even associative.  One
is quickly led to absurd conclusions, if one tries to reason about
floating point operations as if they formed a group, a ring, or a
field.  To reason correctly about floating point, one must accept
these operations on their own terms.  To distinguish the imprecise
machine (floating point) operations from standard operations on the
field of real numbers, in the rest of this section (except in computer
code listings), place a dot over the floating point operations $(\dot
+)$, $(\dot -)$, and so forth.

The precise behavior of floating point operations on a computer is
governed by the IEEE-754 floating point standard \cite{Gol}.  By
giving a precise specification of properties of floating point
arithmetic, the standard makes it possible to reason about the
behavior of floating point.  It is possible to prove theorems about
the behavior of IEEE floating point.  For example, it is possible to
prove that a computer that correctly implements the standard should
return the values in reponse to the inputs given above (in the nearest
rounding mode).

Let $F$ be the set of machine-representable floating point numbers.
Assume that $F$ contains two special symbols $\pm\infty$.  The total
order on the real numbers is extended to $\ring{R}\cup\{\pm\infty\}$,
with $-\infty < x < \infty$ for all $x\in\ring{R}$.  Because of these
two special symbols, the floating point floor $x\to\floor{x}_F\in F$
and ceiling $x\mapsto\ceil{x}_F\in F$ functions, with domain
$\ring{R}$ can be defined.  In this section, we drop the subscript $F$
on the floor and ceiling functions.  We map the set of real numbers to
$F^2$, by sending $x$ to $[a,b]$, where $a = \floor{x}$ and
$b=\ceil{x}$.  If $x$ is Hastings, then $b$ is a point on the shore
north of Hastings.  It is the deliberate error to one side of the
target.





On most modern processors, the rounding mode can be set to directed
rounding, as described in the standard.  When the rounding mode is set
upward, the result of any basic arithmetic floating-point operation
$(\dot +)$, $(\dot -)$, $(\dot *)$, $(\dot /)$ applied to two floating
point numbers $(x,y)\mapsto x\dot\diamond y$ is defined by standard to
be the $\ceil{x\diamond y}$.  That is, make the calculation in the
field of real numbers and round up to the next floating point.  When
the rounding mode is set downward, set $x\dot\diamond y =
\floor{x\diamond y}$.  The notation $\dot\diamond$ does not show
rounding mode, but it should, because it is mode dependent.


Interval arithmetic, like the method of deliberate error, does not
seek to eliminate the sources of floating point round off error.
Rather it brings it under scientific control.


Let $I_F = \{[a,b] \in F^2 \mid a \le b\}$ be the set of floating
point intervals.  Basic machine operations extend to intervals.  The
sum $[a_3,b_3]$ of $[a_1,b_1]$ and $[a_2,b_2]$, is defined as
$a_3=\floor{a_1+a_2}$ and $b_3 = \ceil{b_1+b_2}$.  Write $[a_3,b_3] =
[a_1,b_1] \dot+ [a_2,b_2]$.  This addition of intervals is not
associative.  However, addition of intervals satisfy a crucial
inclusion property.  If $x\in[a_1,b_1]$ and $y\in [a_2,b_2]$, and $z =
x+y$, then $z\in [a_3,b_3]$.  The other arithmetic operations can be
extended to machine interval operations in a similar way, so as to
satisfy the corresponding inclusion properties.  Division requires
special treatment when $0$ belongs to an interval denominator.  We
will not go into those details here.

These operations are easily implemented in code.  For example, here is
the actual snippet of {\tt C++} code that implements the addition of
intervals.  The functions {\tt interMath::up()} and {\tt
  interMath::down()} set the rounding modes on the computer.
\begin{verbatim}
inline interval interval::operator+(interval t2) const
{
interval t;
interMath::up(); t.hi = hi+t2.hi;
interMath::down(); t.lo = lo+ t2.lo;
return t;
}
\end{verbatim}

By implementing interval operations on a computer, we can develop a
procedure that takes as input an arbitrary arithmetic expression over
the rational numbers and returns an interval $[a,b]$ with floating
point endpoints (augmented by $\pm\infty$ as usual) that contains the
rational value of that expression.  The true answer, still unknown,
lies trapped between the interval endpoints.  Since the associative
and distributive laws fail, the returned interval $[a,b]$ depends on
the actual syntax of the expresssion, on the placement of parentheses,
and so forth.



\subsection{analysis}

This essay is not a treatise on interval arithmetic.  Its purpose is
to give an introduction, to demonstrate the possibility of rigorously
bounding the error of floating point performed according to IEEE
standards.  We become increasingly sketchy.

The theory of interval analysis progresses step by step, starting with
basic arithmetic and developing through higher levels of analysis.  If
$p$ is a polynomial with rational coefficents
$p\in\ring{Q}[x_1,\ldots,x_n]$, then it has an interval extension
$\bar p : I_F^n \to I_F$.  Just as in the case of rational numbers,
the interval extension $\bar p$ depends on the syntax used to
expressed to polynomial $p$.  The polynomial extension is {\it
  monotonic}: if $z_i\in [a_i,b_i]$ for all $i$, then
$p(z_1,\ldots,z_n) \in \bar p([a_1,b_1],\ldots,[a_n,b_n])$.  Interval
extensions of rational functions is similar.

Consider a function $f:[a,b]\to\ring{R}$ that
has a rational function approximation $r$ with known
error bound:  $|f(x) - r(x)|<\epsilon$ for $x\in[a_1,b_1]$,
with $\pm\epsilon\in F$.
Define a monotonic interval extension $\bar f$ of $f$ by
$\bar f([c,d]) = \bar r([c,d]) \dot + [-\epsilon,\epsilon]$, for
$[c,d]\subset [a,b]$.  
Multivariate functions are similar.   
%
A large class of analytic functions fall within this framework.
We have interval extensions of trigonometric functions, logs, and
exponentials.  Interval extensions of real-valued functions can be
added, multiplied and composed.  

To prove that a function $f$ is positive on a rectangular domain
$[a_1,b_1]\cdots[a_n,b_n]$, compute the
value of an interval extension $[c,d]=\bar f([a_1,b_1]\cdots[a_n,b_n])$.
By the monotonic property of interval extensions, if $c>0$, then $f$
is positive on the domain.  If this is done naively, with the
first interval extension $\bar f$ that comes to mind, the image interval
$[c,d]$ may be so large that no worthwhile information results.
If we naively 
compute $x+(-x)$ by interval arithmetic for $x$ in the interval $[-1,1]$,
we find that $x+(-x)$ lies in the interval sum
$[-1,1]\dot + [-1,1] = [-2,2]$.  Useless!
There is a large mathematical literature describing various 
efficient algorithms
to compute image intervals $[c,d]$ with accuracy.
Kearfott's book is a recommended starting point, because it comes
close to describing the types of algorithms implemented for the
solution to the packing problem \cite{Kea96}. 



\subsection{archive}

Although this book is long, it represents only a fraction of the
solution of the packing problem.  The other resources that are needed
to understand the full solution are available on the internet.

There is an archive of several hundred inequalities that have been
proved by computer.  The list of inequalities can be found at
\cite{web}.\footnote{The archive of interval arithmetic inequalities
  appears at \url{http://flyspeck.googlecode.com/svn/trunk/}} The list
of inequalities are in computer readable form in the rigorous
mathematical syntax of HOL-Light (for Higher Order Logic).

For example, one of the inequalities from that file reads as follows.
\begin{verbatim}
let I_572068135=
all_forall `ineq 
[((square (#2.3)), x1, (#6.3001));
((#4.0), x2, (#6.3001));
((#4.0), x3, (#6.3001));
((#4.0), x4, (#6.3001));
((#4.0), x5, (#6.3001));
((#4.0), x6, (#6.3001))
]
((((tau_sigma_x x1 x2 x3 x4 x5 x6) -.  ((#0.2529) *.  
(dih_x x1 x2 x3 x4 x5 x6))) >. (--. (#0.3442))) \/ 
((dih_x x1 x2 x3 x4 x5 x6) <.  (#1.51)))`;;
\end{verbatim}
The first part of this snippet of code gives lower and upper bounds on
each of the six variables $x_1,\ldots,x_6$.  The final part of this
code gives an inequality (or rather a disjunction of two inequalities)
of nonlinear real-valued functions that holds on the given domain.
The $\#$ symbol marks exact decimal constants.  The functions {\tt
  dih\_x}, {\tt tau\_sigma\_x} are defined rigorously in a separate
file.  They correspond to the functions $\dih$ and $\tau$ in this
book.  The symbols for arithmetic operations are followed by periods
($*.$ and so forth) to distinguish them from the corresponding
operations on natural numbers.

The inequality carries a nine-digit identifier {\tt 572068135}.  This
number is a tracking number that can be used in a search engine to
locate everything known about a given inequality.  For example, if one
googles this number, the search engine returns nine matches related to
this inequality, including a preprint on the arXiv, the website of the
Annals of Mathematics, a Springer Link to the relevant issue of the
journal Discrete and Computational Geometry, a flyspeck discussion
group, the {\it C++} computer code proving the inequality, and the
output file from running that code.  A search on the pdf file of this
book links this inequality to Lemma~\ref{lemma:11.16}.  Every interval
arithmetic calculation in this book carries a nine-digit identifier,
for easy tracking.

Currently, the most convenient way to access the code that proves the
inequality is through the Google's Code Search, a custom search engine
for computer code.\footnote{The interval arithmetic code that proves
  the inequalities is available at
  \url{http://code.google.com/p/flyspeck/}.}


Further information about proving these inequalities by interval
arithmetic can be found in \cite{algorithm} and \cite{part1}.
%\indy{Index}{calc@\calc{123456789}}%

\subsection{code verification}

S. Ferguson and I put considerable effort into developing trustworthy
code.  The two of us made entirely independent implementations of
the code. %automated inequality proving by interval arithmetic. 
(We shared algorithms
but not source code.)  This allowed us to check our answers against each
other to ensure mutual consistency, 
and to eliminate certain sources of bugs.  We also made independent
implementations in Mathematica and {\tt C} of traditional floating point
versions of the functions used for the nonlinear
optimizations.  By requiring these different implementations to give
compatible answers, we eliminated further sources of bugs.

Each nonlinear inequality was also checked independently with the
nonlinear optimization package {\tt cfsqp}.  This is a collection of
{\tt C} routines that searches for the minimum of a smooth function on
a domain described by a system of constraints.  As is the case with
many nonlinear optimization packages, there is no guarantee that the
search will converge to the true global minimum of the function.  By
repeating the search with a large collection of initial values for the
search, it becomes more probable that the true global minimum will be
found.  In practice it works remarkably well.

This package was not used in any proofs.  The numerical testing with
{\tt cfsqp} was used to discover false inequalities before they were
shipped to the interval arithmetic prover for verification.  Those
that failed never shipped.  This extra level of testing adds an extra
level of robustness to this part of the proof.  Testing gives us
(nonrigorous) reasons to believe the inequalities, even if a bug
should appear in our interval arithmetic code.  This gives us some
hope that an undetected bug would be unlikely to affect the overall
design of the solution to the packing problem.


There are certain types of bugs that would be very difficult to
detect.  For example, since floating point arithmetic is not
associative, a misplaced pair of parentheses might throw a calculation
off by a machine epsilon.  This potential source of bugs is evidence
that the entire project was not sufficiently automated: the
parentheses should all have been worked out automatically.  To make
the calculations more robust, we have tried to design the collection
of inequalities so that they hold with a considerable margin of error,
rather than just squeeze by.  (There are only a few inequalities that
are sharp, and they are sharp for clear mathematical reasons related
to the theory of packings.)  In a bug-free environment, such
precautions would not be necessary.  Nonetheless, we take precautions.

As far as I know, no comprehensive efforts were made by the referees
and editors to check the correctness of the computer code before the
publication of the 1998 solution to the packing problem.  The editors'
preface to \cite{DCG} states that during the review process ``some
computer experiments were done in a detailed check.''


The flyspeck project is a long-term project intended to make the
solution to the packing problems one of the most thoroughly checked
computer proofs of all times.  Part of this project calls for a formal
proof of correctness of the computer code used in the interval
verification of inequalities.

Flyspeck is still far from completion in 2008.  Nevertheless, there
are various ongoing projects related to this second-generation
verification of the interval code.  S. McLaughlin has an independent
implementation of the interval arithmetic code used in the packing
problem~\cite{McL08}.  His work has exposed some data-entry bugs.
They are reported in the comprehensive errata to the 1998 proof, which
is maintained at \cite{errata} with additional discussion
at~\cite{flydis}.  Fortunately, no bugs have surfaced over the past
decade in the underlying interval arithmetic inequality proving
algorithms.  The reported errors have been at the data-entry level: a
mismatch between the data as typed into the preprint and data in
computer code.

One of the formal proof assistants under most active development is
the COQ system~\cite{COQ}.  R. Zumkeller has implemented automated
inequality proving with interval arithmetic inside the theorem prover
COQ, with the flyspeck project in mind; although (as of 2008) the
inequalities that are used in this book have not yet been checked in
this way \cite{Zu}.


\subsection{interval analysis and proof}


The editors of the Annals of Mathematics have posted a statement on
computer-assisted proof.  At first, the editors planned to make a
disclaimer directed at the computer solution of the packing problem.
Eventually, they formulated a general policy on computer-assisted
proofs.  The policy mentions interval arithmetic as one way to control
sources of computer error.


\begin{quote}
%Statement by the Editors on Computer-Assisted Proofs

  ``Computer-assisted proofs of exceptionally important mathematical
  theorems will be considered by the Annals.

  ``The human part of the proof, which reduces the original
  mathematical problem to one tractable by the computer, will be
  refereed for correctness in the traditional manner. The computer
  part may not be checked line-by-line, but will be examined for the
  methods by which the authors have eliminated or minimized possible
  sources of error: (e.g., round-off error eliminated by interval
  artihmetic, programming error minimized by transparent surveyable
  code and consistency checks, computer error minimized by redundant
  calculations, etc. [Surveyable means that an interested person can
  readily check that the code is essentially operating as claimed]).

  ``We will print the human part of the paper in an issue of the
  Annals. The authors will provide the computer code, documentation
  necessary to understand it, and the computer output, all of which
  will be maintained on the Annals of Mathematics website online.''
  \cite{Ann06}

%http://annals.princeton.edu/EditorsStatement.html
\end{quote}

A number of proofs in pure and applied mathematics have been based on
interval analysis.  W. Tucker implemented a rigorous ODE solver with
interval arithmetic and used it to prove that the Lorenz equations
have a strange attractor \cite{Tuc02}. The existence of strange
attractors is problem 14 on Smale's list of 18 Centennial Problems
\cite{Sma98}.  Another prominent problem solved by interval methods is
the double bubble conjecture, a generalization of the isoperimetric
problem in three dimensional Euclidean space.  A sphere gives the
solution to the classical isoperimetric problem.  The work of J. Hass,
M. Hutchings, and R. Schlafly shows that the surface area minimizing
way to enclose two regions of equal volume is the double bubble, which
consists of two partial spheres, separated by a flat circular disk
\cite{HHS95}.

Interval arithmetic has also yielded a number of new results on the
problem of packing circles in a square. M. Cs. Mark\'ot and T. Csendes
have obtained optimality proofs for packings of $28$, $29$, and $30$
circles in a square.  See Figure~\ref{fig:optimal-circles}.  This is
an area of active research. See, for example, \cite{Sza07} and
\cite{Mark07}.

%% WW not yet done.
\begin{figure}[htb]
\centering
\myincludegraphics{noimage.eps}
\caption{Optimal circle packings in a square}
\label{fig:optimal-circles}
\end{figure}



\subsection{note}

There are those who have tried to downplay the role of computers and
interval methods in the solution to the packing problem.  In fact,
they play an absolutely central role.  To segregate the computation
destroys the proof.  After writing the paper {\it The Sphere Packing
  Problem}, I had all but given up on solving the problem.  I had an
extremely difficult nonlinear optimization problem on my hands and no
rigorous mathematical method to solve it.  In the summer 1993, I
happened upon book on Pascal-XSC (a language extension of Pascal for
interval analysis) at the Seminary Coop Bookstore next to the
University of Chicago.  This book described the method I had lacked.
With fresh hope, in January 1994, I set aside all else and devoted
full effort to the packing problem.  The interval code was the most
difficult part of the computer code to implement because its speed was
crucial.  Thanks to the improvements of S. Ferguson, eventually the
code could run from beginning to end in about three months.  The
interval verifications were the last part of the proof to be completed
in August 1998.


\clearpage

\section{Inequality Listings}

%% XX MOVE ALL THIS ELSEWHERE.

\subsection{packings, general inequalities}

This appendix gives a summary of the nonlinear inequalities that have
been cited in the chapter on packings.  Information about the computer
verifications can be found at \cite{hales:2009:nonlinear}.


\begin{note}%XX
This book contains a number of nonlinear inequalities that have been
established by interval-arithmetic calculations by computer.  Some
of these interval arithmetic calculations are still in the process
of being verified.  The approach to the proof of the Kepler
conjecture described here is still work in progress.  A description
of the inequalities and their current status can be found
at~\cite{hales:2009:nonlinear}.
\end{note}


(Note that the following is an inequality in at most six variables; the most
difficult case to prove is that of a $4$-cell.)  Formulas for the
volumes and solid angles appear in Chapter~\ref{chapter:volume}.  An
explicit formula for the dihedral angle appears in
Chapter~\ref{part:trig}.


\begin{calculation}\label{calc:marchal}\guid{WJDLOCM}\guid{1025009205}\guid{3564312720}\rating{ZZ}
%% cc:mar are the k-cell estimates for non-cell clusters.
Define the function $M$ by equations (\ref{eqn:M}) and
(\ref{eqn:m-def}).  Define the function $\gamma(X,M)$ by equation
(\ref{eqn:gamma-def}).  If $X$ is a $0$, $1$, $2$, $3$, or $4$-cell,
then
\begin{displaymath}
\gamma(X,M)\ge 0.
\end{displaymath}
\end{calculation}

\begin{calculation}\label{calc:cc:qtr}\guid{GLFVCVK}\guid{4869905472}\guid{2477216213}\guid{8328676778}\rating{ZZ}
Let $\gamma_L$ be given by Definition~\ref{def:gammaL}, $\op{wt}$ by
Definition~\ref{def:wt}, and $\beta$ by Definition~\ref{def:beta}.
If $X$ is any $k$-cell that is not a quater with $k\in\{2,3,4\}$,
then % gammaL is nonneg on quarters. cc:qtr
\begin{displaymath}
\gamma_L(X) \op{wt}(X) + \beta(e,X)\ge 0.
\end{displaymath} 
\end{calculation}

\begin{calculation}\label{calc:cc:2bl}\guid{FHBVYXZ}\guid{1118115412}\rating{ZZ}
Let $\gamma_L$ be given by Definition~\ref{def:gammaL}.  Let $X$ be
any quarter.  Let $Y$ be a $3$-cell that flanks it.  Then
\begin{displaymath}
\gamma_L(X)+\gamma_L(Y)\ge 0,
\end{displaymath}
% 2-leaf calculation, gammaL(fourcell)+gammaL(threecell) >=0. % cc:2bl:
\end{calculation}

\begin{calculation}\label{calc:cc:5bl}\guid{ZTGIJCF}\rating{ZZ}
Let
\begin{displaymath}
a= 0.0560305, \quad\text{and}\quad  b= -0.0445813.
\end{displaymath}
\begin{itemize}
\item \case{1821661595} A $4$-cell $X$ along a spine $e$ satisfies
\begin{displaymath}
\gamma_L(X)\op{wt}(X) + \beta(e,X) \ge a + b\,\op{azim}(X),
\end{displaymath}
\item \case{7907792228} The $2$-cell $X_2$ and two $3$-cells $X_1,X_3$
that flank it along a spine $e$ satisfy
\begin{displaymath}
\sum_{i=1}^3 \left(\gamma_L(X_i)\op{wt}(X_i) + \beta(e,X_i)\right)\ge a + b\,\sum_{i=1}^3\op{azim}(X_i).
\end{displaymath}
\end{itemize}
\end{calculation}

\begin{calculation}\label{calc:cc:disks}\guid{8550443271}\rating{ZZ}
Let
\begin{displaymath}
g(h) = \arccos(h/2) - \pi/6.
\end{displaymath}
If $h_1,h_2\in [1,\hm]$, then
\begin{displaymath}
\op{arc}(2h_1,2h_2,2) - g(h_1) - g(h_2)\ge 0.
\end{displaymath}
\end{calculation}

\begin{calculation}\label{calc:cc:alin}\guid{7991525482}\rating{ZZ}
Let $L$ be given by Definition~\ref{def:L}.
Let
\begin{displaymath}
g(h) = \arccos(h/2) - \pi/6.
\end{displaymath}
Let
\begin{displaymath}
\op{reg}(a,k) = 2\pi - 2 k (\arcsin(\cos(a)\sin(\pi/k))).
\end{displaymath}
Then
\begin{displaymath}
\op{reg}(g(h),k) \ge c_0 + c_1 k + c_2 L(h),\quad
k = 3,4,\ldots,\quad 1\le h\le \hm,
\end{displaymath}
where
\begin{displaymath}c_0 = 0.6327,\quad c_1 = -0.0333,\quad c_2 =
0.4754.\end{displaymath}
\end{calculation}

\begin{calculation}\label{calc:cc:alin2}\guid{8540377696}\rating{ZZ}
Let $L$ be given by Definition~\ref{def:L}.
Let
\begin{displaymath}
g(h) = \arccos(h/2) - \pi/6.
\end{displaymath}
Let
\begin{displaymath}
\op{reg}(a,k) = 2\pi - 2 k (\arcsin(\cos(a)\sin(\pi/k))).
\end{displaymath}
Let
\begin{displaymath}a'=\arc(2,2,2\hm)-g(\hm) \approx
0.797.\end{displaymath} Then for $k=3,4,\ldots$,
\begin{displaymath}\op{reg}(a',k) \ge c_0 + c_1 k + c_2 L(1) +
c_3\end{displaymath}
where 
\begin{displaymath}c_0 = 0.6327,\quad c_1 = -0.0333,\quad c_2 =
0.4754,\quad c_3 = 0.85.\end{displaymath}
\end{calculation}

\begin{calculation}\label{calc:shorts}\rating{ZZ}
The following calculations involve many cases that are enumerated by
computer code.
\begin{itemize}
\item \case{BIXPCGW} Let $Z$ be any cell-cluster along a spine $e$
with three leaves.  Then
\begin{displaymath}
\Gamma(Z)> 0.
\end{displaymath}
\item \case{QITNPEA} Let $Z$ be any cell-cluster along a spine $e$
with four leaves.  Then
\begin{displaymath}
\Gamma(Z)> 0.
\end{displaymath}
\end{itemize}
\end{calculation}




\subsection{local fan: listing}

\begin{calculation}\guid{2065952723}\rating{ZZ}\label{calc:Lexell}
%See Mathematica numerical calculation.
Let
\begin{displaymath}
g(s;a,b,c,e_1,e_2,e_3) = \sum_{i=1}^3 \dih_i(2,2,2,a+s,b,c) e_i,
\end{displaymath}
where $\dih_i$ is given by Definition~\ref{def:tau}.
Let $\Delta = \Delta(4,4,4,a^2,b^2,c^2)$.
Let primes denote derivatives with respect to the variable $s$.
Assume that
$e_i\in\leftclosed1,1+\sol_0/\pi\rightclosed$,  that
$a,b,c\in\leftclosed2/\hm,4\rightclosed$.
Then
\begin{equation}\label{eqn:calc:Lexell}
  \Delta (g'(0;a,b,c,e_1,e_2,e_3))^2 
- 0.01\Delta^{3/2}g''(0;a,b,c,e_1,e_2,e_3) > 0.
\end{equation}
(The factors of $\Delta$ clear the denominator in
(\ref{eqn:calc:Lexell}) to simplify the inequality to be proved.)
\end{calculation}

\begin{calculation}\guid{2158872499}\rating{ZZ}\label{calc:2der}
%% checked in Mathematica NMaximize
Let $y_1,y_2\in \leftclosed 2,2\hm\rightclosed$.  
\begin{itemize}
\item 
Let $g(t) = \arc(y_1,y_2+t,2)$.  Then $g''(0) < 0$.
Explicitly,
\begin{displaymath}
  g''(0) = \dfrac{
    -64 + 48y_1^2 - 12 y_1^4 + y_1^6 
  + 80 y_2^2 - 8 y_1^2 y_2^2 - 3 y_1^4 y_2^2
    - 12 y_2 ^4 + 3 y_1^2 y_2^4 - y_2^6
  }{y_2^2 \sqrt{\ups(y_1^2,y_2^2,4)}^3}
\end{displaymath}
and the polynomial in the numerator takes negative values on the given
domain.
\item
Let $g(t) = \arc(y_1+t,y_2-t,2)$.  Then $g''(0) < 0$.
Explicitly,
\begin{displaymath}
  g''(0) = \dfrac{\sqrt{\ups(y_1^2,y_2^2,4)} \left(
      -4 y_1^2 + y_1^4 - 4y_1^3 y_2 - 4y_2^2 
   + 6 y_1^2 y_2^2 - 4 y_1 y_2^3 +y_2^4
    \right)}{y_1^2 y_2^2 (2+y_1-y_2)^2 (2+y_2-y_1)^2}
\end{displaymath}
and the polynomial in the numerator takes negative values on the given
domain.
\end{itemize}
\end{calculation}

\begin{calculation}\guid{2986512815}\rating{ZZ}\label{calc:cc:qua}  %m11
Let $y_1y_2,y_3,y_7\in \leftclosed 2,2\hm\rightclosed$,
$y_5,y_8,y_9\in \{2,2\hm\}$, $y_4,y_6\ge 2\hm$.
Let $x_i = y_i^2$.
Assume that
\begin{displaymath}
\Delta(x_1,x_2,x_3,x_4,x_5,x_6)>0,\quad{ and }
\Delta(x_3,x_2,x_7,x_9,x_8,x_4)>0.
\end{displaymath}
Assume that
\begin{displaymath}
\dih(y_3,y_1,y_2,y_6,y_4,y_5)+\dih(y_3,y_2,y_7,y_9,y_8,y_4) < \pi
\end{displaymath}
and\footnote{If $\{\v_1,\ldots,\v_4\}$ is a set of vectors such that
$y_i = \normo{\v_i}$ and $y_{ij} = \norm{\v_i}{\v_j}$, then
$\op{cross}(y_4,\ldots,y_{12}) = \norm{\v_2}{\v_4}$.}
\begin{displaymath}
\op{cross}(y_1,y_2,y_3,y_4,\ldots,y_9) \ge y_4.
\end{displaymath}
Let 
\begin{displaymath}g(t;y_1,\ldots,y_9) =
  \tau_{tri}(y_1,y_2,y_3,y_4+t,y_5,y_6)+\tau_{tri}(y_3,y_2,y_7,y_9,y_8,y_4+t).
\end{displaymath}
Then \begin{displaymath}g'(0)^2 - 0.01 g''(0) > 0.\end{displaymath}
\end{calculation}


\begin{calculation}\guid{EFJSUSK}\rating{ZZ}\label{calc:irred} %cc:tau
%%cc:par
Let $(V,E,F,G)$ be an irreducible minimal fan with parameters
$(r,s)$.  Then $\tau(V,E,F) \ge d(r,s)$.  (A separate calculation
has been made for each of the cases in the list given above.)
% Interval arithmetic
% calculations~ %% cc:par partition cases for tau[r,s].
\end{calculation}










%\begin{calculation}\guid{5779862781}\rating{ZZ}\label{calc:cc:d2a}
%  Let $y_5,y_6\in \{ 2,2\hm\}$, $y_1,y_2,y_3\in \leftclosed
%  2,2\hm\rightclosed$, and $y_4\in \leftclosed 2,4\hm\rightclosed$.
%  Let $g(t) = \tau_{tri}(y_1+t,y_2,y_3,y_4,y_5,y_6)$.  If
%  $\Delta(y_1^2,y_2^2,\ldots,y_6^2)> 0$ and if $ g'(0)=0, $ then
%  $g''(0)<0$.\footnote{The function $g(t)$ may fail to be
%    differentiable at $t=0$ for parameters $y_1,\ldots,y_6$ for which
%    $\Delta(y_1^2,\ldots,y_6^2)=0$.  Thus, it is necessary to work on
%    the noncompact domain $\Delta>0$ for this inequality.}
%%  (The proof is an interval arithmetic calculation over a
%%  four-dimensional space~cc:d2a.  %%cc:d2a
%%  The calculation verifies that the second derivative is negative
%%  whenever the derivative is zero.)
%\end{calculation} 
%

%\begin{calculation}\guid{6645853705}\rating{ZZ}\label{calc:cc:d2b}
%  Let $y_5\in \{2,2\hm\}$, $y_1,y_2,y_3\in \leftclosed
%  2,2\hm\rightclosed$, and $y_4\in \leftclosed 2,4\hm\rightclosed$.
%  Let $g(t) = \tau_{tri}(y_1+t,y_2,y_3,y_4,y_5,y_6)$.  If
%  $\Delta(y_1^2,y_2^2,\ldots,y_6^2)> 0$, if
%\begin{displaymath}
%\arc(y_1,2\hm,2) + \arc(y_2,2\hm,2) 
%\le y_6 \le \arc(y_1,2,2\hm)+\arc(y_2,2,2\hm)
%\end{displaymath} 
%and if
%$
%g'(0)=0,
%$
%then $g''(0)<0$.
%\end{calculation}
%
%\begin{calculation}\guid{5606476569}\rating{ZZ}\label{calc:cc:qua}
%Let $y_{12},y_{23},y_{34},y_2,y_3\in\{2,2\hm\}$.
%Let $y_1,y_4\in \leftclosed 2,2\hm\rightclosed$.
%Let $y_{14}\in\leftclosed 2\hm,y_1+y_4\rightclosed$.
%Let $y_{13}\in\leftclosed 2\hm,y_1+y_3\rightclosed$.
%Let $g(t) = \tau_{tri}(y_1+t,y_2,y_3,y_4,y_5,y_6)$.
%If 
%\begin{displaymath}
%\Delta(y_1^2,y_2^2,y_3^2,y_{23}^2,y_{13}^2,y_{12}^2)> 0 \text{ and }
%\Delta(y_1^2,y_4^2,y_3^2,y_{43}^2,y_{13}^2,y_{14}^2)> 0,
%\end{displaymath} 
%if\footnote{If $\{\v_1,\ldots,\v_4\}$ is a set of vectors such that
%  $y_i = \normo{\v_i}$ and $y_{ij} = \norm{\v_i}{\v_j}$, then
%  $\op{cross}(y_4,\ldots,y_{12}) = \norm{\v_2}{\v_4}$.}
%\begin{displaymath}
%\op{cross}(y_4,y_1,y_3,y_{13},y_{34},y_{14},y_2,y_{23},y_{12})\ge y_{13},
%\end{displaymath}
%and if
%$
%g'(0)=0,
%$
%then $g''(0)<0$.
%\end{calculation}
%
