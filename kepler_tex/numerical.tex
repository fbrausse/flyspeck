% File started Jan 24, 2010.
%
\chapter{Numerical Methods}

Computers have been used at several significant places in the book.     The calculations
can be sorted into three general categories: linear programming, nonlinear
optimization, and the combinatorics of graphs. This
appendix goes into further detail about these general categories.  


\section{Linear Programming}

The solution of the packing problem is based on the {\it vanishing
box trick.}  At the outset, the set of 
counterexamples to the problem has an unknown size and structure.
These counterexamples are all placed in a large box.   
The width of the box is measured and is found to be negative.
Therefore the box is empty, it contains no counterexamples, and
the problem is solved.

It may seem that such a strategy is too hopelessly naive to work.
In fact, precisely this strategy is pursued, and it works beautifully.
The act of placing a counterexample in a box is called linear 
relaxation.  The counterexamples form an unknown nonlinear set.
The box that contains them is defined by linear constraints.  
Relaxation refers to a relaxation of constraints, so that the
box contains the counterexamples -- it is not merely a linear
approximation to the set of counterexamples.  The box needn't be
rectangular.  Any polyhedron will do.  To determine that a box
has negative width is to say that it gives an {\it infeasible} linear
program.  With the strategy, there is no need to limit ourselves
to a single box, we can use a finite collection of boxes that
contain the set of counterexamples.  This leads to branch and bound
methods.


In this appendix, we will discuss these methods in further detail,
including linear programming, linear relaxation,  infeasibility,
and branch and bound methods.

The introduction of linear programming methods to the packing
problem was gradual and tranquil.  
Many of the
inequalities in the packing problem have an obvious linear form.
As an engineering student
at Stanford, I had studied
linear and nonlinear optimization.
Given my background, 
it was natural for me to express them as a linear program.  
Once linear programming had made an appearance, the 
repeated successes of the method led me to rely on it more and more.
In the end, approximately $10^5$ linear programs appear in the solution,
each involving some $200$ variables and around $2000$ constraints.
This is a inconsequential task for modern algorithms.  

The consequential part is to organize the output of this many linear
programs into a comprehensible proof narrative.  How do I convince you
the reader that this vast collection of linear programs constitutes
the proof of a theorem?  What is the precise statement of that
theorem, expressed in a way that does not refer to $10^5$ separate
problems? There is an entire chapter of this book devoted to these
questions.  This introductory essay gives enough background to carry
us through to that chapter.


\subsection{preliminaries}

There is a vast mathematical literature describing algorithms to
solve the constrained optimization problem of finding a column vector
$x\in\ring{R}^n$ that maximizes the objective function $ x$
\begin{equation}\label{eqn:lp1}
\max_{x}  c x
\end{equation}
where $c\in\ring{R}^n$ is a fixed row vector, subject to a system of
linear inequalities,
\begin{equation}\label{eqn:lp2}
A x\le b
\end{equation} 
for some matrix $A$ and vector $b\in \ring{R}^m$.  (An inequality $A
x\le b$ of vectors means that each component of vectors satisfy the
inequality.)  Such a maximization problem is called a {\it linear
  program}, an unusual name for what it is, not a program in the sense
of computer code, rather a program in the sense of military logistics,
growing out of G. Dantzig's war experience at the Pentagon
\cite{Dan91}.  G. Dantzig proposed the simplex method to solve such
problems in 1947.  The system of constraints $A x \le b$ defines a
polyhedron, the vector $c$ gives a preferred direction in space, and
the simplex method walks along edges of the polyhedron, from extreme points to
extreme point, until reaching a extreme point where no further progress can be made.
The simplex method has been named one of the top ten algorithms of the
twentieth century \cite{Cip00}.

Later in 1947, within minutes of first hearing about
linear programming, von Neumann introduced linear programming
duality, as an outgrowth of his theory of games.
In 1979, L. Khachiyan found a way to solve linear programs in
polynomial time \cite{Kha79}.  Still later, 
N. Karmarkar found a polynomial
time algorithm of practical value \cite{Kar84}.  
M. Wright gives a recent survey
of interior-point algorithms (algorithms that search for the 
optimal solution by moving through the interior of the polyhedron
rather than following the simplex method's strategy of searching
along the boundary of the polyhedron) in \cite{Wri05}. 
M. Todd also gives an excellent survey of algorithms \cite{Tod02}.
These algorithms have been implemented in various
software packages.
Because of this extensive and well-developed literature on the
subject of linear programming, we will take it as given that we
can solve large-scale linear programs.


The basic linear program admits many variations.  We can minimize
the objective function
\[x \mapsto c x\] 
rather than maximize it.
Any linear equation
$a x = b$ can be written as a system of inequalities
\[a  x \le b \text{ and } -a x \le -b.\]
This allows us to reduce constrained linear optimization with
equality constraints to a constrained linear optimization with
inequality constraints.

\subsection{duality}

A linear program is feasible if there exists an $x$ satisfying
all the constraints $A x \le b$.  A linear program is bounded
if the set $c x$ is bounded from above for all $x$ satisfying the
constraints.  

Given the linear program of Equations~\ref{eqn:lp1},\ref{eqn:lp2}
defined from $A,b,c$, there is another linear program defined by
the same data, but in dual form
\begin{equation}
\min_y {y b}
\end{equation}
such that
\begin{equation}
y A = c \text{ and } y \ge 0,
\end{equation}
where $y\in\ring{R}^m$ is a row vector.
To distinguish the two linear programs, the one given by
Equations~\ref{eqn:lp1} and \ref{eqn:lp2} is called the primal.
If $x$ is any feasible solution to the primal and  if
$y$ is any feasible solution to the dual (meaning that they satisfies
the constraints but are not necessarily optimal), then by
the inequality constraints, we have trivially that
$$y b \ge y A x = c x.$$
That is, any feasible solution $y$ to the dual linear program gives an
upper bound to the primal.  If the left-hand side is minimized
for $y=y^*$ and the right-hand side is maximized when $x=x^*$, then
$$y^* b \ge c x^*.$$
The linear programming duality theorem asserts that if the primal
is feasible and bounded, then the dual is also feasible and bounded,
and moreover $y^* b = c x^*$.  In summary, we can bound
the primal with any feasible solution to the dual, and find
the optimal bound on the primal by minimizing the dual.

Linear programming duality produces certificates of a bound
on the primal.  Suppose we treat a linear programming package as
an untrusted 
black box without any knowledge of its internal algorithms.
To handle round-off errors, we now also assume that we have
a priori bounds $|x_i| \le m$ on the variables $x_i$.
The untrusted package produces a vector $y$, which it claims
is (approximately) a feasible solution to the dual problem.  
First, replacing
any negative coefficients in $y$ with $0$, we may assume that
it satisfies the constraint $y\ge 0$.  The quantity $\delta = c - y A$
should be exactly zero for a feasible solution to the dual program, but
because of round-off errors, it may be  off.  
Then for any feasible $x$ to the primal problem
$$
c x = (\delta + y A)  x \le \delta  x + y b.
$$
Using the a priori bounds on $x$, we get $\delta x\le \sum 
m|\delta_i|=D$.  An upper bound on the primal is the value
$D + y b$, which is computed without any knowledge of the
algorithms used by the software package.  By the duality theorem
for linear programming, there exists a certificate for which this
computed value is exactly the maximum of the primal.

In the packing problem, in practice we have a priori bounds
on the variables $x$, and this method works extremely well.  Note
that the a priori bound $m$ is allowed to be an extremely crude estimate,
because in producing the estimate $D$, it is multiplied by $\delta$,
which typically has the magnitude of a machine epsilon.

\subsection{infeasibility}


In practice, many of the primal linear programs that appear
in the packing problem are infeasible.  This requires
some small adjustments to the previous discussion.   In the
linear programs that we encounter in this book, it is not
necessary to determine the exact upper bound of a the linear program.
We have an explicit constant $K$, and we wish to prove that
the maximum of the objective function is less than $K$.
Rather
than consider two separate
types of problems, feasible and infeasible, we recast all the linear
programs in this book in terms of infeasibility.
When the maximum of $c x$ is less than $K$, by
adding the constraint $c x\ge K$ to the system of constraints
$A x\le b$,  we get an infeasible linear program.
This is the vanishing box trick:  the counterexamples to a nonlinear
problem are constrained to lie inside an infeasible system of linear
inequalities.


We now drop the objective function, and work
with a system of inequalities  of the form
\begin{eqnarray}
\label{eqn:lpsys}
A x &\le& c\\
a \le x    &\le& b,
\end{eqnarray}
with explicitly given lower and upper bounds $(a\le x\le b)$ on
the variables $x$.  The problem
is to verify that the system is infeasible; that is, there does
not exist an $x$ satisfying the system of inequalities.  By
translating dual linear program certificates into this new context
we are led to the following definition.

\begin{definition}[certificate~of~infeasibility]
A certificate of infeasibility for the system \ref{eqn:lpsys},
is a pair $(u,v)$ satisfying
\begin{eqnarray*}
0&\le& u\\
0&\le& v\\
0&\le& u A + v\\
0& >& u(c-Aa) + v(b-a).
\end{eqnarray*}
\end{definition}

\begin{example}
If there is an index $i$ for which $b_i < a_i$, then the system is
clearly infeasible.  In this case, we have an obvious certificate
of infeasibility given by $(u,v)=(0,e_i)$, where $e_i$ is the
standard basis vector at index $i$.
\end{example}

\begin{lemma}
If a certificate of infeasibility $(u,v)$ exists for the system
\ref{eqn:lpsys}, then the system is infeasible.
\end{lemma}

\begin{proof}
Suppose for a contradiction that $x$ is a feasible solution.
Then
\begin{eqnarray*}
0 &\le& (u A + v)(x-a) \\
&=& [u (c- A a) + v (b- a)] - [u (c - A x)] - [v (b - x)]\\
&<& 0.\qedhere
\end{eqnarray*}
\end{proof}


As in the case of dual linear program certificates, 
to prove a system infeasible, it is not necessary to know how a
certificate of infeasibility is produced.  It can be produced 
by an untrusted computer algorithm.  All that is needed
to be known is that it is a certificate.

If an approximation $(u',v')$ to a certificate of infeasibility is
produced (say by computer), it can often be adjusted to give a
true certificate.  Given an approximation $(u',v')$, define
$(u,v)$ by
\begin{eqnarray*}
u &=& \max(0,u')\\
v &=& \max(0,v',-u A).
\end{eqnarray*}
If $(u,v)$ satisfies
$$u( c-A a) + v(b-a) <0,$$
then it is a certificate of infeasibility.

The theory of duality for linear programming insures that if a
system is infeasible, then a certificate of infeasibility exists.
Since the equations defining a certificate of infeasibility are
linear in $u$ and $v$, linear programming algorithms may be used
to produce certificates:
\[
\min_{u,v} u (c-A a) + v(b-a)
\]
such that 
\begin{eqnarray*}
0 &\le& u\\
0 &\le& v \\
0& \le& u A + v\\
\end{eqnarray*}
This has a feasible solution $u=v=0$.  If we add the constraint
that the objective function is at least $-1$, then there exists a
bounded feasible solution.


\subsection{linear relaxation}

Suppose that we have a continuous
nonlinear function $f$ that we wish to maximize
over a compact domain $X\subset \ring{R}^n$.    A linear relaxation
of the domain is a polyhedron defined by constraints $A x \le b$
such that the polyhedron contains the domain $X$.  That is,
$x\in X$ implies $A x \le b$.    A linear relaxation of the objective
function $f$ is a linear function $x\mapsto c x$ such that
$f(x) \le c x$ for all $x\in X$.  It then follows trivially
that the maximum of $f$ over $X$ is at most the value of 
the linear program that maximizes $c x$ such that $A x \le b$.

(A popular article about the proof of the packing problem
illustrated the method by depicting the nonlinear function $f$
as the rolling hills in the countryside, cows grazing in the background.  
A helicopter,
carrying a large roof, was slowing
descending over the hilltops, bounding the height of a hilltop
with the piecewise linear roof.)

Relaxation discards information about the nonlinear behavior of the
optimization problem.  It should be expected that the bounds
obtained by this method will often be crude.  However, it has the
definite advantage that linear programs can be solved efficiently,
while general nonlinear programming problems cannot.

For the problem to be a true linear relaxation, there must be rigorous
proofs that each $x\in X$ satisfies each linear constraint $A_i x\le
b_i$.  This means that the maximum of $A_i x$ over $X$ must be at most
$b_i$.  This again, is a nonlinear optimization problem on the domain
$X$ with objective function $A_i x$.  For an arbitrary constraint $A_i
x\le b$, and complicated domain $X$, there is no reason to suspect
that this is any easier to solve than the original optimization
problem.  Thus, we are in danger of replacing one intractible problem
with another.

The packing problem has a special structure that avoids this
problem.  Even though the nonlinear optimization problem involves
around 150 variables, 
this special structure allows all the nonlinearities to 
be confined to small dimensions (around 6 variables).  In other words,
the packing problem can be
described as a coupled system of small nonlinear subsystems,
and all the coupling comes through linear systems of equations. 
We give a toy model of this in a moment to describe how this works.

This special structure is one of the key points of the entire proof.
If you want to know why this hard nonlinear optimization problem has
been solved, but others have not, you now know the fundamental reason.
By partitioning the linear relaxation of the domain $X$ according
to the small nonlinear subsystems, we can arrange for each inequality
$A_i x\le b_i$ to be sparsely populated, and for each relaxation
constraint to be a statement about a nonlinear inequality on a domain
of low dimension.  This is a tractible problem:
we  summon interval arithmetic to prove these
low dimensional inequalities by computer.  In this way we are able
to give a rigorous proof that we have a true linear relaxation of a
difficult nonlinear optimization problem.  In practice, the linear
relaxation  bounds  are good enough to solve the packing
problem.







\section{Nonlinear Optimization}

\section{Graph Generation}

\section{List of Computer Calculations}

The following table gives a summary of the proofs in the book where
computer calculations have been used.  The table lists the lemma
number, gives a tracking number, indicates whether the calculation
involves nonlinear optimization (NL), linear programming (LP), or
graph generation (GR).  The table also gives a brief description of
the nature of the calculation.  The tracking number can be used to
search the internet for further details of the computer calculation.

% Chqpter: Packing

\calcentry{Lemma~\ref{lemma:MI}}{XHVKJFS}{NL}{This is Marchal's
  inequality for $k$-cells. It separates into cases according to
  $k\in\{0,1,2,3,4\}$.}

\calcentry{Lemma~\ref{lemma:cluster}}{SQMANBP}{NL}{This is the cell
  cluster estimate for clusters.  It separates into cases according to
  the number of blades.}

\calcentry{%Lemma~\ref{lemma:13-14},~
  Inequality~\eqn{eqn:disks}}{8550443271}{NL}{The inequality asserts
  the disjointness of disks on the unit sphere of variable radius
  $g(h)$.}

\calcentry{Inequality~\eqn{eqn:alin}}{7991525482}{NL}{This is a linear
  lower approximation to the area of a regular spherical $n$-gon.}

\calcentry{Inequality~\eqn{eqn:alin2}}{8540377696}{NL}{This is a
  linear lower approximation to the area of a regular spherical
  $n$-gon.}

% Chapter: Local Fan:

\calcentry{Inequality~\eqn{eqn:g''}}{2065952723}{NL}{This is a
  calculation of the sign of a second derivative to show that the
  function $\tau$ does not have a local minimum as a function of the
  edge lengths.  It initially appears to depend on six variables, but
  the dependence on three of the variables is linear and will be
  extremal at the endpoints.}

\calcentry{Inequality~\eqn{eqn:arc''}}{2158872499}{NL}{This is a
  calculation of the sign of a second derivative of $\arc$.  There are
  two cases.  It is an optimization problem in two variables.}

\calcentry{Lemma~\ref{lemma:triangle-free}}{MPWFVCK}{NL}{The main
  estimate holds for triangles.  There are four cases depending on the
  cardinality of $S$.  It is an optimization problem in three
  variables.}

\calcentry{Lemma~\ref{lemma:flat-exists}}{2986512815}{NL}{The function
  $\tau$ does not have a local minimum along the indicated curve.  It
  is an optimization problem in four variables.}

\calcentry{Lemma~\ref{lemma:quadrilateral-free}}{FVPVRZP}{NL}{The main
  estimate holds for quadrilaterals.  There are two cases according to
  whether there is a flat node.  When there is no flat node, it is an
  optimization in five variables.  When there is a flat node, there
  are further subcases $0,1,2$ according to the cardinality of $S$.
  Each subcase is an optimization problem in four variables.}

\calcentry{Lemma~\ref{lemma:pentagon-free}}{DAKGBMR}{NL}{The main
  estimate holds for pentagons.  There are two flat nodes, giving two
  cases according to whether the flat nodes are adjacent.  Each case
  has two two subcases $0,1$ according to the cardinality of $S$.
  Each subcase is an optimization problem in as many as five
  variables.}

\calcentry{Lemma~\ref{lemma:hexagon-free}}{INRCFAH}{NL}{The main estimate
  holds for pentagons.  There are several cases accordin to the
  configuration of flat nodes.  Depending on the case, there may be as
  many as seven variables.}




\calcentry{Lemma~\ref{lemma:face-size}}{UTIICTO}{NL}{A linear lower
  approximation to the function $\arc$ is computed.}

\calcentry{Lemma~\ref{lemma:D-local}}{XRULQNO}{NL}{This is the main cell inequality that arises in the
proof of the strong dodecahedral conjecture.  It breaks into cases according to the type of cell
$k=2,3,4$.}


%\begin{tabular}{llll}
%Lemma~\ref{lemma:face-size}&VEXKBSL&NL&A linear lower approximation to the function $\arc$ is computed.\\
%\end{tabular}


