% File started Jan 24, 2010.
%
\chapter{Numerical Methods}

Computers have been used at several significant places in the book.     The calculations
can be sorted into three general categories:  nonlinear
optimization, linear programming, and the combinatorics of hypermaps. This
appendix goes into further detail about these general categories.  


\section{Nonlinear Optimization}

\subsection{Interval Analysis}%DCG 8.3, p75
\label{sec:bounds-simplex}

Interval analysis is a method to obtain trustworthy computations over
the real numbers from a computer.  This subsection gives a basic
introduction to this method.  Interval arithmetic can trace its
origins to the method of deliberate error.  This is an ancient method
of navigation with imperfect instruments.  When William the Conqueror
crossed the English Channel in 1066, he deliberately steered to the
north of Hastings.

\begin{quote}
  % ``There is no direct evidence of how a twelfth-century pilot found
  % his way across the English Channel $\ldots$
  ``Any pilot even now who had to make that crossing without a chart
  or compass, as William's pilots did, would use the ancient method of
  Deliberate Error: he would not steer directly towards his objective
  but to one side of it, so that when he saw the coast he would know
  which way to turn.'' \cite[p81]{How81}
  % 1066: The Year of the Conquest, Penguin paperback edition
  % 1981. page 148.
\end{quote}

Deliberate error implements ``better safe than sorry.''
%To find an address on a one-way street,  a  driver enters the
%street too early, to point in the right direction.  
When searching for a familiar landmark
on a two-way street, it is more efficient  
to start the search safely to one side of the landmark, 
rather than search in ever
expanding zigzags in both directions.  
%Deliberate error 
%is the idea of trapping a target inside a large enough net, in the
%way that
%Khachiyan traps the optimal solutions of a linear program inside
%an ellipsoid.


The method of deliberate error cushions the adverse effects of
imperfect technology.  The method does not aim to minimize the
imperfections.  It aims to reliably contain the error.
% The method of deliberate error seeks not to minimize the error of
% imperfect technolo


Let $F$ be the linearly ordered set of machine-representable floating point numbers.
Assume that $F$ contains two special symbols $\pm\infty$, representing a floating-point
number larger than all real numbers and another that is smaller than all real numbers.   
 Because of these
two special symbols, the floating point floor function $x\to\floor{x}_F\in F$
and ceiling function $x\mapsto\ceil{x}_F\in F$, with domain
$\ring{R}$ can be defined.  In interval arithmetic, each real number $x$
is represented by a pair of floating-point numbers:
$a=\floor{x}_F$ and $b=\ceil{x}_F$. The real number $x$ lies in the closed interval 
$\leftclosed a,b\rightclosed$.

On most modern processors, the rounding mode can be set to directed
rounding.  For example, when the rounding mode is set
upward, the computer approximation of any basic arithmetic operation $\diamond$
(addition, subtraction, multiplication, or division)
on two floating
point numbers $x$ and $y$ is 
 $\ceil{x\diamond y}_F$.  That is, the floating-point operation
is the smallest element of $F$ that is no greater than the actual value $x\diamond y$.

As exact arithmetic operations are performed on real numbers, 
the computer follows along with corresponding intervals $\leftclosed a,b\rightclosed$ 
that contain the result.  As the computation progresses, 
intervals increase in width as needed so that
the real number remains sandwiched between two floating point numbers in $F$.
The net result is an interval that reliably contains the result of a real-number
calculation.

Interval arithmetic, like the method of deliberate error, does not
seek to eliminate the sources of floating point round off error.
Rather it controls it through a strict scientific standard.


A number of proofs in pure and applied mathematics have been based on
interval analysis.  W. Tucker implemented a rigorous ODE solver with
interval arithmetic and used it to prove that the Lorenz equations
have a strange attractor \cite{Tuc02}. The existence of strange
attractors is problem 14 on Smale's list of 18 Centennial Problems
\cite{Sma98}.  Another prominent problem solved by interval methods is
the double bubble conjecture, a generalization of the isoperimetric
problem in three dimensional Euclidean space.  A sphere gives the
solution to the classical isoperimetric problem.  The work of J. Hass,
M. Hutchings, and R. Schlafly shows that the surface area minimizing
way to enclose two regions of equal volume is the double bubble, which
consists of two partial spheres, separated by a flat circular disk
\cite{HHS95}.

Interval arithmetic has also yielded a number of new results on the
problem of packing circles in a square. M. Cs. Mark\'ot and T. Csendes
have obtained optimality proofs for packings of $28$, $29$, and $30$
circles in a square.  See Figure~\ref{fig:optimal-circles}.  This is
an area of active research. See, for example, \cite{Sza07} and
\cite{Mark07}.


\subsection{Bernstein polynomials}

One method of proving polynomial inequalities is based on Bernstein polynomials.

\subsection{nonpolynomial optimization}

\section{Linear Programming}

\subsection{primal program}

Starting with Dantzig's famous simplex algorithm in 1947, 
a vast mathematical literature describes algorithms to
solve the constrained optimization problem of finding a column vector
$x\in\ring{R}^n$ that maximizes the objective function $ x$
\begin{equation}\label{eqn:lp1}
\max_{x}  c x
\end{equation}
where $c\in\ring{R}^n$ is a fixed row vector, subject to a system of
linear inequalities,
\begin{equation}\label{eqn:lp2}
A x\le b
\end{equation} 
for some matrix $A$ and vector $b\in \ring{R}^m$.  (An inequality $A
x\le b$ of vectors means that each component of the vectors satisfies the
inequality.)  Such a maximization problem is called a \newterm{linear
  program}.  Surveys of algorithms appear in \cite{Wri05} and
\cite{Tod02}.

%, an unusual name for what it is, not a program in the sense
%of computer code, rather a program in the sense of military logistics,
%growing out of G. Dantzig's war experience at the Pentagon
%\cite{Dan91}.  G. Dantzig proposed the simplex method to solve such
%problems in 1947.  The system of constraints $A x \le b$ defines a
%polyhedron, the vector $c$ gives a preferred direction in space, and
%the simplex method walks along edges of the polyhedron, from extreme points to
%extreme point, until reaching a extreme point where no further progress can be made.
%The simplex method has been named one of the top ten algorithms of the
%twentieth century \cite{Cip00}.
%

A linear program is \newterm{feasible} if there exists an $x$ satisfying
all the constraints $A x \le b$.  A linear program is \newterm{bounded}
if the set 
\[
\{c x \mid A x \le b\}
\]
is bounded from above.

\subsection{duality}


Given the linear program of Equations~\ref{eqn:lp1},\ref{eqn:lp2}
defined from $A,b,c$, there is another linear program defined by
the same data, but in \newterm{dual} form
\begin{equation}
\min_y {y b}
\end{equation}
such that
\begin{equation}
y A = c \text{ and } y \ge 0,
\end{equation}
where $y\in\ring{R}^m$ is a row vector.
To distinguish the two linear programs, the one given by
Equations~\ref{eqn:lp1} and \ref{eqn:lp2} is called the \newterm{primal}.
If $x$ is any \newterm{feasible solution} to the primal and  if
$y$ is any feasible solution to the dual (meaning that they satisfies
the constraints but are not necessarily optimal), then by
the inequality constraints, we have trivially that
$$y b \ge y A x = c x.$$
That is, any feasible solution $y$ to the dual linear program gives an
upper bound to the primal, and a feasible solution $x$ to the primal linear program
gives a lower bound to the dual.  If the optimal dual solution is
 $y=y^*$ and the optimal primal solution is $x=x^*$, then 
$$y^* b \ge c x^*.$$
The \newterm{linear programming duality theorem} asserts that if the primal
is feasible and bounded, then the dual is also feasible and bounded,
and moreover $y^* b = c x^*$.  In summary, we can bound
the primal with any feasible solution to the dual, and find
the maximum of the primal by minimizing the dual.

%Linear programming duality produces certificates of a bound
%on the primal.  Suppose we treat a linear programming package as
%an untrusted 
%black box without any knowledge of its internal algorithms.
%To handle round-off errors, we now also assume that we have
%a priori bounds $|x_i| \le m$ on the variables $x_i$.
%The untrusted package produces a vector $y$, which it claims
%is (approximately) a feasible solution to the dual problem.  
%First, replacing
%any negative coefficients in $y$ with $0$, we may assume that
%it satisfies the constraint $y\ge 0$.  The quantity $\delta = c - y A$
%should be exactly zero for a feasible solution to the dual program, but
%because of round-off errors, it may be  off.  
%Then for any feasible $x$ to the primal problem
%$$
%c x = (\delta + y A)  x \le \delta  x + y b.
%$$
%Using the a priori bounds on $x$, we get $\delta x\le \sum 
%m|\delta_i|=D$.  An upper bound on the primal is the value
%$D + y b$, which is computed without any knowledge of the
%algorithms used by the software package.  By the duality theorem
%for linear programming, there exists a certificate for which this
%computed value is exactly the maximum of the primal.
%
%In the packing problem, in practice we have a priori bounds
%on the variables $x$, and this method works extremely well.  Note
%that the a priori bound $m$ is allowed to be an extremely crude estimate,
%because in producing the estimate $D$, it is multiplied by $\delta$,
%which typically has the magnitude of a machine epsilon.
%


\subsection{infeasibility}

The problem we consider in this subsection is to verify that the
maximum of $c x$ is less than a given constant $M$, when subject to
\eqn{eqn:lp2} and to given bounds on the entries of $x$.
That is, we wish to verify that the system of inequalities
\begin{equation}\label{eqn:empty}
A x \le b,\quad \ell \le x\le u,\quad c x \ge M
\end{equation}
has no solutions in $x$. Any vector $y$ with the properties
\begin{equation}\label{eqn:y}
  y A = c,\quad y\ge 0,\quad y b < M
\end{equation}
is a \newterm{certificate} that the system \eqn{eqn:empty} is infeasible.  Indeed,
if $y$ has these properties, then for any $x$ satisfying
$A x \le b$, it follows that
\begin{equation}\label{eqn:cxM}
  c x = y A x \le y b < M
\end{equation}
as desired.


\subsection{numerical solution}


The theory of duality for linear programming can be used to show that if a
system is infeasible, then a certificate of infeasibility exists.  A certificate
may be found by solving the given linear programming problem \eqn{eqn:ci}.
It is not necessary to know how a
certificate of infeasibility is produced.  It can be produced 
by an untrusted algorithm.  

The concept of certificate can be
adapted to a context that allows for small  errors, produced by
numerical approximations on a computer.
Because of inexact computer arithmetic, the
equality in \eqn{eqn:y} will only be approximately correct. The imprecision in the
dual certificate $y$ can be readily eliminated as follows. If $u$ is any
vector, let $u^+$ be the vector obtained by replacing the negative
entries of $u$ with $0$, and let $u^-$ be the vector obtained by
replacing the positive entries of $u$ with $0$.  In the
following lemma, $\epsilon_1$ and $\epsilon_2$ are small error terms
that result from machine approximation. By including them in the
bounds on $c x$, a rigorous bound can be recovered.


\begin{lemma}  Suppose that the real-valued vectors and matrices
$A,A_1,A_2$, $c,c_1,c_2$, $x,b,\ell,u$, $y$ satisfy the following
relations
  $$
  A x\le b, \quad A_1 \le A \le A_2,
  \quad c_1 \le c \le c_2,\quad \ell\le x\le u,\quad
  0\le y.
  $$
Define residuals
  $$
   \epsilon_1 = c_1 - y A_2,\quad \epsilon_2 = c_2  - y A_1.
  $$
If
$$
y b + \epsilon_2^+ u^+ + \epsilon_1^+ u^- + \epsilon_2^- l^+ + \epsilon_1^- l^- < M,
$$
then $c x < M$.
\end{lemma}

\begin{proof} S. Obua has given a formal proof of this lemma in the
Isabelle/HOL system \cite[3.7.2]{Obua:2008:Thesis}. In fact, the proof
follows from a simple embellishment of Inequality~\ref{eqn:cxM}:
$$c x -y b\le c_2 x^+ + c_1 x^- -y A x\le (\epsilon_2 x^+ + \epsilon_1 x^-)
  + y (A_1-A) x^+ + y (A_2 - A) x^- \le \cdots.$$
\end{proof}

The numerical data $A_1,A_2,c_1,c_2,\ell,u,y,b$ are all explicitly given,
so that the method yields explicit bounds.
It is not necessary to trust the software
that produces the certificate $y$, because
all of the assumptions of the lemma can be checked directly.
The conditions $A x \le b$ and $\ell\le x\le u$ hold by the assumption
of the feasibility of $x$; the condition $0\le y$ is produced by replacing the
negative entries of $y$ with $0$; the other assumptions of the lemma are
checked by simple matrix operations by computer.  Interval arithmetic 
(or exact rational arithmetic) guarantees
the reliability of these matrix
operations.

In practice, it is necessary for us to solve hundreds of thousands of
linear program feasibility problems\eqn{eqn:empty}, each involving
hundreds of variables and thousands of linear constraints.

\subsection{linear relaxation}

\newterm{Nonlinear optimization} searches for the maximum of a
continuous function $f$ over a compact domain $X\subset \ring{R}^n$.
A \newterm{linear relaxation} of the domain is a polyhedron defined by
constraints $A x \le b$ such that the polyhedron contains the domain
$X$.  That is, $x\in X$ implies $A x \le b$.  A \newterm{linear
  relaxation} of the objective function $f$ is a linear function
$x\mapsto c x$ such that $f(x) \le c x$ for all $x\in X$.  It then
follows trivially that the maximum of $f$ over $X$ is at most the
value of the linear program that maximizes $c x$ such that $A x \le
b$.

Relaxation gives a mathematically sound linearization of a
nonlinear optimization problem.  It should be expected that the bounds
obtained by this method will often be crude.  Nevertheless, linear
programs can be solved efficiently, while general nonlinear
programming problems cannot.

\begin{remark}[popularization]
A popular article about the proof of the packing problem
illustrated the method by depicting the nonlinear function $f$
as the rolling hills in the countryside, cows grazing in the background.  
A helicopter,
carrying a large roof, was slowing
descending over the hilltops, bounding the height of a hilltop
with the piecewise linear roof.
\end{remark}

\begin{remark}[vanishing box trick]
Relaxation can also be seen as a {\it vanishing
box trick.}  Consider the set of counterexamples
\[
\{x\in X\mid f(x)\ge c x\}
\]
to a desired nonlinear inequality ($f(x)< c x$ on $X$).  At the outset, the set
of counterexamples to the problem has an unknown size and structure.
These counterexamples are all placed in a large box.  The width of the
box is measured and is found to be negative.  Therefore the box is
empty, it contains no counterexamples, and the nonlinear inequality is
proved.   It is remarkable that a method based
on such a simple idea is sufficiently powerful to prove many difficult nonlinear
inequalities.  In this intuitive picture, the box represents the polyhedron obtained
by linear relaxation.  The negative width represents the inequality
produced by the dual certificate $y$. 
\end{remark}

\subsection{linear assembly}

The nonlinear optimization problems that we solve in this book have the
structure of \newterm{linear assembly problems}.  These are problems whose
nonlinearities can confined to small dimensions.

The linear assembly problem asks for a proof of an upper bound
\begin{equation}\label{eqn:cxM}
c x < M
\end{equation}
for all $x=(x_1,\ldots,x_s)\in \ring{R}^{m_1}\times\cdots\times\ring{R}^{m_s}=\ring{R}^n$
subject to constraints
\begin{eqnarray}
A x &\le& b,\notag\\
%x &=& (x_1,x_2,\ldots,x_s),\\
x_i &\in& D_i\notag
\end{eqnarray}
for some matrices $A,b$, natural numbers $m_i$, and sets $D_i\subset \ring{R}^{m_i}$,
where $m_1+\cdots+m_s = n$.
The only nonlinearities are those appearing in the constraints $x_i\in D_i$.

An \newterm{assembly certificate} of the upper bound \eqn{eqn:cxM}
consists of the following data
\[
P_{i,j}\mid i=1,\ldots,s,~~j\in J_i,\quad\text{ and }\quad 
y_k\mid~~k=(j_1,\ldots,j_s),~~j_i\in J_i.
\]
where $P_{i,j}$ are polyhedra in $\ring{R}^{m_i}$ such that 
\begin{equation}\label{eqn:DP}
D_i \subset\cup_{j\in J_i} P_{i,j},
\end{equation} and $y_k$ are linear programming
dual certificates of the infeasibility of the linear programs
\[
c x \ge M,\quad A x \le b,\quad x_i\in P_{i,j_i}.
\]

An assembly certificate provides an immediate proof of the upper bound of the
linear assembly problem.  Indeed,  If $x$ satisfies the constraints of the linear
assembly problem, then $x_i\in D_i$ for all $i$.  Hence $x_i\in P_{i,j_i}$ for some
indices $j_i$.  The dual certificate $y_k$, $k=(j_1,\ldots)$, then certifies that
$c x < M$.

The nonlinear part of the certificate consists in the verification
of \eqn{eqn:DP}.  In practice, the dimensions $m_i$
may be small, making this nonlinear part computationally feasible,
even when $n=\sum m_i$ is large.  (In practice, we may have $m_i\le 6$
and $n > 100$.)

%\bigskip
%For the problem to be a true linear relaxation, there must be rigorous
%proofs that each $x\in X$ satisfies the linear constraints $A x\le b$.
%This again, is a nonlinear optimization problem on the domain $X$ with
%objective function $A_i x$. 
%
%The packing problem has a special structure that avoids this
%problem.  Even though the nonlinear optimization problem involves
%around 150 variables, 
%this special structure allows all the nonlinearities to 
%be confined to small dimensions (around 6 variables).  In other words,
%the packing problem can be
%described as a coupled system of small nonlinear subsystems,
%and all the coupling comes through linear systems of equations. 
%We give a toy model of this in a moment to describe how this works.
%
%This special structure is one of the key points of the entire proof.
%If you want to know why this hard nonlinear optimization problem has
%been solved, but others have not, you now know the fundamental reason.
%By partitioning the linear relaxation of the domain $X$ according
%to the small nonlinear subsystems, we can arrange for each inequality
%$A_i x\le b_i$ to be sparsely populated, and for each relaxation
%constraint to be a statement about a nonlinear inequality on a domain
%of low dimension.  This is a tractible problem:
%we  summon interval arithmetic to prove these
%low dimensional inequalities by computer.  In this way we are able
%to give a rigorous proof that we have a true linear relaxation of a
%difficult nonlinear optimization problem.  In practice, the linear
%relaxation  bounds  are good enough to solve the packing
%problem.
%
%
%\subsection{infeasibility}

%It may seem that such a strategy is too hopelessly naive to work.
%In fact, precisely this strategy is pursued, and it works beautifully.
%The act of placing a counterexample in a box is called linear 
%relaxation.  The counterexamples form an unknown nonlinear set.
%The box that contains them is defined by linear constraints.  
%Relaxation refers to a relaxation of constraints, so that the
%box contains the counterexamples -- it is not merely a linear
%approximation to the set of counterexamples.  The box needn't be
%rectangular.  Any polyhedron will do.  To determine that a box
%has negative width is to say that it gives an {\it infeasible} linear
%program.  With the strategy, there is no need to limit ourselves
%to a single box, we can use a finite collection of boxes that
%contain the set of counterexamples.  This leads to branch and bound
%methods.
%
%
%In this appendix, we will discuss these methods in further detail,
%including linear programming, linear relaxation,  infeasibility,
%and branch and bound methods.
%
%The introduction of linear programming methods to the packing
%problem was gradual and tranquil.  
%Many of the
%inequalities in the packing problem have an obvious linear form.
%As an engineering student
%at Stanford, I had studied
%linear and nonlinear optimization.
%Given my background, 
%it was natural for me to express them as a linear program.  
%Once linear programming had made an appearance, the 
%repeated successes of the method led me to rely on it more and more.
%In the end, approximately $10^5$ linear programs appear in the solution,
%each involving some $200$ variables and around $2000$ constraints.
%This is a inconsequential task for modern algorithms.  
%
%The consequential part is to organize the output of this many linear
%programs into a comprehensible proof narrative.  How do I convince you
%the reader that this vast collection of linear programs constitutes
%the proof of a theorem?  What is the precise statement of that
%theorem, expressed in a way that does not refer to $10^5$ separate
%problems? There is an entire chapter of this book devoted to these
%questions.  This introductory essay gives enough background to carry
%us through to that chapter.
%
%
%
%
%Later in 1947, within minutes of first hearing about
%linear programming, von Neumann introduced linear programming
%duality, as an outgrowth of his theory of games.
%In 1979, L. Khachiyan found a way to solve linear programs in
%polynomial time \cite{Kha79}.  Still later, 
%N. Karmarkar found a polynomial
%time algorithm of practical value \cite{Kar84}.  
%M. Wright gives a recent survey
%of interior-point algorithms (algorithms that search for the 
%optimal solution by moving through the interior of the polyhedron
%rather than following the simplex method's strategy of searching
%along the boundary of the polyhedron) in \cite{Wri05}. 
%M. Todd also gives an excellent survey of algorithms \cite{Tod02}.
%These algorithms have been implemented in various
%software packages.
%Because of this extensive and well-developed literature on the
%subject of linear programming, we will take it as given that we
%can solve large-scale linear programs.
%
%
%
%The basic linear program admits many variations.  We can minimize
%the objective function
%\[x \mapsto c x\] 
%rather than maximize it.
%Any linear equation
%$a x = b$ can be written as a system of inequalities
%\[a  x \le b \text{ and } -a x \le -b.\]
%This allows us to reduce constrained linear optimization with
%equality constraints to a constrained linear optimization with
%inequality constraints.
%
%
%
%\subsection{infeasibility}
%
%Consider a general system of linear inequalities
%\begin{alignat}{1}
%\label{eqn:lpsys}
%A x &\le c\notag\\
%a &\le x  \,\le b,
%\end{alignat}
%with given matrix coefficients $A,c$ and given 
%lower and upper bounds $(a\le x\le b)$ on a
%vector $x$ of unknowns.  
%
%
%
%
%%In practice, many of the primal linear programs that appear
%%in the packing problem are infeasible.  This requires
%%some small adjustments to the previous discussion.   In the
%%linear programs that we encounter in this book, it is not
%%necessary to determine the exact upper bound of a the linear program.
%%We have an explicit constant $K$, and we wish to prove that
%%the maximum of the objective function is less than $K$.
%%Rather
%%than consider two separate
%%types of problems, feasible and infeasible, we recast all the linear
%%programs in this book in terms of infeasibility.
%%When the maximum of $c x$ is less than $K$, by
%%adding the constraint $c x\ge K$ to the system of constraints
%%$A x\le b$,  we get an infeasible linear program.
%%This is the vanishing box trick:  the counterexamples to a nonlinear
%%problem are constrained to lie inside an infeasible system of linear
%%%inequalities.
%%
%%
%%We now drop the objective function, and work
%%with a system of inequalities  of the form
%%  By
%%translating dual linear program certificates into this new context
%%we are led to the following definition.
%%
%
%\begin{definition}[certificate~of~infeasibility]
%A \newterm{certificate of infeasibility} for the system \ref{eqn:lpsys},
%is a pair $(u,v)$ satisfying
%\begin{eqnarray*}
%0&\le& u\\
%0&\le& v\\
%0&\le& u A + v\\
%0& >& u(c-Aa) + v(b-a).
%\end{eqnarray*}
%\end{definition}
%
%\begin{example}
%If there is an index $i$ for which $b_i < a_i$, then the system is
%clearly infeasible.  In this case, we have an obvious certificate
%of infeasibility given by $(u,v)=(0,e_i)$, where $e_i$ is the
%standard basis vector at index $i$.
%\end{example}
%
%\begin{lemma}
%If a certificate of infeasibility $(u,v)$ exists for the system
%\ref{eqn:lpsys}, then the system is infeasible.
%\end{lemma}
%
%\begin{proof}
%Suppose for a contradiction that $x$ is a feasible solution.
%Then
%\begin{eqnarray*}
%0 &\le& (u A + v)(x-a) \\
%&=& [u (c- A a) + v (b- a)] - [u (c - A x)] - [v (b - x)]\\
%&<& 0.\qedhere
%\end{eqnarray*}
%\end{proof}
%
%%The set of certificates form a cone: if $(u,v)$ is a certificate and $t>0$,
%%then $(t u, t v)$ is also a certificate.  
%
%Since the equations defining a certificate of infeasibility are
%linear in $u$ and $v$, linear programming algorithms may be used
%to produce certificates:
%\begin{align}\label{eqn:ci}
%\min_{u,v} &\, u (c-A a) + v(b-a)
%\intertext{such that }
%0 &\le u\notag\\
%0 &\le v\notag\\
%0& \le u A + v\notag
%\end{align}
%Trivial manipulations transform this minimization problem into the standard
%format \eqn{eqn:lp1} \eqn{eqn:lp2}.
%The system of constraints has an obvious feasible solution $u=v=0$.
%If we add the constraint that the objective function is at least $-1$,
%then there exists a bounded feasible solution.  
%
%\begin{lemma}  Let $A_1,A_2,A$.
%\end{lemma}
%
%If an approximation $(u',v')$ to a certificate of infeasibility is
%produced (say by computer), it can often be adjusted to give a
%true certificate.  Given an approximation $(u',v')$, define
%$(u,v)$ by
%\begin{eqnarray*}
%u &=& \max(0,u')\\
%v &=& \max(0,v',-u A).
%\end{eqnarray*}
%If $(u,v)$ satisfies
%$$u( c-A a) + v(b-a) <0,$$
%then it is a certificate of infeasibility.
%







\section{Hypermap Generation}
