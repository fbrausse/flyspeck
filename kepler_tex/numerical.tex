% File started Jan 24, 2010.
%
\chapter{Numerical Methods}

Computers have been used at several significant places in the book.
The calculations can be sorted into three general categories:
nonlinear optimization, linear programming, and the combinatorics of
hypermaps. This appendix goes into further detail about these general
categories.

According to the estimates in \cite{HHMNOZ}, the original proof of
the Kepler conjecture required well over 187,000 lines of custom
computer code (including an unknown amount of duplicated code).  By
contrast, the computer calculations for this version of the proof can
be implemented in significantly fewer than $10,000$ source lines of code.
Moreover, the function and correctness of the new computer code is
much more transparent than in the original code.  This reduction in
code complexity is one of the most significant improvements of this
proof over the earlier one.  The computer code is freely available
from a public repository~\cite{website:FlyspeckProject}.

To solve a difficult nonlinear optimization problem, we use linear
programming to factor out the linear part.  On what remains, we use
Bernstein polynomials to factor out the polynomial part.  All that
remains are a few thoroughly understood nonlinear functions, such as
the square root, arctangent, and division.  By this approach, the difficulties
disappear, and the problem is solved.


\section{Nonlinear Optimization}

\subsection{interval analysis}%DCG 8.3, p75
\label{sec:bounds-simplex}

Interval analysis is a method of obtaining trustworthy computations
over the real numbers from a computer.  This subsection gives a basic
introduction to this method.  Details may be found in standard
texts~\cite{Kearfott:1996:Interval},
\cite{Moore:1966:IntervalAnalysis},~\cite{Moore:2009:IntroIntAnalysis}.
% Kearfott, Moore, etc.
Interval arithmetic  traces its
origins to the method of deliberate error,  an ancient method
of navigation with imperfect instruments.  When William the Conqueror
crossed the English Channel in 1066, he deliberately steered to the
north of Hastings.

\begin{quote}
  % ``There is no direct evidence of how a twelfth-century pilot found
  % his way across the English Channel $\ldots$
  ``Any pilot even now who had to make that crossing without a chart
  or compass, as William's pilots did, would use the ancient method of
  Deliberate Error: he would not steer directly towards his objective
  but to one side of it, so that when he saw the coast he would know
  which way to turn.'' \cite[p81]{How81}
  % 1066: The Year of the Conquest, Penguin paperback edition
  % 1981. page 148.
\end{quote}

Deliberate error implements the principle ``better safe than sorry.''
%To find an address on a one-way street,  a  driver enters the
%street too early, to point in the right direction.  
When searching for a familiar landmark on a two-way street, it is more
efficient to start the search safely to one side of the landmark,
rather than search in ever expanding zigzags in both directions.  When
Archimedes approximated the circumference of a circle with a inscribed
polygon to one side and a circumscribed polygon to the other, he was
using interval analysis in essence.


The method of deliberate error cushions the adverse effects of
imperfect technology.  The method does not aim to minimize the
imperfections, but works directly with a faulty chart and compass.  It
aims to reliably contain the error to one side.
% The method of deliberate error seeks not to minimize the error of
% imperfect technolo


Let $F$ be the linearly ordered set of machine-representable floating
point numbers.  Even though we call $F$ a set of floating-point
numbers, our presentation will be general enough to allow various
implementations of the set $F$.  For example, $F$ may be a set of
double precision floating point numbers, exact rational numbers, or
constructible real numbers.  Assume that $F$ contains two special
symbols $\pm\infty$, representing a floating-point number larger than
all real numbers and another that is smaller than all real numbers.

The floating point floor function $x\to\floor{x}_F\in
F$ and ceiling function $x\mapsto\ceil{x}_F\in F$, with domain
$\ring{R}$ can be defined.  In interval arithmetic, each real number
$x$ is represented by a pair of floating-point numbers:
$a=\floor{x}_F$ and $b=\ceil{x}_F$. The real number $x$ lies in the
closed interval $\leftclosed a,b\rightclosed$.  The floor and ceiling
functions are defined in such as way that this is the smallest interval with
endpoints in $F$ that contains $x$.

On most modern processors, the rounding mode can be set to directed
rounding.  For example, when the rounding mode is set upward, the
computer approximation of any basic arithmetic operation $\diamond$
(addition, subtraction, multiplication, or division) on two floating
point numbers $x$ and $y$ is $\ceil{x\diamond y}_F$.  That is, the
floating-point operation is the smallest element of $F$ that is no
greater than the actual value $x\diamond y$.

As exact arithmetic operations are performed on real numbers, the
computer follows along with corresponding intervals $\leftclosed
a,b\rightclosed$ that contain the result.  As the computation
progresses, intervals increase in width as needed so that the real
number remains sandwiched between two floating point numbers in $F$, marking
the endpoints of the interval.
The net result is an interval that reliably contains the result of a
real-number calculation.

Interval arithmetic, like the method of deliberate error, does not
seek to eliminate the sources of floating point round off error.
Rather it controls it through a strict scientific standard.


A number of proofs in both pure and applied mathematics have been
based on interval analysis.  W. Tucker implemented a rigorous ODE
solver with interval arithmetic and used it to prove that the Lorenz
equations have a strange attractor \cite{Tuc02}. The existence of
strange attractors is problem 14 on Smale's list of 18 Centennial
Problems \cite{Sma98}.  Another prominent problem solved by interval
methods is the double bubble conjecture, a generalization of the
isoperimetric problem in three dimensional Euclidean space.  The
classical isoperimetric inequality states that the sphere has the
least surface area of any surface that encloses a given volume.
solution to the classical isoperimetric problem.  The work of J. Haas,
M. Hutchings, and R. Schlafly shows that the surface area minimizing
way to enclose two regions of equal volume is the double bubble, which
consists of two partial spheres, separated by a flat circular disk
\cite{HHS95}.

Interval arithmetic has also yielded a number of new results on the
problem of packing circles in a square. M. Cs. Mark\'ot and T. Csendes
have obtained optimality proofs for packings of $28$, $29$, and $30$
circles in a square.  See Figure~\ref{fig:optimal-circles}.  This is
an area of active research. See, for example, \cite{Sza07} and
\cite{Mark07}.


\subsection{Bernstein polynomials}

The \newterm{Bernstein polynomials} $B_{i,n}\in\ring{Z}[x]$ are defined as
\begin{equation}
B_{i,n}(x) = {n\choose i} x^i (1-x)^{n-i},\quad i=0,\ldots,n.
\end{equation}
The set $\{B_{i,n}\mid i=0,\ldots,n\}$ is a basis of the vector space
of polynomials $\ring{R}[x]_n$ of degree at most $n$.

The polynomials are manifestly nonnegative on the unit interval:
$B_{i,n}(x)\ge 0$, when $0\le x\le 1$.  The polynomials form a
partition of unity:
\[
\sum_{i=0}^n B_{i,n}(x) = \sum_{i=0}^n {n\choose i} x^i (1-x)^{n-i} = 
(x + (1-x))^n = 1.
\]

Any polynomial $p$ of degree at most $n$, 
when expressed in terms of the Bernstein basis:
\[
p(x) =  \sum_{i=0}^n a_i B_{i,n}(x),
\]
can be easily bounded from below and above:
\[
\min_i a_i \le p(x) \le \max_i a_i,\quad\text{ when } 0 \le x \le 1.
\]
Indeed, if $a = \min_i a_i$, then by nonnegativity and the partition
of unity properties
\[
a  =  \sum_{i=0}^n a B_{i,n}(x) \le \sum_{i=0}^n a_i B_{i,n}(x) = p(x).
\]
The upper bound is similar.

These bounds lead to a simple algorithm to prove a polynomial bound
$p(x) < M$ for $a\le x\le b$.  First, rescale the polynomial $p$ so
that the domain is $\leftclosed 0,1\rightclosed$:
\[
p_1(x) = p(a (1-x) + b x), \quad 0\le x \le 1.
\]
Next, express $p_1(x)$ as a linear combination $\sum a_i B_{i,n}(x)$.
If $\max_i a_i < M$, then the desired bound is proved and the algorithm
terminates.  Otherwise,
 partition the domain
\[
\leftclosed 0,1\rightclosed = \leftclosed  0,\frac{1}{2}\rightclosed
\cup \leftclosed  \frac{1}{2},1\rightclosed,
\]
and then recursively apply the algorithm to prove $p_1(x)<M$ on each
subinterval.   (If ever $\min_i a_i > M$, then the desired inequality is false.)


\subsection{multivariate polynomials}

A multivariate polynomial $p(x_1,\ldots,x_n)\in \ring{R}[x_1,\ldots,x_k]$ that
has degree at most $n$ in each variable $x_i$ can be written as a linear
combination of the polynomials
\[
B_{\alpha,n}(x_1,\ldots,x_n)=B_{i_1,n}(x_1)\cdots B_{i_k,n}(x_k),\qquad
 i_1,\ldots,i_k\in \{0,\ldots,n\},
\]
In multi-index notation $\alpha=(i_1,\ldots,i_k)$, 
\[
p = \sum_{\alpha\mid \max{\alpha}\,\le\, n} a_\alpha B_{\alpha,n}.
\]
On the unit cube $[0,1]^k$, we have
\[
\min a_\alpha \le p(x_1,\ldots,x_k) \le \max a_\alpha.
\]
As with univariate polynomials, this gives an algorithm to prove
polynomial bounds $p(x_1,\ldots,x_k)<M$ on rectangular domains:
rescale so that the domain is the unit cube; check whether
\[
\max a_\alpha < M;
\]
if not, subdivide the rectangle and repeat recursively.



\subsection{nonpolynomial optimization}

The nonlinear inequalities that are required in this book are close to
being polynomial.  In fact, many  of the inequalities can be put into a
general form:
\begin{equation}\label{eqn:ABCD}
\sum_{i=0}^k A_i \atn(\sqrt{B_i},C_i) < D
\end{equation}
for some multivariate polynomials $A_i,B_i,C_i,D$.  There are a few
variations on this general form, but generally speaking, the only
nonpolynomial functions that are needed in this book are the square
root, division $(/)$, and the arctangent.  These
are functions of one or two variables.  Polynomial upper and lower
approximations are readily obtained.

For example, if $A_i\ge 0$, the inequality \eqn{eqn:ABCD} is a
consequence of two simpler inequalities: a polynomial inequality
\[
\sum_{i=0}^k A_i L_i(B_i,C_i) < D.
\]
and polynomial upper bounds to nonpolynomial functions in two variables:
\begin{equation}\label{eqn:sL}
\atn(\sqrt{x},y) < L_i(x,y),
\end{equation}
for all $(x,y)$ in the range of the polynomials $B_i,C_i$, for some
polynomials $L_i$.  The polynomial
inequality can be proved by Bernstein polynomial methods.  The
inequality \eqn{eqn:sL} can be proved by various methods.  After all,
the analytic properties of $\atn(\sqrt{x},y)$ are thoroughly
understood.

If two simpler inequalities of the given form cannot be produced over
the entire $k$-dimensional rectangular domain, one can recursively
cover the domain by smaller rectangles until two such inequalities can
be found on each smaller rectangle.

The material in this section is based on the work of R. Zumkeller, who
has developed Bernstein polynomial methods in the context of the
Flyspeck project~\cite{roland-thesis}, \cite{zumkeller-nonlinear}.


\section{Linear Programming}\label{sec:lp}

\subsection{primal program}

Starting in 1947 with Dantzig's famous simplex algorithm growing out
of his work on military logistics for the Pentagon~\cite{Dan91}, a
vast mathematical literature describes algorithms for solving the
constrained optimization problem of finding a column vector
$x\in\ring{R}^n$ that maximizes the objective function
\begin{equation}\label{eqn:lp1}
\max_{x}  c x
\end{equation}
where $c\in\ring{R}^n$ is a fixed row vector subject to a system of
linear inequalities; and
\begin{equation}\label{eqn:lp2}
A x\le b,
\end{equation} 
for some matrix $A$ and vector $b\in \ring{R}^m$.  (An inequality $A
x\le b$ of vectors means that each component of the vectors satisfies
the inequality.)  Such a maximization problem is called a
\newterm{linear program}.  Surveys of algorithms appear in
\cite{Wri05} and \cite{Tod02}.  

The system of constraints $A x \le b$ defines a
polyhedron, the vector $c$ gives a preferred direction in space, and
the simplex algorithm moves along edges of the polyhedron, from extreme
point to extreme point, until reaching a extreme point where no
further progress the direction $c$ can be made.  The algorithm has been named one
of the top ten algorithms of the twentieth century \cite{Cip00}.


A linear program is \newterm{feasible} if there exists an $x$ satisfying
all the constraints $A x \le b$.  A linear program is \newterm{bounded}
if the set 
\[
\{c x \mid A x \le b\}
\]
is bounded from above.

\subsection{duality}

Later in 1947, within minutes of first hearing of Dantzig's work, with
characteristic speed, von Neumann introduced linear programming
duality as an outgrowth of his theory of games.  Given the linear
program of Equations~\ref{eqn:lp1},\ref{eqn:lp2} defined from $A,b,c$,
there is another linear program defined by the same data, but in
\newterm{dual} form
\begin{equation}
\min_y {y b}
\end{equation}
such that
\begin{equation}
y A = c \text{ and } y \ge 0,
\end{equation}
where $y\in\ring{R}^m$ is a row vector.  To distinguish between the two linear
programs, that given by Equations~\ref{eqn:lp1} and \ref{eqn:lp2}
is called the \newterm{primal}.  If $x$ is any \newterm{feasible
  solution} to the primal and if $y$ is any feasible solution to the
dual (meaning that each satisfies the constraints, but neither is
necessarily optimal), then by the inequality constraints, we have
trivially that
\[y b \ge y A x = c x.\]
That is, any feasible solution $y$ to the dual linear program gives an
upper bound to the primal, and a feasible solution $x$ to the primal
linear program gives a lower bound to the dual.  If the optimal dual
solution is $y=y^*$ and the optimal primal solution is $x=x^*$, then
we have
\[y^* b \ge c x^*.\]
The \newterm{linear programming duality theorem} asserts that if the
primal is feasible and bounded, then the dual is also feasible and
bounded, and moreover $y^* b = c x^*$.  In summary, we can bound the
primal with any feasible solution to the dual and find the maximum of
the primal by minimizing the dual.

%Linear programming duality produces certificates of a bound
%on the primal.  Suppose we treat a linear programming package as
%an untrusted 
%black box without any knowledge of its internal algorithms.
%To handle round-off errors, we now also assume that we have
%a priori bounds $|x_i| \le m$ on the variables $x_i$.
%The untrusted package produces a vector $y$, which it claims
%is (approximately) a feasible solution to the dual problem.  
%First, replacing
%any negative coefficients in $y$ with $0$, we may assume that
%it satisfies the constraint $y\ge 0$.  The quantity $\delta = c - y A$
%should be exactly zero for a feasible solution to the dual program, but
%because of round-off errors, it may be  off.  
%Then for any feasible $x$ to the primal problem
%$$
%c x = (\delta + y A)  x \le \delta  x + y b.
%$$
%Using the a priori bounds on $x$, we get $\delta x\le \sum 
%m|\delta_i|=D$.  An upper bound on the primal is the value
%$D + y b$, which is computed without any knowledge of the
%algorithms used by the software package.  By the duality theorem
%for linear programming, there exists a certificate for which this
%computed value is exactly the maximum of the primal.
%
%In the packing problem, in practice we have a priori bounds
%on the variables $x$, and this method works extremely well.  Note
%that the a priori bound $m$ is allowed to be an extremely crude estimate,
%because in producing the estimate $D$, it is multiplied by $\delta$,
%which typically has the magnitude of a machine epsilon.
%


\subsection{infeasibility}

The problem we consider in this subsection is to verify that the
maximum of $c x$ is less than a given constant $M$, when subject to
\eqn{eqn:lp2} and to given bounds on the entries of $x$.
That is, we wish to verify that the system of inequalities
\begin{equation}\label{eqn:empty}
A x \le b,\quad \ell \le x\le u,\quad c x \ge M
\end{equation}
has no solutions in $x$. Any vector $y$ with the properties
\begin{equation}\label{eqn:y}
  y A = c,\quad y\ge 0,\quad y b < M
\end{equation}
is a \newterm{certificate} that the system \eqn{eqn:empty} is infeasible.  Indeed,
if $y$ has these properties, then for any $x$ satisfying
$A x \le b$, it follows that
\begin{equation}\label{eqn:cxM}
  c x = y A x \le y b < M
\end{equation}
as desired.


\subsection{numerical solution}


The theory of duality for linear programming can be used to show that if a
system is infeasible, then a certificate of infeasibility exists.  A certificate
may be found by solving the given linear programming problem \eqn{eqn:ci}.
It is not necessary to know how a
certificate of infeasibility is produced.  It can be produced 
by an untrusted algorithm.  

The concept of certificate can be
adapted to a context that allows for small  errors, produced by
numerical approximations on a computer.
Because of inexact computer arithmetic, the
equality in \eqn{eqn:y} will only be approximately correct. The imprecision in the
dual certificate $y$ can be readily eliminated as follows. If $u$ is any
vector, let $u^+$ be the vector obtained by replacing the negative
entries of $u$ with $0$, and let $u^-$ be the vector obtained by
replacing the positive entries of $u$ with $0$.  In the
following lemma, $\epsilon_1$ and $\epsilon_2$ are small error terms
that result from machine approximation. By including them in the
bounds on $c x$, a rigorous bound can be recovered.  


\begin{lemma}  Suppose that the real-valued vectors and matrices
$A,A_1,A_2$, $c,c_1,c_2$, $x,b,\ell,u$, $y$ satisfy the following
relations
  $$
  A x\le b, \quad A_1 \le A \le A_2,
  \quad c_1 \le c \le c_2,\quad \ell\le x\le u,\quad
  0\le y.
  $$
Define residuals
  $$
   \epsilon_1 = c_1 - y A_2,\quad \epsilon_2 = c_2  - y A_1.
  $$
If
$$
y b + \epsilon_2^+ u^+ + \epsilon_1^+ u^- + \epsilon_2^- l^+ + \epsilon_1^- l^- < M,
$$
then $c x < M$.
\end{lemma}

\begin{proof} S. Obua has given a formal proof of this lemma in the
Isabelle/HOL system \cite[3.7.2]{Obua:2008:Thesis}.  The proof
is a simple embellishment of Inequality~\ref{eqn:cxM}:
\begin{eqnarray*}
c x -y b &\le& c_2 x^+ + c_1 x^- -y A x\\
              &=& (\epsilon_2 x^+ + \epsilon_1 x^-)
  + y (A_1-A) x^+ + y (A_2 - A) x^- \\
              &\le& (\epsilon_2^+ +\epsilon_2^-) x^+ 
       + (\epsilon_1^++\epsilon_1^-) x^-\\
              &\le& \epsilon_2^+ u^+ +  \epsilon_2^- l^+ 
           + \epsilon_1^+ u^- + \epsilon_1^- l^-.
\end{eqnarray*}
\end{proof}

The numerical data $A_1,A_2,c_1,c_2,\ell,u,y,b$ are all explicitly given,
so that the method yields explicit bounds.
It is not necessary to trust the software
that produces the certificate $y$, because
all of the assumptions of the lemma can be checked directly.
The conditions $A x \le b$ and $\ell\le x\le u$ hold by the assumption
of the feasibility of $x$; the condition $0\le y$ is produced by replacing the
negative entries of $y$ with $0$; the other assumptions of the lemma are
checked by simple matrix operations by computer.  Interval arithmetic 
(or exact rational arithmetic) guarantees
the reliability of these matrix
operations.

In practice, it is necessary for us to solve hundreds of thousands of
linear program feasibility problems\eqn{eqn:empty}, each involving
hundreds of variables and thousands of linear constraints.

\subsection{linear relaxation}

\newterm{Nonlinear optimization} searches for the maximum of a
continuous function $f$ over a compact domain $X\subset \ring{R}^n$.
A \newterm{linear relaxation} of the domain is a polyhedron defined by
constraints $A x \le b$ such that the polyhedron contains the domain
$X$.  That is, $x\in X$ implies $A x \le b$.  A \newterm{linear
  relaxation} of the objective function $f$ is a linear function
$x\mapsto c x$ such that $f(x) \le c x$ for all $x\in X$.  It then
follows trivially that the maximum of $f$ over $X$ is at most the
value of the linear program that maximizes $c x$ such that $A x \le
b$.

Relaxation gives a mathematically sound linearization of a
nonlinear optimization problem.  It should be expected that the bounds
obtained by this method will often be crude.  Nevertheless, linear
programs can be solved efficiently, while general nonlinear
programming problems cannot.

\begin{remark}[popularization]
A popular article about the proof of the packing problem
illustrated the method by depicting the nonlinear function $f$
as the rolling hills in the countryside, cows grazing in the background.  
A helicopter,
carrying a large roof, was slowing
descending over the hilltops, bounding the height of a hilltop
with the piecewise linear roof.
\end{remark}

\begin{remark}[vanishing box trick]
Relaxation can also be seen as a {\it vanishing
box trick.}  Consider the set of counterexamples
\[
\{x\in X\mid f(x)\ge c x\}
\]
to a desired nonlinear inequality ($f(x)< c x$ on $X$).  At the
outset, the set of counterexamples to the problem has an unknown size
and structure.  These counterexamples are all placed in a large box.
The width of the box is measured and is found to be negative.
Therefore the box is empty, it contains no counterexamples, and the
nonlinear inequality is proved.  It is remarkable that a method based
on such a simple idea is sufficiently powerful to prove many difficult
nonlinear inequalities.  In this intuitive picture, the box represents
the polyhedron obtained by linear relaxation.  The negative width
represents the inequality produced by the dual certificate $y$.
\end{remark}

\subsection{linear assembly}

The nonlinear optimization problems that we solve in this book have
the structure of \newterm{linear assembly problems}.  These are
problems whose nonlinearities can confined to small dimensions.

The linear assembly problem asks for a proof of an upper bound
\begin{equation}\label{eqn:cxM}
c x < M
\end{equation}
for all $x=(x_1,\ldots,x_s)\in \ring{R}^{m_1}\times\cdots
\times\ring{R}^{m_s}=\ring{R}^n$
subject to constraints
\begin{eqnarray}
A x &\le& b,\notag\\
%x &=& (x_1,x_2,\ldots,x_s),\\
x_i &\in& D_i\notag
\end{eqnarray}
for some matrices $A,b$, natural numbers $m_i$, and sets $D_i\subset
\ring{R}^{m_i}$, where $m_1+\cdots+m_s = n$.  The only nonlinearities
are those appearing in the constraints $x_i\in D_i$.

An \newterm{assembly certificate} of the upper bound \eqn{eqn:cxM}
consists of the following data
\[
P_{i,j}\mid i=1,\ldots,s,~~j\in J_i,\quad\text{ and }\quad 
y_k\mid~~k=(j_1,\ldots,j_s),~~j_i\in J_i.
\]
where $P_{i,j}$ are polyhedra in $\ring{R}^{m_i}$ such that 
\begin{equation}\label{eqn:DP}
D_i \subset\cup_{j\in J_i} P_{i,j},
\end{equation} and $y_k$ are linear programming
dual certificates of the infeasibility of the linear programs
\[
c x \ge M,\quad A x \le b,\quad x_i\in P_{i,j_i}.
\]

An assembly certificate provides an immediate proof of the upper bound
of the linear assembly problem.  Indeed, If $x$ satisfies the
constraints of the linear assembly problem, then $x_i\in D_i$ for all
$i$.  Hence $x_i\in P_{i,j_i}$ for some indices $j_i$.  The dual
certificate $y_k$, $k=(j_1,\ldots)$, then certifies that $c x < M$.

The nonlinear part of the certificate consists in the verification
of \eqn{eqn:DP}.  In practice, the dimensions $m_i$
may be small, making this nonlinear part computationally feasible,
even when $n=\sum m_i$ is large.  (In practice, we may have $m_i\le 6$
and $n > 100$.)

%\bigskip
%For the problem to be a true linear relaxation, there must be rigorous
%proofs that each $x\in X$ satisfies the linear constraints $A x\le b$.
%This again, is a nonlinear optimization problem on the domain $X$ with
%objective function $A_i x$. 
%
%The packing problem has a special structure that avoids this
%problem.  Even though the nonlinear optimization problem involves
%about 150 variables, 
%this special structure allows all the nonlinearities to 
%be confined to small dimensions (about 6 variables).  In other words,
%the packing problem can be
%described as a coupled system of small nonlinear subsystems,
%and all the coupling comes through linear systems of equations. 
%We give a toy model of this in a moment to describe how this works.
%
%This special structure is one of the key points of the entire proof.
%If you want to know why this hard nonlinear optimization problem has
%been solved, but others have not, you now know the fundamental reason.
%By partitioning the linear relaxation of the domain $X$ according
%to the small nonlinear subsystems, we can arrange for each inequality
%$A_i x\le b_i$ to be sparsely populated, and for each relaxation
%constraint to be a statement about a nonlinear inequality on a domain
%of low dimension.  This is a tractible problem:
%we  summon interval arithmetic to prove these
%low dimensional inequalities by computer.  In this way we are able
%to give a rigorous proof that we have a true linear relaxation of a
%difficult nonlinear optimization problem.  In practice, the linear
%relaxation  bounds  are good enough to solve the packing
%problem.
%
%
%\subsection{infeasibility}

%It may seem that such a strategy is too hopelessly naive to work.
%In fact, precisely this strategy is pursued, and it works beautifully.
%The act of placing a counterexample in a box is called linear 
%relaxation.  The counterexamples form an unknown nonlinear set.
%The box that contains them is defined by linear constraints.  
%Relaxation refers to a relaxation of constraints, so that the
%box contains the counterexamples -- it is not merely a linear
%approximation to the set of counterexamples.  The box needn't be
%rectangular.  Any polyhedron will do.  To determine that a box
%has negative width is to say that it gives an {\it infeasible} linear
%program.  With the strategy, there is no need to limit ourselves
%to a single box, we can use a finite collection of boxes that
%contain the set of counterexamples.  This leads to branch and bound
%methods.
%
%
%In this appendix, we will discuss these methods in further detail,
%including linear programming, linear relaxation,  infeasibility,
%and branch and bound methods.
%
%The introduction of linear programming methods to the packing
%problem was gradual and tranquil.  
%Many of the
%inequalities in the packing problem have an obvious linear form.
%As an engineering student
%at Stanford, I had studied
%linear and nonlinear optimization.
%Given my background, 
%it was natural for me to express them as a linear program.  
%Once linear programming had made an appearance, the 
%repeated successes of the method led me to rely on it more and more.
%In the end, approximately $10^5$ linear programs appear in the solution,
%each involving some $200$ variables and about $2000$ constraints.
%This is a inconsequential task for modern algorithms.  
%
%The consequential part is to organize the output of this many linear
%programs into a comprehensible proof narrative.  How do I convince you
%the reader that this vast collection of linear programs constitutes
%the proof of a theorem?  What is the precise statement of that
%theorem, expressed in a way that does not refer to $10^5$ separate
%problems? There is an entire chapter of this book devoted to these
%questions.  This introductory essay gives enough background to carry
%us through to that chapter.
%
%
%
%
%Later in 1947, within minutes of first hearing about
%linear programming, von Neumann introduced linear programming
%duality, as an outgrowth of his theory of games.
%In 1979, L. Khachiyan found a way to solve linear programs in
%polynomial time \cite{Kha79}.  Still later, 
%N. Karmarkar found a polynomial
%time algorithm of practical value \cite{Kar84}.  
%M. Wright gives a recent survey
%of interior-point algorithms (algorithms that search for the 
%optimal solution by moving through the interior of the polyhedron
%rather than following the simplex method's strategy of searching
%along the boundary of the polyhedron) in \cite{Wri05}. 
%M. Todd also gives an excellent survey of algorithms \cite{Tod02}.
%These algorithms have been implemented in various
%software packages.
%Because of this extensive and well-developed literature on the
%subject of linear programming, we will take it as given that we
%can solve large-scale linear programs.
%
%
%
%The basic linear program admits many variations.  We can minimize
%the objective function
%\[x \mapsto c x\] 
%rather than maximize it.
%Any linear equation
%$a x = b$ can be written as a system of inequalities
%\[a  x \le b \text{ and } -a x \le -b.\]
%This allows us to reduce constrained linear optimization with
%equality constraints to a constrained linear optimization with
%inequality constraints.
%
%
%
%\subsection{infeasibility}
%
%Consider a general system of linear inequalities
%\begin{alignat}{1}
%\label{eqn:lpsys}
%A x &\le c\notag\\
%a &\le x  \,\le b,
%\end{alignat}
%with given matrix coefficients $A,c$ and given 
%lower and upper bounds $(a\le x\le b)$ on a
%vector $x$ of unknowns.  
%
%
%
%
%%In practice, many of the primal linear programs that appear
%%in the packing problem are infeasible.  This requires
%%some small adjustments to the previous discussion.   In the
%%linear programs that we encounter in this book, it is not
%%necessary to determine the exact upper bound of a the linear program.
%%We have an explicit constant $K$, and we wish to prove that
%%the maximum of the objective function is less than $K$.
%%Rather
%%than consider two separate
%%types of problems, feasible and infeasible, we recast all the linear
%%programs in this book in terms of infeasibility.
%%When the maximum of $c x$ is less than $K$, by
%%adding the constraint $c x\ge K$ to the system of constraints
%%$A x\le b$,  we get an infeasible linear program.
%%This is the vanishing box trick:  the counterexamples to a nonlinear
%%problem are constrained to lie inside an infeasible system of linear
%%%inequalities.
%%
%%
%%We now drop the objective function, and work
%%with a system of inequalities  of the form
%%  By
%%translating dual linear program certificates into this new context
%%we are led to the following definition.
%%
%
%\begin{definition}[certificate~of~infeasibility]
%A \newterm{certificate of infeasibility} for the system \ref{eqn:lpsys},
%is a pair $(u,v)$ satisfying
%\begin{eqnarray*}
%0&\le& u\\
%0&\le& v\\
%0&\le& u A + v\\
%0& >& u(c-Aa) + v(b-a).
%\end{eqnarray*}
%\end{definition}
%
%\begin{example}
%If there is an index $i$ for which $b_i < a_i$, then the system is
%clearly infeasible.  In this case, we have an obvious certificate
%of infeasibility given by $(u,v)=(0,e_i)$, where $e_i$ is the
%standard basis vector at index $i$.
%\end{example}
%
%\begin{lemma}
%If a certificate of infeasibility $(u,v)$ exists for the system
%\ref{eqn:lpsys}, then the system is infeasible.
%\end{lemma}
%
%\begin{proof}
%Suppose for a contradiction that $x$ is a feasible solution.
%Then
%\begin{eqnarray*}
%0 &\le& (u A + v)(x-a) \\
%&=& [u (c- A a) + v (b- a)] - [u (c - A x)] - [v (b - x)]\\
%&<& 0.\qedhere
%\end{eqnarray*}
%\end{proof}
%
%%The set of certificates form a cone: if $(u,v)$ is a certificate and $t>0$,
%%then $(t u, t v)$ is also a certificate.  
%
%Since the equations defining a certificate of infeasibility are
%linear in $u$ and $v$, linear programming algorithms may be used
%to produce certificates:
%\begin{align}\label{eqn:ci}
%\min_{u,v} &\, u (c-A a) + v(b-a)
%\intertext{such that }
%0 &\le u\notag\\
%0 &\le v\notag\\
%0& \le u A + v\notag
%\end{align}
%Trivial manipulations transform this minimization problem into the standard
%format \eqn{eqn:lp1} \eqn{eqn:lp2}.
%The system of constraints has an obvious feasible solution $u=v=0$.
%If we add the constraint that the objective function is at least $-1$,
%then there exists a bounded feasible solution.  
%
%\begin{lemma}  Let $A_1,A_2,A$.
%\end{lemma}
%
%If an approximation $(u',v')$ to a certificate of infeasibility is
%produced (say by computer), it can often be adjusted to give a
%true certificate.  Given an approximation $(u',v')$, define
%$(u,v)$ by
%\begin{eqnarray*}
%u &=& \max(0,u')\\
%v &=& \max(0,v',-u A).
%\end{eqnarray*}
%If $(u,v)$ satisfies
%$$u( c-A a) + v(b-a) <0,$$
%then it is a certificate of infeasibility.
%







\section{Hypermap Generation}\label{sec:hyper-gener}

Recall that a restricted hypermap  $H = (D,e,n,f)$ is one with the following
properties.
\begin{enumerate}
\item The hypermap $H$ has no double joins, and is nonempty, planar,
  connected, plain and simple.
\item The edge map $e$ has no fixed points.  % Needed in Lemma:[flag quotient]
\item The node map $n$ has no fixed points.
\item The cardinality of every face is at least $3$.
%%  (All hypoth. Needed?)
\end{enumerate}

Section~\ref{sec:generation} develops an algorithm to generate all
restricted hypermap up to isomorphism (or rather all that satisfy
prescribed bounds on number of nodes and faces).

The set of tame hypermaps is a subset of the a finite set of
restricted hypermaps (satisfying prescribed bounds).  We can generate
all tame hypermaps up to isomorphism: generate all the restricted
hypermaps and filter out those that are not tame.

These algorithms have been implemented in software.  The computer code
has been executed to obtain an explicit list of all tame hypermaps, up
to isomorphism.  By setting different initial parameters, the program
can also generate various other classes such as hypermaps with tame
contact, Tammes-tame hypermaps, and the class of hypermaps needed for
the proof of the dodecahedral conjecture
~\cite{Hales:2010:Dodec}. % McLaughlin.
G. Bauer's thesis makes an exhaustive analysis of the computer
program, and reimplements the code~\cite{Bauer:2006:Thesis}.  T. Nipkow and G.
Bauer subject the computer code to formal proof~\cite{Nipkow:2005:Tame}.



