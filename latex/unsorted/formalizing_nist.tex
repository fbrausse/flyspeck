% Template for an AMS article style
\documentclass[11pt]{amsart} %{llncs} % [11pt]{amsart}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{alltt}
\usepackage{url}

%tikz graphics
\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{chains,shapes,arrows,trees,matrix,positioning,decorations,backgrounds,fit}

\newtheorem{definition}[section]{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{assumption}[section]{Assumption}
\newtheorem{corollary}[section]{Corollary}
\newtheorem{remark}{Remark}


\parindent=0pt
\parskip=\baselineskip


% automatically generate revision number by
% svn propset svn:keywords "LastChangedRevision" revision.tex
\def\svninfo{{\tt
  filename: revision.tex\hfill\break
  PDF generated from LaTeX sources on \today; \hfill\break
  Repository Root: https://flyspeck.googlecode.com/svn \hfill\break
  SVN $LastChangedRevision: 746 $
  }
  }


% Math notation.
\def\op#1{{\operatorname{#1}}}
\newcommand{\ring}[1]{\mathbb{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Formalizing NIST cryptographic standards}
\author{Thomas C. Hales} 
%\institute{University of Pittsburgh\\
%\email{hales@pitt.edu}
\maketitle

% Date: October 25, 2013

\begin{abstract} Based on the Snowden documents, the New York Times has reported that 
the NSA has worked to subvert NIST cryptographic standards.  
This article discusses general weaknesses in the NIST standard 800-90A for pseudo-random number
generation.
Among other defects, this NIST standard is deficient because of a pervasive sloppiness in the use of
mathematics.
This, in turn, prevents serious mathematical analysis of the standard and promotes careless implementation in code.
We propose formal verification methods as a remedy.
\end{abstract}

\section{Levels of Mathematical Rigor}

We may categorize mathematical argument as being
 informal,  rigorous, or formal.


Informal mathematics is found in the popular press.
Informally, a function is continuous if it can be sketched without lifting a pencil from a sheet of paper;
chaos is the unpredictable
effect of a butterfly flapping its wings in Brazil; and 
a pseudo-random number is one that is deterministic yet effectively random.
Informal mathematics is not wrong so much as it is unsuitable for careful science
and engineering.

 A more rigorous approach
to mathematics became necessary in the final decades of the nineteenth century  
to resolve the problems with Newton's infinitessimals,
paradoxes in Cantor's set theory, and other troubles.
For example, disputes about continuity were resolved by clarifying definitions,
eventually refining a single intuitive notion of continuity into a family of related notions: 
continuity, uniform continuity, 
and so forth.  Most professional mathematical publications now adhere to widely accepted 
standards of mathematical
rigor.  The quality of a mathematical publication relies on the diligence of the author and human referees.

Formal mathematics is yet a higher standard.   Here,
it becomes necessary to abandon English and other natural languages altogether, 
and to use languages that have been specially designed for their mathematical
precision. The syntax and the semantics of language are precisely formulated.
The system specifies
every admissible rule of logic and  every mathematical axiom.  
Quality is assured by a computer, which exhaustively checks every logical step of a proof.

Formal mathematics becomes appropriate in situations where referees
reach the limits of human endurance, including the verification of mathematical proofs that are extremely
 long or  complex. (A wiki page catalogues numerous long mathematical theorems that might benefit
from formalization~\cite{WikiLong}.)  
Formal methods are well-suited for many computer-assisted mathematical proofs.  
In fact, at the formal level the line is erased between algorithms implemented as computer code
and mathematical reasoning.  In a single software system, 
we can handle  the formal verification of both code and the underlying mathematics.

Formal methods have been under development for decades, and in recent years the verification
of complex software systems, hardware, and intricate theorems has become a reality~\cite{FVI}.  
Already in 1989, it was possible to formally specify and verify simple computer systems from high-level language
to microprocessor~\cite{BHMY}.  As recent examples, 
we can site the full
verification of the correctness of a C compiler~\cite{CC-Web}
and complex mathematical theorems such as the Feit-Thompson theorem~\cite{OOT-13} and the four-color theorem~\cite{gonthier:2008:formal}.


\section{Formal Verification in Cryptography}

Formal verification of computer code can be advised in
situations where human life is at stake or large economic interests depend on the reliability of the code:
aircraft control systems, widely adopted cryptographic standards, or nuclear reactor controllers.
Formal verification is able to reduce the number of defects in software to a level that is
scarcely attainable by other means.

For several reasons, it is natural to turn to formal verification in cryptography.
The field is highly mathematical.  Many key algorithms can be implemented as
a small block of code.  A tiny defect can potentially defeat the entire algorithm.
Adversaries actively seek out bugs to exploit.
Cryptography safeguards
large financial interests and fundamental human freedoms.

Various formal verification tools have been constructed especially for application to cryptography.
Paulson's tools allow for the rapid verification in Isabelle of cryptographic protocols that are inductively defined~\cite{PaulsonXX}.  
Milner's $\pi$-calculus models the exchange of messages among
agents~\cite{RM99}.
% R. Milner. Communicating and Mobile Systems: the Pi-calculus. Cambridge University Press, 1999.
Extensions of the $\pi$-calculus have been adapted to cryptographic protocols~\cite{AF},~\cite{SPI}.
Proofs in the applied $\pi$-calculus can be completely automated.
% M. Abadi and C. Fournet, Mobile values, new names, and secure communication. In POPL '01: Proceedings of the 28th ACM SIGPLAN-SIGACT
% symposium on the principles of programming languages, pages 104-115, New York, NY, USA 2001.
% M. Abadi and A. D. Gordon, A calculus for cryptographic protocols: The spi calculus. Information and Communication, 148(1):1--70, January 1999.
General proof assistants  (such as Coq, Isabelle, and HOL) can be used for the verification of code implementing
cryptographic protocols.
Formal methods in cryptography are promoted through 
the working group on the {\it Theoretical Foundations of Security
Analysis and Design}
~\cite{TFSAD}
% http://www.dsi.unive.it/IFIPWG1_7/
\cite{FOSAD}
% Foundations of Security Analysis and Design.
%% The volumes of FOCAD.
and the Computer Security Foundations Symposium of the IEEE~\cite{CSF2013}.
%http://csf2013.seas.harvard.edu/index.html
%http://www.ieee-security.org/CSFWweb/

In truth, there are limitations in our current understanding of cryptography that force
formalization to proceed as a patchwork of interrelated proofs rather than as a single comprehensive system.
Most fundamentally, basic cryptographic assumptions are notoriously difficult mathematical
problems such as P versus NP  and the existence of one-way functions.  
We lack solid lower bounds on the computational complexity of concrete problems such as
factoring of integers.
%Lower complexity bounds established for a class of
%problems do not automatically translate to lower bounds for a subclass.
Research into security reductions is ongoing.  

Also, there is no comprehensive security model.  Rather there is a collection of models operating at various
levels of generality.
For example, the Dolev-Yao model works at a high level of abstraction, assuming that cryptographic primitives function perfectly~\cite{DY}.
% D. Dolev and A. C. Yao, On the security of public-key protocols. IEEE Transaction on Information Theory, 2(29):198--208, March 1983.
More detailed models may include computational complexity of algorithms,
side-channel attacks, implementation in code, quantum computer attacks, and so forth.

Nevertheless, we can work with these limitations, implementing a collection of interrelated formal proofs
based on current technological capabilities.  We do not advocate an unattainable system
that would rely on non-existent technologies; rather we are asking NIST
to adopt standards of widely used formal verification technology and to push forward from there.


\section{NIST standards}

Through the Snowden disclosures, the NIST standard 800-90A for pseudo-random number
generation has fallen into disrepute.  Earlier critiques of the standard have focused on specific
defects.  Here we argue that 
mathematical weaknesses run throughout the standard.  Amid the accusations that the NSA has undermined NIST
cryptographic standards, we remind NIST that one of the most effective ways to subvert a cryptographic standard is to 
muddle the math.

The first requirement of a standard is to set the tone and level of discourse that reflects the current
technological capabilities of the matter at hand.  By choosing to present an informal standard, 
avoiding both rigor and formal mathematics,
NIST has produced a standard that is out of step with the mathematical technology of the times.

The definitions in the NIST standard 
operate at an extreme level of informality.
For example,  the NIST standard on pseudo-randomness defines pseudo-randomness as ``deterministic yet also effectively random'' \cite[page 7]{NIST}.%
\footnote{Here is the full definition from NIST: 
``A process (or data produced by a process) is said to be pseudorandom when the
outcome is deterministic, yet also effectively random, as long as the internal action of the process
is hidden from from observation.  For cryptographic purposes, `effectively' means `within the limits
of intended cryptographic strength.'"}
A mathematically rigorous definition of pseudo-random generators
requires much more care, referencing rigorous notions of measure and probability,
complexity theory, and so forth.  For examples of properly formulated definitions, see \cite{Luby1996}, \cite{Yao82}, \cite{BM84}.
As it is manifestly impossible to base rigorous or formal mathematical proofs
on something so vague as ``deterministic yet effectively random,'' the early pages of the NIST standard 
effectively undermine all careful mathematical analysis.

The lack of rigor continues throughout the document.  Algorithms are described with English-text
pseudo-code that carry no rigorous semantics.  With more care,  NIST might
have provided a formal specification and a reference implementation in executable code in a language
with documented semantics.
Overall, NIST provides few mathematical claims or arguments,
and when they do appear they do not inspire confidence.   The document shows a sloppy  willingness
to slide back and forth between correct concepts and  convenient inaccuracies:  
heuristics rather than proofs, a single small free-floating constant $2^{-64}$ rather than deltas and epsilons,%
\footnote{This is analogous to the use of
 computer calculations that rely on floating point, represented by a mantissa of
fixed width rather than more mathematically reliable methods such as interval arithmetic.}
and Shannon entropy rather than min-entropy.  (See \cite[Appendix C.2]{NIST}.)
In fact, the standard is imprecise to such a degree that competing definitions of entropy are largely irrelevant.

\section{An example of NIST reasoning}

This section goes into detail about a particular mathematical argument that appears in the
NIST standard.  For our purposes here, the topic of disucssion matters less to us than the nature of the NIST committee's mathematical
thought.  Do they reason as a mathematician would in a unbroken chain of logic, or is the committee
satisfied by a lesser standard? 
It is telling that
the most significant mathematical argument in the document is there to support a weakening
of standards for the sake of efficiency.

The context is the following.  Let $E$ be an elliptic curve defined over a finite field $\ring{F}_p$,
defined on an affine chart by an equation
\[
y^2 = x^3 - 3 x + b.
\]
The pseudo-random generator extracts bits from the
 $x$ coordinates of a sequence of points $P_1, P_2,\ldots $ 
on the elliptic curve.  The construction of the sequence of points $P_i$ does not concern us here.
The issue is that the $x$ coordinates of points on an elliptic curve are not random: they
are constrained to be numbers such that $x^3 - 3 x + b$ is a square.  Research estimates are needed to
determine how big an issue this constraint is.   Aggressive truncation of bits from the binary representation of $x$ would improve randomness
while making the algorithm less efficient.%
\footnote{In light of the much discussed back door to the elliptic curve algorithm, NSA had a secret interest
in persuading users not to discard many bits from  $x$; aggressive truncation would
make the back door more difficult to use.}

NIST quotes the research \cite{MS2000} as ``an additional reason that argues against increasing truncation.''
In this short argument, there are numerous gaps in NIST reasoning.
\begin{enumerate}
\item A bound on discrepancy is not the same as uniform distribution~\cite{SDA1651}.
% See http://www.encyclopediaofmath.org/index.php/Discrepancy
\item Uniform distribution is not the same as cryptographically secure pseudo-randomness.
\item The sets $\{P_i\}$ of points  used in real-world implementations have cardinalities far too small to
be relevant to the given asymptotic estimates.
\item The research does not support the specific NIST rule that ``the recommended number of bits discarded
from each $x$-coordinate will be $16$ or $17$'' and does not convincingly argue against increasing truncation.
\end{enumerate}
Nevertheless, NIST uses the research as the basis of the inflated claim that ``certain
guarantees can be made about the uniform distribution of the resulting truncated quantities'' \cite{NIST}.

\section{Assurances}

The NIST standard 800-90A states that ``a user of a DRBG for cryptographic purposes requires assurance
that the generator actually produces (pseudo) random and unpredictable bits.  The user needs assurance
that the design of the generator, its implementation and its use to support cryptographic services are
adequate to protect the user's information'' \cite{NIST}. We agree.

What assurances does NIST actually provide about the generator?  
The document contains no mathematical proofs of pseudo-randomness and no supporting citations.  
Indeed, careful proofs would be impossible,
because as we have noted,
the definitions are more informal than rigorous.  Instead, the user of the standard must
rely on NIST's authoritative claim that ``the design of each DRBG mechanism in this Recommendation has
received an evaluation of its security properties prior to its selection for inclusion in this Recommendation.''
That's it;  that one sentence is the full extent of NIST assurance of designs.   It seems that for NIST,
assurance means to comfort with soothing words. To a mathematician, this attitude is exasperating.
 There is no mathematical statement of what
those security properties are, and no discussion of the methods that were used to reach the conclusion.
We are not told who made the evaluation or what the results of the evaluation were.

Based on the Memorandum of Understanding between NIST and NSA \cite[p.601]{ScAC}, we might
wonder whether NIST's part in the evaluation was limited.
According to the memo, ``The NIST will $\ldots$ recognize the NSA-certified rating of evaluated trusted
systems under the Trusted Computer Security Evaluation Criteria Program {\it without requiring additional
evaluation}'' (italics added). 

Here is the NIST discussion of {\tt HMAC\_DRBG}
(deterministic random bit generation based on hash message authentication codes).  It states, ``In general, even
relatively weak hash functions seem to be quite strong when used in the HMAC construction.
On the other hand, there is not a reduction proof from the hash function's collision resistance
properties to the security of the DRBG'' \cite[Appendix E.2]{NIST}. 
Note the informal tone of the discussion, the  reassurance
that a weakness is strength, the brevity, and absence of mathematical theorems.  

Cryptographic standards derive their security through the
underlying mathematics.   We can place our trust in mathematics but not in shallow assurances such as these.

\section{Conclusions}


According to NIST aspirations,
``NIST works to publish the strongest cryptographic standards possible.'' \cite{NIST-Supp}.
% http://csrc.nist.gov/publications/nistbul/itlbul2013_09_supplemental.pdf
The analysis we have made above shows that judged by professional mathematical standards, NIST is currently
very far from its goal.


NIST sets the standard both
by its choice of algorithms and by its attitude towards rigor.
Overall,  NIST's general careless tone will 
carry over into careless and vulnerable implementations of the standard.  
Today's demands on critical cryptographic standards far exceed what NIST has provided in rigor.
Better technologies are available.  In fact,
a number of formal verification projects have been completed (such as a full formal verification of a C compiler
mentioned above) that dwarf what we ask NIST  to do. 

The technology is available for NIST standards to be elevated from the current informal presentation
to formal specification of standards.  Improvements in the formal description of NIST standards is the first
step in a larger process of formal verification along the entire chain, including
the
underlying mathematical concepts, cryptographic primitives, protocols and algorithms, 
and end implementations
in computer code.

NIST standards should evolve with the times as technology improves and as new threats develop.
 We ask NIST to
catch up with current verification technology, and to give us a standard that we can
work with using the formal verification tools that are in widespread use.

\bibliographystyle{amsplain}
\bibliography{../bibliography/all}


\end{document}


