% Bourbaki talk.
% Developments in Formal Proofs
% Author Thomas C. Hales
% File created May 12, 2014.


\documentclass[brochure,english,12pt]{bourbaki}
\usepackage[matrix,arrow]{xy}
\usepackage{amssymb,amsfonts,amsmath,footnote}
\usepackage[francais]{babel}
\usepackage{MnSymbol}
\usepackage{url}
\usepackage{listings}
%\usepackage{pxfonts}
\addressindent 100mm



\date{June 2014}
\bbkannee{6x\`eme ann\'ee, 2013-2014}
\bbknumero{10xx}
\title{Developments in Formal Proofs}
\author{Thomas C. HALES}
\address{University of Pittsburgh\\
Department of Mathematics\\
Pittsburgh, PA 15260-2341\\
U.S.A.\\}
\email{hales@pitt.edu}

% sectioning.
\theoremstyle{plain}
\newtheorem{example}[equation]{Example}
\newtheorem{definition}[equation]{Definition}
\newtheorem{theorem}[equation]{Theorem}
\newtheorem{lemma}[equation]{Lemma}
\newtheorem{corollary}[equation]{Corollary}


% Math notation.
\def\op#1{{\operatorname{#1}}}
%\newcommand{\ring}[1]{\mathbb{#1}}
\def\ring#1{{\mathbb{#1}}}


\def\AD{\ring{A}}
%general
\def\b{\backslash }
%\def\mass{{\mathbf\mu}}
\def\card{\op{card}}
\def\a{{\scriptsize\text{ani}}}
\def\good{{\scriptsize\text{good}}}
\def\diamond{{\blacklozenge}}
\def\SO{{\mathbf {SO}}}
\def\OO{{\mathbf O}}
\def\st{{\text{st}}}


%frak
\def\so{\mathfrak{so}}
\def\sp{\mathfrak{sp}}
\def\gl{\mathfrak{gl}}
\def\sl{\mathfrak{sl}}
\def\g{\mathfrak{g}}
\def\t{\mathfrak{t}}
\def\cc{\mathfrak{c}}
\def\DIV{{\mathfrak{D}}}
\def\RDIV{{\mathfrak{R}}}


%mathcal
\def\A{{\mathcal A}}
\def\T{{\mathcal T}}
\def\M{{\mathcal M}}
\def\P{{\mathcal P}}
\def\O{{\mathcal O}}
\def\tA{{\tilde{\mathcal A}}}
\def\tP{{\tilde{\mathcal P}}}
\def\tM{{\tilde{\mathcal M}}}
\def\tU{{\tilde{\mathcal U}}}






\begin{document}



\maketitle



{

\narrower{\it Si la math\'ematique formalis\'ee \'etait
aussi simple que le jeu
d'\'echecs, \ldots il n'y aurait plus qu'\`a r\'ediger nos d\'emonstrations dans ce
language, comme l'auteur d'un trait\'e d'\'echecs \'ecrit dans sa notation.\ldots
Mais les choses sont loins d'\^etre aussi faciles, et point n'est besoin d'une
longue pratique pour s'appercevoir qu'un tel projet est absolument irr\'ealisable.
 -- N. Bourbaki, 1966 \cite{bourbaki1966theorie}}. % page 5. T. d. Ensembles.




}

\bigskip



A proof assistant is interactive computer software that humans use to prepare scripts of mathematical proofs.
These proof scripts can be parsed and verified, directly from
the fundamental rules of logic and the foundational axioms of mathematics.  
The
technology underlying proof assistants and formal proofs has been under development for decades and grew out
of efforts in the early twentieth century to place mathematics on solid foundations.
Various proof assistants have been built upon various mathematical foundations, including
Zermelo-Fraenkel set theory (Mizar), Higher Order Logic (HOL), and dependent type theory (Coq)
\cite{Mizar}, \cite{HOLL}, \cite{Coq}.
A {\it formal proof} is one that has been verified from first principles (generally by computer).  


This report
will focus on three  particular technological advances.
The HOL Light proof assistant will be used to illustrate the design of a highly reliable system.
Today, proof assistants can verify large bodies of advanced 
mathematics.    As an example, we will
turn to the formal proof in Coq of the Odd Order theorem in group theory.
Finally, we will discuss recent advances in the automation of formal proofs, as implemented in proof assistants
such as Mizar, Isabelle, and HOL Light.

%\subsection{origins of the fundamental lemma (FL)}\label{sec:origin}





\section{Building a trustworthy system with HOL Light}

\def\bool{\op{bool}}
\def\Fun{\op{Fun}}

HOL Light is a lightweight implementation of a foundational system
based on Higher Order Logic (HOL).  Because it is such a lightweight system,
it is a natural system to use for explorations of the reliability of formal proof assistants.



\subsection{Naive type theory}



The foundational system of mathematics, HOL, that we describe in this section is based
on a $\lambda$-calculus with a simple type theory.  This subsection describes a simple type theory in
naive terms.  

A salient feature of set theory is that it is so amorphous; everything is a set:
ordered pairs are sets, elements of sets are sets,
and functions between sets are sets.
Thus, it is meaningful in set theory to ask bizarre questions such as whether 
a Turing machine is a minimal surface.  In type theory,
the very syntax of the language prohibits this question.  Computer systems benefit from
the extra structure provided by types.

Naively, a simple type system is a countable collection of disjoint nonempty sets called types.
The collection of types satisfies a closure property: for every two types $A$ and $B$,
there  is a further type, denoted $A\to B$, that can be naively identified with the set of functions from
$A$ to $B$.  

In addition to types, there are terms, which are naively thought of as elements of types.
Each term $x$ has a unique type $A$.  This relationship between a term and its type is denoted $x:A$.
In particular, $f:A\to B$ denotes a term $f$ of type $A\to B$.

There are variables that range over types called {\it type variables}, and another
collection of variables that run over terms.


\subsection{models of HOL}

The naive interpretation of types as sets can be made precise.
We build a model of HOL in Zermelo-Fraenkel-Choice (ZFC) set theory, to see that HOL is consistent assuming that ZFC is.
In this section, we review this routine exercise in model theory.  
At the same time, we will give some indications of the structure of HOL Light.
See \cite{harrison2009hol}, for a more comprehensive introduction to HOL Light.

%The interpretations we construct will map every type of HOL to a nonempty set and every
%term of HOL to an element of a set.

%For mathematicians with a background in ZFC, it can be very helpful at first to think of the
%set of
%types of HOL as a countable collection of nonempty sets and the terms
%as elements of those nonempty sets.

\subsubsection{an interpretation of types as sets, terms as elements of sets}

The interpretation
of variable-free types as sets is recursively defined.  We use a superscript $M$ to mark the interpretation
of a type as a set.  Specifically, the types in HOL are generated by
the boolean type $\bool$ (which we interpret as a set $\bool^M=\{\downvdash ,\upvdash \}$ 
of cardinality two with labeled elements representing true and false),
the infinite type $I$ (which we interpret as a countably infinite set $I^M$),
and then recursively for any two variable-free types $A$ and $B$, the type $A\to B$ is interpreted
as the set $\Fun(A,B)^M$ of all functions from $A^M$ to $B^M$.
We can arrange that the sets interpreting these types are all disjoint.

%The interpretation extends to types with variables in the usual way by 
%parametrizing the interpretation over an valuation $v$ of variables to variables-free types.
%To interpret terms, we will also parametrize the interpretation $M$ by extending $v$ to
%include an
%valuation of term variables (of type $A$) to elements of the set $A^{M,v}$.

In summary so far,
we fix an interpretation $M$, determining a countable collection $\T =\{A^M\}$ of nonempty sets in 
ZFC.  
%HOL has two kinds of variables: type variables which appear in types and term variables
%which are terms.  
We now extend our interpretation $M$ to a {\it valuation} $v=(M,v_1,v_2)$, where $v_1$ is a function
from the set of type variables in HOL to $\T$, and $v_2$ is a function from the set of term
variables in HOL to $\bigcup\T$.
%We do this in the usual way by parameterizing the interpretation over an valuation $v$ that
%handles the variables.
%The valuation
%$v$  has two parts.  The first part of $v$ is a mapping
%from the set of type variables to $\T$; 
%and the second part is a mapping from the set of term variables to
%the union  $\bigcup\T$.
The valuation $v$ (or $v_1$) extends recursively to give a mapping that assigns
a set $A^{v}\in \T$ to every type $A$.  We require $v_2$ to be chosen so that whenever $x$ is a term variable of type $A$,
then $x^{v_2} \in A^v$.  The valuation $v$ extends recursively to give a mapping on all terms:
\[
t\mapsto t^v \in A^v \in \T, \quad \text{for all } t:A.
\]

For example,
for every type $A$,
there is a HOL term $(=)$ of type $A\to (A\to \bool)$ representing equality for that type. 
This term is interpreted
as the function in $\Fun(A,\Fun(A,\bool))^{v}$ that
takes $a\in A^{v}$ to the delta function $\delta_a$ supported at $a$
(where the support of the function means 
the preimage of $\downvdash$).\footnote{The convention in HOL is to curry functions: using the bijection
$X^{Y \times Z} = (X^Z)^Y$ to write a function whose domain is a product as a function
of a single argument taking values in a function space. In particular, 
equality is a curried function of type $\A\to (A \to\bool)$
rather than a relation on $A\times A$.}
%If $f$ has type $A\to B$ and $x$ has type $A$, there is a term
%$f(x)$, which can be interpreted as the value of the function $f^{v}\in \Fun(A,B)^{v}$ 
%at $x^{v}\in A^{v}$.
%If $x$ is a variable of type $A$, 
%and if $p$ is a term of type $B$, there is a term 
%$\lambda x.~ p$ of type $A\to B$, which can be interpreted as a function in
%$\Fun(A,B)^{v}$, defined to map $a\in A^{v}$ to the element $p^{v[a/x]}\in B^{v}$, where
%$v[a/x]$ is the modification of the valuation $v$ that sends $x$ to $a$, and that remains
%otherwise the same.


%\subsubsection{sequents}

A {\it sequent} is a pair $(L,t)$, traditionally written $L\vdash t$, where $L$ is a finite set of terms
called the {\it assumptions}, and $t$ is a term called the {\it conclusion}.  The
terms of $L$ and $t$ must all have type $\bool$.   If $L$ is empty, it is omitted from the
notation.  A {\it theorem} in HOL is a sequent that is generated from the mathematical axioms
and rules of logic.  

\begin{theorem} HOL is consistent.
\end{theorem}

\begin{proof}[Proof sketch]
We give the proof in ZFC.  Here, HOL is treated purely syntactically as a set of strings in 
a formal language.

If $L$ is a finite set of boolean terms, and if $v$ is a valuation extending $M$,  
write $L^{v}$ for the corresponding set of elements of the set $\bool^M$.
We say a sequent $L \vdash t$ is {\it logically valid}
if for every valuation $v$ for which every element of $L^{v}$ is $\downvdash \in \bool^M$, we also have
$t^{v}=\downvdash $ in $\bool^M$.

We run through the rules of logic of HOL one by one and check that each one preserves
validity.\footnote{There are 10 such rules, giving the behavior of equality, $\lambda$-abstractions, $\beta$-reduction,
and the discharge of assumptions.
The discussion in this section omits the rules for the creation of new  term constants and types.
}
For example, the reflexive law of equality in HOL states that for any term $t$
of any type $A$, we have a theorem $\vdash t = t$.  By the interpretation of equality described above, under
any valuation $v$, this equality is interpreted as the value in $\bool^M$ of the delta function: 
$\delta_{t^{v}}(t^{v})$, which is $\downvdash $.  Hence the reflexive law preserves validity.
The other rules (transitivity of equality, and so forth) are checked similarly.

We may well-order
 each set in the collection $\T$. In what follows, we 
assume that this
has been done.  HOL posits a choice operator of type $(A\to\bool)\to A$ for every type $A$.  The well-ordering  
allows us to 
interpret HOL's choice operator, as a functional
in $\Fun(\Fun(A,\bool),A)^{v}$, 
which maps a function $f\in \Fun(A,\bool)^{v}$ with nonempty support to
the minimal element of its support.

We run through the mathematical axioms of HOL and check their validity. 
There are only three.  The {\it axiom
of infinity} positing the existence of an infinite type $I$
is logically valid by our requirement to interpret $I$ as a countably infinite set.  The {\it axiom of
choice} is logically valid by the well-ordering we have placed on the sets $A^M$.  The {\it axiom
of extensionality} also holds in this model, because it holds for sets.
Thus, every axiom is logically valid.  
Since all axioms are logically valid and every rule of inference
preserves validity, every theorem is logically valid.

There is a boolean constant $\op{\tt FALSE}$ in HOL.
An easy calculation based on its definition gives that $\op{\tt FALSE}^v = \upvdash$ for every valuation $v$.
Hence, $\vdash \op{\tt FALSE}$ is not a logically valid sequent, and hence not a theorem.
This proves HOL consistent.
\end{proof}

\subsection{Computer implementation}

The bare consistency proof is just the beginning.  We can push matters  much further when
the logic is implemented in computer code.

The HOL Light system is implemented in the Objective Caml programming language, which is one of the many dialects of the ML
language.  The language ML (an acronym for Meta Language) was originally designed as a
meta language to automate mathematical proof commands \cite{Gor}.  It is significant that the development
of the ML language and the development of proof assistants have progressed hand in hand, with many
of the same researchers participating in language design and formal proofs.  
The result is a programming language that can stand up to intense mathematical scrutiny.

This parallel development of ML and proof assistants\footnote{HOL is a descendant of the original LCF theorem prover that
spurred the development of ML} also means that there are striking similarities between
the syntax of ML and the syntax of HOL.    The code listing shows a few parallels between Ocaml syntax and
HOL syntax.

\begin{lstlisting}[keepspaces=true,stringstyle=\tt,basicstyle=\small,frame=single,framesep=8pt,mathescape,morekeywords={type,let,and,in,Ocaml,HOL},columns=flexible]
Ocaml                          versus    HOL
-----                                    ---
3:int                                    3:num
[0;1;2;3]                                [0;1;2;3]  
let x = 3 and y = 4 in x + y             let x = 3 and y = 4 in x + y
map (fun x -> x + 1) [0;1;2]             map (\x. x + 1) [0;1;2]
\end{lstlisting}

%(3:int);;
%[0;1;2;3];;
%let x = 3 and y = 4 in x + y;;
%map (fun x -> x + 1) [0;1;2];;

%`3:num`;;
%`[0;1;2;3]`;;
%`let x = 3 and y = 4 in x + y`;;
%`map (\x. x + 1) [0;1;2]`;;

In ML as in HOL every term has a type and  functions $f:A\to B$ of the right type
are required to convert from data of one type $A$ to another $B$.
One of Milner's key ideas in the design of ML was to have an abstract datatype representing theorems.
The strict type system of the language ML prevents the construction of any theorems except through
a carefully secured kernel that expresses the axioms and rules of inference.  In a formal proof in HOL, 
every theorem -- no matter how long or how complex -- is checked exhaustively by the kernel,
rigidly enforced by the structure of the language.

\subsection{Verification of the code that implements HOL Light}

The code in the kernel that expresses the rules of HOL is of critical importance.  
Even a minor bug in the kernel might be exploited to create an inconsistent system.  Fortunately,
there are good reasons to believe that the kernel does not have a single bug.

1.  The kernel is remarkably small.  It takes only about 400 lines of computer code to express
all of the kernel functions, including the type system, the term constructors, sequents, the rules of inference,
the axioms, and theorems.  For example, it only takes seven lines of computer code to describe the datatypes
for HOL types, terms, and theorems, as shown in the following listing of code~\cite{HOLL}.

\begin{lstlisting}[keepspaces=true,stringstyle=\tt,basicstyle=\small,frame=single,framesep=8pt,mathescape,morekeywords={type},columns=flexible]
  type hol_type = Tyvar of string
                | Tyapp of string *  hol_type list

  type term = Var of string * hol_type
            | Const of string * hol_type
            | Comb of term * term
            | Abs of term * term

  type thm = Sequent of (term list * term)
\end{lstlisting}

2.  The code has been written in a clean, readable style and has been scrutinized by many computer scientists,
logicians, and mathematicians (including me).

3.  The correctness of the kernel has been formally verified, using the HOL Light proof assistant itself (extended by a large
cardinal) \cite{HaSelf}.  Specifically,
a model of HOL can be built inside HOL itself\footnote{A large cardinal axiom 
 gives the existence of a large type corresponding to the set $\bigcup\T$ that we used above in the construction of a model.
By G\"odel, we do not expect to construct a model of HOL in HOL except by 
adding an axiom to strengthen the system.} along the same
lines as the model of HOL in ZFC described above.  

This formal verification of HOL in HOL goes  further than the construction of a model.
It also checks that the code implementing the logic is bug free.  The code verification is based on the 
parallels mentioned above between the meta language and HOL itself, allowing the Ocaml source code for the HOL kernel to be
translated back into HOL for verification.  A stricter standard of code verification, based
on the semantics of the programming language, has been completed and is discussed in the next subsection.
The proof of HOL in HOL removes most practical doubts about the correctness of the kernel.

As independent corroboration, the correctness proof of the kernel of HOL Light 
has been automatically translated into the HOL Zero and HOL4 assistants
and reverified there \cite{adams2010introducing}.



%The proof of HOL in HOL removes practical doubts about the correctness of the kernel, but there
%are vulnerabilities that we discuss in the next subsection.
%In particular, there are gaps between the semantics of Ocaml as defined and real-world Ocaml.\footnote{Ocaml has mutable strings and object magic.}
%As Harrison puts it elsewhere, ``the final jump from an abstract function inside the logic to a 
%concrete implementation in a serious programming language which {\it appears to correspond to it} is a glaring leap of faith.'' 
%\cite{harrison:meta}.  



\subsection{HOL in Machine Code}

The formal verification of HOL in HOL does not settle the issue of trust once and for all.
The reliability of the proof assistant ultimately
rests on the entire computer environment in which the software operates, 
including the semantics of programming languages,
compilers, operating systems, and hardware.  These issues should be the concern of
every mathematician who cares about the foundations of mathematics at a time when
the practice of mathematics is gradually migrating to  computers.


 The current working goal of researchers is to create an unbroken formally verified chain extended from the
HOL Light kernel all the way down to machine code.  Most of the links have been forged, but a few
remaining parts of the chain
are still unsecured.  
\footnote{A brilliant success has been the construction of a formally verified C compiler~\cite{CC}. Another remarkable
project is the full formal verification an operating system kernel~\cite{sel4}.  
On the formal verification of  Coq and Milawa (an ACL2-like system), see
\cite{barras2010sets} \cite{myreen2012reflective}.
In this survey article,
we focus on the work done on formal verification related to the ML programming language, because it fits more closely
with our narrative of building a trustworthy system in HOL.}

CakeML, which
is a dialect of ML with mathematically rigorous operational semantics.  We turn our
attention to this dialect.    According to the designers, ``Our overal goal for CakeML
is to provide the most secure system possible for running verified software and other programs that require a high-assurance
platform'' \cite{CakeML}.
The CakeML teams has extended the HOL in HOL verification \cite{myreen2013steps}, \cite{myreen2013proof}, 
\cite{kumar2014hol}.  
The verification now checks the soundness of  the procedures for extending HOL with new definitions and types.
It verifies the soundness of the implementation of the HOL kernel as CakeML, according to the formally specified 
operational semantics of CakeML.  

In related work, a verified compiler has been constructed for the CakeML language 
 \cite{CakeML}, \cite{sarkar2009semantics}.
This brings us close to end-to-end verification of HOL Light, from its high-level logical description down to
execution in machine code.

%(XX rewrite this paragraph and add references)
%Harrison's formal proof of HOL in HOL has been extended in the following ways.  1.  Support has been added for 
%HOL Light kernel functions that extend the system, by adding new definitions, types, and axioms.
%2.  The ML code implementing the kernel of HOL Light is generated automatically from the HOL4 specification of the kernel with
%an automatically generated theorem asserting that the generated code conforms to the specification, according to the operational
%semantics of ML.




\bigskip


\subsection{Disclaimers}

At the conclusion of this section, we make the usual disclaimers.
Without exception, all physical devices are prone to failure.  Soft errors (typically 
caused by alpha particle interactions between memory and its environment) 
produce a steady
stream of errors depending on complex factors such as hardware architecture and
the elevation above sea level at which the
calculation is performed.  A formal proof in HOL of the correctness of HOL carries
the evident dangers of self-justification.  Finally, proofs of correctness are made relative to 
mathematically precise idealized descriptions of things rather than the physical objects
themselves.

Notwithstanding all these disclaimers,  formalized mathematics reduces defect rates
in mathematical proofs to levels that are simply not possible by any other available process.
In particular, formalized mathematics can now claim to be orders of magnitude more reliable than 
traditionally refereed papers.  

\bigskip

In this section, we have discussed the construction of a reliable proof assistant.  In the next section,
we turn to a different proof assistant, Coq, and look at the formalization of group theory.
We warn the reader that the Coq system, and its underlying logic -- Calculus of Inductive Constructions, 
are significantly different from the HOL system in the previous section~\cite{CiC}.


\section{Advanced Developments in Coq: The Feit-Thompson Theorem}


Feit and Thompson published their famous theorem in 1963~\cite{FT}.

\begin{theorem}[The Odd Order Theorem, Feit-Thompson (FT)]  All finite groups of odd order are solvable.
\end{theorem}

\begin{lstlisting}[keepspaces=true,stringstyle=\tt,basicstyle=\small,frame=single,framesep=8pt,mathescape,morekeywords={Theorem},columns=flexible]
Theorem Feit_Thompson (gT : finGroup Type) (G : {group gT}) :
  odd #|G| -> solvable G.
\end{lstlisting}

Solomon writes this about the significance of the Odd Order theorem, ``This short sentence and its long proof
were a moment in the evolution of finite group theory analogous to the emergence of fish onto dry
land.  Nothing like it had happened before; nothing quite like it has happened since'' \cite{Sol01}.

The Feit-Thompson paper broke through various barriers that cleared the way for a 
remarkably fruitful massive research collaboration that eventually led to the classification
of finite simple groups.  Significantly, the  255 page Feit-Thompson result
triggered an avalanche of long complex proofs related to the classification, 
culminating in the 1221 page classification of quasi-thin groups by Aschbacher and Smith~\cite{aschbacher2004classification}.
% references at https://en.wikipedia.org/wiki/Quasithin_group. 


Background material for the proof appears in
textbooks {\it Finite Groups} by Gorenstein~\cite{gorenstein2007finite}, 
 {\it Finite  Group Theory} by Aschbacher \cite{aschbacher2000finite}, and 
{\it Character Theory of finite groups} by Isaacs~\cite{isaacs2013character}.
The necessary background includes a basic graduate-level understanding of rings, 
modules, linear and multilinear algebra (including 
direct sums, tensor
products  and determinants);
fields, algebraic closures, and basic Galois theory; the structure theorems of Sylow and Hall; Jordan-H\"older;
Wedderburn's structure theorem for semisimple algebras; representation theory with Schur's lemma, Maschke's theorem; 
and character theory including Frobenius reciprocity, and orthogonality. 

I will not say much about the actual proof of the 
Feit-Thompson theorem.
%\footnote{I was a student of some of the great Cambridge group theorists at the peak of
%the classification era, but Thompson's influence pushed me towards modular forms and Conway towards sphere packings,
%leaving me as a failed finite group theorist.}  
We have now had
more than 
$50$ years to assimilate the ideas of the proof.  
There are numerous surveys of the proof \cite[p. 450]{gorenstein2007finite}, \cite{glauberman1999new}, 
\cite{thompson1968nonsolvable},
\cite{Sol01}.
The original proof of Feit and Thompson was later reworked  and simplified in two books \cite{bender1994local},
\cite{peterfalvi2000character}.

In very brief terms, the proof starts by assuming a minimal counterexample to the statement.  This
counterexample will be a finite simple group $G$ of odd order in which every proper subgroup is solvable.
Each maximal subgroup of $G$ is {\it $p$-local}; that is, the normalizer of a nontrivial subgroup
of $G$ of $p$-power order, for some prime $p$.  A major part of the proof of FT consists in establishing
restrictions on the structure of the maximal subgroups and their embeddings into $G$.
In the special case when $G$ is a {\it $CN$-group} (a group whose centralizers of non-identity elements are all nilpotent),
the maximal subgroups are {\it Frobenius groups} \cite{FHT}.  
A Frobenius group is
a nontrivial semidirect product $K\rtimes H$, where $H$ is disjoint from its conjugates, and $H$ is its own normalizer \cite[Th. 7.7]{gorenstein2007finite}.
Back in general case, the strategy is to prove that as many of the maximal subgroups as possible are as close as possible to
being Frobenius groups.  This strategy encounters many exceptions and detours, but eventually the local
analysis shows that the maximal subgroups are mostly rather Frobenius-like. 
This can be made precise \cite[Sec. 16]{bender1994local}.

% Frobenius : http://ssr2.msr-inria.inria.fr/~jenkins/current/frobenius.html

A second major portion of the proof is uses the complex character theory of $G$ to obtain inequalities over the real numbers
that restrict and ultimately eliminate all possibilities for $G$. 

A final part of the proof uses generators and relations to remove a special case in which there
are maximal subgroups isomorphic to the group of all permutations of a finite field $\ring{F}$
of the form $x \mapsto a\, \sigma (x) + b$, where $\sigma$ is a field automorphism, $b\in\ring{F}$,
and $a$ is an element $\ring{F}$ of norm $1$.





\subsection{Formal verification}


The Feit-Thompson theorem has been formally verified in the Coq proof assistant by a team led by Georges Gonthier.
\cite{gonthier2013machine}.  This is an extraordinary milestone in the history of formal proofs.
What is particularly significant about the formalization itself?

1. The Feit-Thompson theorem itself has never been seriously questioned,
but the premature announcement of the classification of finite simple groups drew sustained criticism.
 Gorenstein wrote, ``In February 1981, the classification of the finite simple groups was
completed'' \cite[page 1]{gorenstein2007finite}; and yet essential work on the classification
continued for decades after that date \cite{aschbacher2004status}. 
Serre wrote in 2004 that ``for years I have been arguing with group theorists who claimed
that the `Classification Theorem' was a `theorem', i.e. had been proved'' \cite{raussen2004interview}.
Because of the prominence of the Feit-Thompson theorem within the classification, 
this formal proof plants a flag in the middle of the larger classification project.
%those squabbles and provides a
%methodology for conflict resolution that is beyond reproach.

% http://www.ams.org/notices/200407/fea-aschbacher.pdf


2. Traditional methods of refereeing mathematical research become strained, when
proofs are unusually long or computer assisted.  There is a wiki page listing numerous proofs
in mathematics that set records for length~\cite{WikiLong}.
According to the list,
long papers cluster in certain areas
such as finite group theory related to the classification, 
the Langlands program,
and graph theory.  The odd order theorem, the four-color theorem (Gonthier's previous formalization
project), and the Kepler conjecture are all on that list. 
These three recent formalization projects send a clear defiant message  to mathematicians:
no matter how long or how complex your mathematical proofs may be, we can formalize them.

Absolutely no technological barriers 
prevent the formalization of large parts of the mathematical corpus.  The issues now
are how to make the technology more efficient, cost effective, and user friendly.

\subsection{Constructive proof of FT}

The formal proof of the FT theorem is based on the second-generation proof described in
two books cited above \cite{bender1994local},  \cite{peterfalvi2000character}.
If we were to translate the formal proof of Feit-Thompson back into a humanly readable book, 
the most notable difference would be that the original proof uses classical logic,
but the formal proof uses constructive logic.  In particular, a constructive proof avoids
 the law of excluded middle $\phi\lor \neg \phi$ and proofs by contradiction.

Several different strategies were used to translate the proof from book form to
constructive computer proof.

1.  Often, mathematicians use proof by contradiction out of sheer laziness when a direct proof
would work equally well.  
``Let $G$ be the finite group of minimal order that is a counterexample'' gets
replaced with an induction on the order of the group, and so forth.

2. In \cite{peterfalvi2000character},  the character theory of finite groups relies heavily on 
vector spaces over $\ring{C}$ and complex conjugation.  In the constructive formal proof, the corresponding
vector spaces over an algebraic clousure $\bar{\ring{Q}}$ are used.  An algebraic closure of $\ring{Q}$ 
is obtained as the union of an increasing tower of  
number fields $\ring{Q}(\alpha_i)$, for $i=0,\ldots$, each  an explicitly constructed splitting field
over the preceding one.
Complex conjugation is replaced with
conjugation with respect to a maximal real subfield of $\bar{\ring{Q}}$, also constructed
as an explicit union of real subfields of finite degree over $\ring{Q}$.

3.  Some constructions are justified by the decision procdure for the first-order theory
of algebraically closed fields~\cite{cohen2010formal}, \cite{Ha09}.  
For example, the socle of a module is defined as the sum of its
simple submodules.  This decision procedure gives a constructive test for the simplicity of a submodule,
leading to a constructive definition of the socle  (for a finitely generated 
$k[G]$-module where $G$ is a finite group and $k$ is an algebraically closed field)  \cite{gonthier2011point}.

On a related note, the construction of a simple submodule of a given module $M$ requires choice;
by confining attention to modules that are countable as sets, countable choice suffices, which 
is provable in Coq from the well-foundedness of the natural numbers.


4.  Various intermediate results in FT use classical logic, but the use of classical
logic is eliminated from the final statement of the Feit-Thompson theorem.  The wedding of constructive and
classical logics is  arranged
through a predicate $\op{\it classically}~\phi$ that marks every result proved by classical logic.
The definition of $\op{\it classically}~\phi$ is
\[
\op{classically}~\phi : Prop := \forall {b:\bool}, (\phi\to b) \to b,
\]
which is essentially the usual double negation translation of a classical formula into
constructive logic.  The definition of {\it classically} works in such a way that $\phi$ may
be used as an assumption in the proof of any boolean goal $b$, whenever $\op{\it classically}~\phi$ is known.
%Specifically, matching a goal
%with $\op{\it classically}~\phi$ reduces the goal $b$ to subgoal $\phi\to b$, whereby the assumption
%$\phi$ is introduced.

5.  Function extensionality does not hold in Coq; that is, it is not provable in Coq
that two functions are equal, if they are provably equal on every input.  This creates various
complications in Coq.  There are no general quotient types in Coq: types corresponding
to the set of classes on a type under an equivalence relation.  Instead, Coq introduces {\it setoids} 
(that is, a set together 
with an equivalence relation on that set) without passing to the quotient.  Unfortunately, setoids carry a certain
overhead, which was not acceptable in the FT proof, which makes ubiquitous use of quotient
groups $G/N$.   

To allow for genuiune quotients, the FT theorem is developed in the context of {\it finite types} with {\it decidable equality};
(that is, the type is equipped with boolean procedure that decides whether any two elements of that type are equal).
In this context, function extensionality holds and genuine quotients groups can be formed.


6.  For all its advantages, at times the type theory of Coq is best forgotten.  In particular,
in arguments involving a finite number of finite groups, there would be excessive overhead in
assigning a separate type, a separate binary operation on each group, 
and writing explicit homomorphisms embedding subgroups into groups.
 For such arguments, the finitely many finite groups are
often considered via Cayley as subgroups of a larger ambient finite group giving them a common binary
operation and type.  These ambient group arguments are one of the biggest stylistic differences
between the Coq proof and the original.


\subsection{Library of abstract algebra}

Most of the work in developing the proof of the Feit-Thompson theorem went into development of the libraries in
abstract algebra and the related computer infrastructure.  The libraries include all the formalization of all
of the necessary background material described at the beginning of this section, from Frobenius reciprocity
 to Wedderburn's theorem.  From a software engineering point of view, it has
been a major undertaking to get the computer to understand algebra at a level
comparable to that of a working mathematician, and for it all to be formally
justified.  This includes notation, so that for example,  in a 
given context the computer is able to correctly infer the correct binary operation 
to be used in a given expression $g*h$.  

This also includes much of the implicit domain knowledge that is required to
read an advanced proof.
For example, mathematicians do not need to be reminded again and again that $K \cap H$ is a group when
$K$ and $H$ are subgroups of some group.  The FT team has worked very hard
to automate such details within their system.



\section{Automating Formalization}

One way to make proof assistants more useable is to increase the amount of
automation.   Ideally, the human should provide the high level structure of a proof
as it is done in a traditionally published paper, and the computer should use
search algorithms to insert the low-level reasoning.   

As technology has developed, computers have become capable of providing
more and more of the low-level reasoning.   This section describes a 
 promising technology in this field.

Before getting into the details, we point out that proof search algorithms
are based on fundamentally different design considerations than the proof
assistants themselves.  As we explained in the first section of this report, it is a matter of
immense importance for the underlying logic of the proof assistant to be sound
and for the implementation in code to be free of bugs.  

By contrast, proof search algorithms should be allowed to use whatever
reckless methods they please, with the understanding that their results must
eventually be verified by  the proof assistant before being admitted as part of
formal proof.  In particular, the soundness or completeness of these algorithms
is not a necessity.  It is more important to have fast algorithms with a high
success rate.  The research area is more like a branch of engineering than traditional
math, making use of 
experimentation, statistical methods, and machine learning.



\subsection{Automated First-order theorem provers (ATP)}

% Give Herbrand.


A {\it first-order formula with equality in the predicate calculus} is a formula built
from variables $x,y,\ldots$, logical operations $\neg, \land, \Rightarrow,\ldots$, $n$-ary function symbols $f,g\ldots$, 
 $n$-ary predicate symbols $P, Q,\ldots$  and quantified variables $\forall x, \exists y,\ldots$,
according to syntactic rules that I will not spell out here.     For example,
\[
((\exists x.~P(x)) \Rightarrow (\forall y. Q(y))) \Leftrightarrow (\forall x\forall y.~P(x)\Rightarrow Q(y))
\]
is a first-order formula.
In contrast to higher-order logics discussed earlier, in a first-order formula,
quantifiers over function symbols and predicate symbols are not allowed.
The equality predicate  symbol $(=)$ is treated differently from other relation symbols.  Equality is required to have the
standard interpretation, but other relation symbols are uninterpreted.

We are interested in algorithms that are {\it refutation complete}; that is, given
the input of an unsatisfiable first-order formula, the algorithm outputs 
a proof of its unsatisfiability.  A refutation complete
algorithm can also produce proofs of logically valid formulas, by refuting the negation of the formula.

One of the great success stories in algorithmic logic in recent decades has been the development of
powerful automated first-order theorem provers (ATP) 
and their implementation in software~\cite{bachmair2001resolution}, \cite{Ha09}.   
Refuting unsatisfiable formulas
 is something that can be done really well.  ATPs can handle problems with thousands of axioms.
There is an annual competition (CASC) among ATPs.  In recent years, the {\it Vampire} 
theorem prover developed has
dominated the competition, with other strong contenders such as the {\it Spass} and {\it E} theorem provers~\cite{riazanov2002design}.  
%The competition
%problems in the LTB division can contain millions of premises.
% See our "trophies" at www.vprover.org.


The first-order formula is first preprocessed according to well-established rules that preserve satisfiability
(prenex form, skolemization of variables, conjunctive normal form).  After preprocessing, quantifiers appear
as a single universally quantified block and can be omitted from the notation.  What remains is a 
formula in conjunctive normal form
\[
C_1 \land C_2 \land \cdots \land C_n,
\]
where each {\it clause} $C_i$ is a disjunction of {\it literals} (a predicate symbol applied to terms or its negation).
It is enough to work with the set of clauses $\{C_1,\ldots,C_n\}$, dropping the conjunction
symbol from notation.

What has been understood for a long time are rules of inference ({\it resolution}  together with
special inferences that dealing with equality) that are refutation complete.  The  challenge is to
find efficient ways to manage and contain the vast explosion of the number of clauses that result by repeated
application of the inference rules.  Poorly designed algorithms exhaust available memory before
a refutation is found.

Vampire uses various strategies to restrict the number of clauses.  The algorithm
depends on an well-founded partial ordering on the set of terms  that drives the direction of the 
 inference rules.  
A set $S$ of clauses
is {\it saturated} with respect to a set of inference rules if the application of an inference rule to clauses in the set
yields another clause in the set.  A clause in $S$ is {\it redundant} if it is a logical consequence of smaller clauses
in $S$ (with respect to the ordering).  
The algorithm repeatedly updates a set of clauses that is saturated
modulo redundancy, aggressively removing redundant clauses, and giving strong preferential
treatment to the inference rules (called simplifying inferences) that do not increase the cardinality of the set of clauses,
according to the maxim
 ``apply simplifying inferences eagerly; apply generating inferences lazilly.'' \cite{kovacs2013first}.


\subsection{Hammers}

There are some automated theorem provers based on higher-order logic % LEO and TPS,
but the most prevalent practice is to translate problems in higher-order logic into first-order logic,
solve them with an ATP, 
then translate the answers back into higher-order logic.\footnote{There are many
issues that come up in translating formulas in higher-order logic into first-order logic that I will not
discuss here.  In particular, some of the procedures might not be logically sound.}

An early example of translating proofs in this way is Harrison's MESON (based on model elimination), 
which follows this approach
to automated much of the elementary logical reasoning in HOL Light.  With MESON, the human
is required to supply all the relevant mathematical theorems, and MESON provides the logical
glue that combines the given theorems to prove a desired goal.   

Long ago, it was possible for a human expert to remember the names of all the theorems in the libraries
of the proof assistant, and to type out the name of every needed theorem on every occasion of its use.
Over time, large  mathematical libraries of formally proved theorems have been developed.  It is no longer
a reasonable request to ask the human to remember and type the names of all the theorems that are relevant
for a proof.  
This practice is still widespread; but it is a practice that must come to an end.

Larry Paulson's idea is that when trying to give a formal proof 
of a goal, we should
ship the goal together with all of the libraries of tens of thousands of theorems to the ATP
and let it figure out which of many theorems to use in the proof~\cite{Paar}.  
The method is called a {\it sledgehammer}  for its ability to deliver a powerful blow and its complete lack of finesse.

Actually, Paulson's approach is better engineered than what I have described.  Rather than ship tens of thousands
of theorems the ATP, 
he first applied an automated heuristic procedure that selects a few hundred theorems
deemed to be the most relevant.  For example, to prove a new trig identity, we might select other
trig identities as likely to be relevant.  This premise selection  can dramatically
improve the success rate of the procedure.  When the ATP is successful, it finds
a proof of the theorem and a (now small) list of theorems that are required in the proof.  
The small list of relevant theorems is shipped back to the proof assistant, which uses them to automatically
reconstruct a formal proof.

\subsection{Learning}


To summarize the basic thesis of data driven learning as J. Urban has explained it to me: the big data approach to solving
challenging problems in AI is now  an established part of corporate
culture with many success stories ranging from machine translation to self-driving cars.
We have large libraries of formal proofs in systems such
as Mizar, Isabelle, Coq, and HOL Light.  
Let's turn those libraries into training sets for machine learning algorithms.
In concrete terms, we may apply machine learning to improve the premise-selection procedure
used by sledgehammer.  

Machine learning algorithms have led to visible success: 40\% of the
theorems in the Mizar math library can now be proved by fully automated procedures \cite{DBLP:journals/corr/KaliszykU13b}.  
In the HOL Light/Flyspeck
libraries the success rate for full automation is now 47\% \cite{kaliszyk2014learning}.
These automation rates represent an enormous savings in human labor.
For more on these subjects, see the recent articles
~\cite{bohme2010sledgehammer}, 
\cite{urban2010evaluation},  
\cite{kaliszyk2012learning}, \cite{kaliszyk2013automated},
\cite{alama2014premise}.





%In some cases, the automated proofs are different from the human proofs.  For example,
%in HOL Light, a theorem states that the convex hull of any three points in $\ring{R}^3$ is a null set.
%The human proof gives this as  corollary of a more general theorem that the convex hull of a finite set
%is a null set if the cardinality of the set is no more than the dimension.  The ATP proof uses
% a theorem stating that the affine hull of three points in $\ring{R}^3$ is a null set, which is a shorter
%proof because it avoids the issue of cardinality.






\section{Final Remarks}

The aim of this report has been to describe some of the recent developments in formal proofs.
Space and time do not permit a comprehensive survey, but in this final section, I briefly mention
a few other projects.


\subsection{Homotopy Type Theory}

My report will be directly followed by a report by Thierry Coquand on dependent types and
the univalence axiom, so I will be brief in my remarks on homotopy type theory.

Homotopy type theory (HoTT) is a foundational system for mathematics that includes
dependent type theory, the univalence axiom, and higher inductive types.  
Introductions to homotopy type theory can be found at \cite{awodey2007homotopy},
\cite{aczel2013homotopy},  \cite{pelayo2012homotopy}.
For models of HoTT, see \cite{kapulkin2012simplicial}.

It goes without saying that as mathematicians, we construct the ground on which we stand;
the foundations of mathematics are of choosing, subject
to only mild constraints such as plausible consistency, expressive power, and a community of users.
In particular, nothing but our own limited imaginations prevents us from relocating the
foundations much closer to home.

By being a foundational system that is close to the actual practice of homotopy theory,
HoTT makes the formalization of this branch of mathematics surprisingly refreshing.
In the last two years many new formal proofs and constructions have been obtained: 
loop spaces, computations of various
fundamental groups of spheres, the Freudenthal suspension theorem,
the Seifert-van Kampen theorem,
construction of Eilenberg-Mac Lane spaces~\cite{licataeilenberg}, and the Blakers-Massey theorem,
Formalization of these results in other systems would have been much more labor intensive.
A new line of research, synthetic homotopy theory, develops homotopy theory on HoTT foundations.
% See Licata https://github.com/dlicata335/hott-agda, homotopy directory.

As an example, 
here we list Grayson's code that constructs the classifying space as the type of a torsor in the univalent
foundations~\cite{Ktheory}.  We include his proof that the classifying space $BG$ is connected.
I challenge any other system to pass from the foundations of math to classifying spaces so directly and elegantly!


\begin{lstlisting}[keepspaces=true,stringstyle=\tt,basicstyle=\small,frame=single,framesep=8pt,mathescape,morekeywords={Definition,Lemma,Proof,Defined},columns=flexible]
Definition ClassifyingSpace G := pointedType (Torsor G) (trivialTorsor G).
Definition E := PointedTorsor.
Definition B := ClassifyingSpace.
Definition $\pi$ {G:gr} := underlyingTorsor : E G -> B G.

Lemma isconnBG (G:gr) : isconnected (B G).
Proof. intros. apply (base_connected (trivialTorsor _)).
  intros X. apply (squash_to_prop (torsor_nonempty X)). { apply propproperty. }
  intros x. apply hinhpr. exact (torsor_eqweq_to_path (triviality_isomorphism X x)). 
Defined.
\end{lstlisting}
% https://github.com/UniMath/UniMath/blob/master/UniMath/Ktheory/GroupAction.v





\subsection{Bourbaki on Formalization}

Over the past generation, the mantle for Bourbaki-style mathematics has  passed to the formal proof community, in the way it
 deliberates carefully on matters of notation and terminology, finds
appropriate level of generalization of concepts and abstractions, and
situates different branches of mathematics within a coherent framework.

The opening quote claims that formalized mathematics is absolutely unrealizable.
Bourbaki objected that formal proofs are too long (``{\it la moindre d\'emonstration \ldots
exigerait d\'ej\`a des centaines de signes}''), that it would be a burden to forego 
the convenience of abuses of notation, and that they do not leave room for useful metamathematical
arguments and conventional abbreviations~\cite{bourbaki1966theorie}.

Bourbaki is correct in the strict sense that no human artefact is absolutely trustworthy and that
the standards of mathematics evolve in a historical process, according to available technology.
Nevertheless,  technological barriers hindering formalization have fallen one
after another.  Today, computers verifications that
check millions of inferences are routine.
As Gonthier has convincingly shown in the Odd Order theorem project, 
many abuses of notation can actually be described by precise rules and implemented as algorithms,
making the term {\it abuse of notation} really something of a misnomer, and
allowing mathematicians to work formally with with customary notation.
Finally, the 
trend over the past decades has been to move more and more features out of the metatheory and into the theory, and to
rework metamathematical argument as reasoning in higher-order logic.  In particular, it is now
standard to treat abbreviations and definitions as part of the logic itself rather than metatheory.

\subsection{Future Work}

This report has described three major projects in the world of formal proofs:
 trustworthy systems with HOL,  advanced mathematical theorems formalized
in Coq, and  increased automation with sledgehammers.   

We are still far from having an automated mathematical journal referee system, but close enough
to propose this as a realistic research program.  Already some 10\% of all papers of the
Principles of Programming Languages (POPL) 
symposium in computer science are completely formalized \cite{SewPOPL2014}, \cite{aydemir2005mechanized}.
%Comparable formalization rates for mathematics as a whole remain distant but eventually attainable.
Other recent research automates the translation of mathematical prose from English
into a computer-parsable form with semantic content \cite{ganesalingam2013language}.
As these technology develops, we may anticipate the day when the precise formal statements of mathematical
theorems may be extracted from the prose.  Once sufficiently many statements from the natural language proof 
can similarly be extracted, proof automation will take over, filling in the remaining details, to produce
a formal proof from the natural language text.


For other  surveys of formal proofs, see  \cite{avigad2014formally}, \cite{Hales:2008:formal}.

\section{Appendix. Some Formally verified theorems}


This section gives a brief overview of some of the theorems that have been sucessfully formalized
in various proof assistants.  The purpose of these examples is to showcase the range
of what can be obtained by current technologies.


The four-color theorem was formalized in Coq~\cite{gonthier2008formal}.



\begin{lstlisting}[keepspaces=true,stringstyle=\tt,basicstyle=\small,frame=single,framesep=8pt,morekeywords={Variable,Theorem,Proof,Qed},columns=flexible]
Variable R : real_model. 
Theorem four_color : (m : (map R))
     (simple_map m) -> (map_colorable (4) m). 
Proof.
    Exact (compactness_extension four_color_finite). 
Qed.
\end{lstlisting}


The elementary proof of Erd\"os and Selberg was formalized in Isabelle \cite{avigad2007formally}.  
The analytic proof of Hadamard and de la Vall\'ee Poussin
was formalized in HOL Light\cite{harrison2009formalizing}.  

\begin{lstlisting}[keepspaces=true,stringstyle=\tt,basicstyle=\small,frame=single,framesep=8pt,mathescape,morekeywords={theorem,fixes,assumes,defines,shows,Variable,Theorem,Proof,Qed},columns=flexible]
  ((\n. &(CARD {p | prime p /\ p <= n}) / (&n / log(&n)))
    ---> &1) sequentially
\end{lstlisting}

Here is the formal statement of the Brouwer fixed point theorem, which was formalized in HOL Light by J. Harrison.

\begin{lstlisting}[keepspaces=true,stringstyle=\tt,basicstyle=\small,frame=single,framesep=8pt,mathescape,morekeywords={theorem,fixes,assumes,defines,shows,Variable,Theorem,Proof,Qed},columns=flexible]
  $\forall$ f:real^N->real^N s. 
  compact s $\land$ 
  convex s $\land$
  ~(s = {}) $\land$
  f continuous_on s $\land$
  IMAGE f s SUBSET s
  $\Longrightarrow$ $\exists$x. x IN s $\land$ f x = x
\end{lstlisting}

The formalization of the central limit theorem was carried out earlier this year in Isabelle \cite{avigad2014formally}.

\begin{lstlisting}[keepspaces=true,stringstyle=\tt,basicstyle=\small,frame=single,framesep=8pt,mathescape,morekeywords={theorem,fixes,assumes,defines,shows,Variable,Theorem,Proof,Qed},columns=flexible]
theorem (in prob_space) central_limit_theorem:
  fixes 
    X :: "nat $\Rightarrow$ 'a $\Rightarrow$ real" and
    $\mu$ :: "real measure" and
    $\sigma$ :: real and
    S :: "nat $\Rightarrow$ 'a $\Rightarrow$ real"
  assumes
    X_indep: "indep_vars ($\lambda$i. borel) X UNIV" and
    X_integrable: "$\bigwedge$n. integrable M (X n)" and
    X_mean_0: "$\bigwedge$n. expectation (X n) = 0" and
    $\sigma$_pos: "$\sigma$ > 0" and
    X_square_integrable: "$\bigwedge$n. integrable M ($\lambda$x. (X n x)$^2$)" and
    X_variance: "$\bigwedge$n. variance (X n) = $\sigma^2$" and
    X_distrib: "$\bigwedge$n. distr M borel (X n) = $\mu$"
  defines
    "S n $\equiv$ $\lambda$x. $\Sigma\,$i<n. X i x"
  shows
    "weak_conv_m ($\lambda$n. distr M borel ($\lambda$x. S n x / sqrt (n * $\sigma^2$))) 
        (density lborel standard_normal_density)"
\end{lstlisting}

% https://github.com/avigad/isabelle/blob/master/Analysis/Central_Limit_Theorem.thy
% https://github.com/avigad/isabelle/blob/master/Analysis/Central_Limit_Theorem.thy


The Kepler conjecture asserts that no packing of congruent balls in $\ring{R}^3$ can have density greater than
the face-centered cubic packing.  The Kepler conjecture is now a theorem~\cite{Hales:2006:DCG}.  
The proof relies on many computer
calculations.  The noncomputer parts of the proof (as well as many of the computer parts of the proof) of the Kepler conjecture 
have been formalized in HOL Light~\cite{website:FlyspeckProject}.  
The statement that follows expresses the noncomputer part of the proof.
This formalization has been a large collaborative effort.

\begin{lstlisting}[keepspaces=true,stringstyle=\tt,basicstyle=\small,frame=single,framesep=8pt,framextopmargin=10pt,mathescape,morekeywords={Variable,Theorem,Proof,Qed},columns=flexible]
    linear_programming_results $\land$      
    import_tame_classification $\land$      
    the_nonlinear_inequalities $\land$
    restricted_hypermaps_are_planegraphs    
    $\Longrightarrow$ the_kepler_conjecture
\end{lstlisting}

\bigskip

\subsubsection{Acknowledgements}

None of the work described in this report represents my own research, except for contributions to the
proof and formalization of the Kepler conjecture.  I would like to thank the many people who helped me
during the preparation of this report, including many speakers and participants at the special program at IHP
on formal proofs, particularly,  A. Mahboubi, (XX Add XX).

%I wish to thank Harrison, Mahboubi, Gonthier, Nipkow, Avigad, Urban...


\raggedright
\bibliographystyle{plain} %plainnat
\bibliography{/Users/flyspeck/Desktop/googlecode/flyspeck/latex/bibliography/all}





\end{document}