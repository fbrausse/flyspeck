% 
% Author: Thomas C. Hales
% Affiliation: University of Pittsburgh
% email: hales@pitt.edu
%
% latex format

% History.  File started Jan 12, 2011
% 
%% 

\documentclass{llncs}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{amssymb}
\usepackage{amsmath}

\usepackage{alltt}
\usepackage{rotating}
\usepackage{floatpag}
 \rotfloatpagestyle{empty}
%\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{multind}\ProvidesPackage{multind}
\usepackage{times}

% my additions
\usepackage{verbatim}
\usepackage{latexsym}
\usepackage{crop}
\usepackage{txfonts}
\usepackage[hyphens]{url}
\usepackage{setspace}
\usepackage{ellipsis} % 
% http://www.ctan.org/tex-archive/macros/latex/contrib/ellipsis/ellipsis.pdf 
%\setstretch{2}  % for double spacing

% fonts
\usepackage[mathscr]{euscript} % powerset.
\usepackage{pifont} %ding
\usepackage[displaymath]{lineno}

%%\usepackage{fancyhdr}
%%\usepackage{mparhack}
%%\usepackage{edmargin} %endnotes


% automatically generate revision number by
% svn propset svn:keywords "LastChangedRevision" turing.tex
\def\svninfo{{\tt
  filename: turing.tex\hfill\break
  PDF generated from LaTeX sources on \today; \hfill\break
  Repository Root: https://flyspeck.googlecode.com/svn \hfill\break
  SVN $LastChangedRevision: 1552 $
  }
  }
%-%

% Math notation.
\def\op#1{{\hbox{#1}}} 
\def\tc{\hbox{:}}
\newcommand{\ring}[1]{\mathbb{#1}}
\def\amp{\text{\&}}
\def\bq{\text{\tt {`}\,}}
\def\true{\text{true}}
\def\false{\text{false}}
% Flags


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Mathematics in the Age of the Turing Machine}
\author{Thomas C. Hales}
\institute{University of Pittsburgh\\
\email{hales@pitt.edu}}
\maketitle


\section*{}

{

\narrower

\it

``And when it comes to mathematics, you must realize that this is the human mind
at the extreme limit of its capacity.'' (H. Robbins) ``\ldots so reduce the use of
the brain and calculate!'' (E. W. Dijkstra)

}

% XX Today, mathematicians routinely use computers informally to prove powerful theorems, while computer scientists are more rigorous, using formal tools to prove weaker mathematical results.  Soon formal methods will be adequate for the kinds of theorems that mathematicians want to prove.



\section{Mathematics by Computer}

Computers have become so pervasive in mathematics that a comprehensive
survey is out of the question.  It would almost be like asking for a
summary of applications of addition to mathematics.  This section
gives many examples that give a composite picture of computers in
mathematical research.


\subsection{transcient uses of computers}

It has become common for problems mathematics to be first verified by
computer and later confirmed without them.

Perhaps the best known example is the construction sporadic groups as
part of the monumental classification of finite simple groups.  The
sporadic groups are the $26$ simple groups that do not fall into
natural infinite families.  For example, the Lyons (1972) predicted
the existence of a sporadic group of order
\[
2^ 8\cdot 3^7\cdot 5^6\cdot  7\cdot 11 \cdot 31 \cdot 37 \cdot 67.
\]
In 1973, Sims proved the existence of this group in a long unpublished
manuscript that relied on many specialized computer programs.
By 1999 \cite{HS99}, the calculations had become standardized in group theory
packages, such as GAP and Magma.
Eventually,  computer-free existence \cite{MParker} and uniqueness \cite{AS92} proofs were found.

%%T Cite ref TR0416 (Sims 1999), for the 1973 calculations
%%T http://dimacs.rutgers.edu/~havas/TR0416.pdf  (Havas and Sims, 1999) on laptop.
%% http://en.wikipedia.org/wiki/Lyons_group




Euler conjectured (1769) that a fourth power cannot
be the sum of three positive fourth powers, that a fifth power cannot be the sum of four positive fifth powers, and so forth.  
In 1966, a computer search \cite{LP66} on a CDC 6600 mainframe uncovered a counterexample 
\[
27^5 + 84^5 + 110^5 + 133^5 = 144^5,
\]
which can be checked by hand (I dare you). 
The two sentence announcement of this counterexample qualifies as one of the shortest mathematical publications of all times.
Twenty years later, a more subtle search
gave~\cite{Elkies88}:
\[
2682440^4 + 15365639^4 + 18796760^4 = 20615673^4.
\]

%% http://en.wikipedia.org/wiki/Euler's_sum_of_powers_conjecture

The Catalan conjecture (1844) asserts that the only solution to the equation
\[
x^m - y^n = 1,
\]
in positive integers $x,y,m,n$ with exponents $m,n$ greater than $1$
is the obvious
\[
3^2 - 2^3 = 1.
\]
That is, $8$ and $9$ are the only consecutive positive perfect powers.
By the late 1970s, Baker's methods in diophantine analysis had reduced
the problem to an astronomically large and hopelessly infeasible finite
computer search.  Mih\u ailescu's proof (2002) of the Catalan
conjecture made light use of computers (a one-minute calculation), and
later the computer calculations were entirely eliminated~\cite{Mih},~\cite{TM03}.

%% computer use in Catalan ... Bulletin article, TAUNO METS... on laptop drive. May directory.
%% eliminated in Steiger article on laptop drive.  May directory.

Bailey, Borwein, and Plouffe found an algorithm for calculating the
$n$th binary digit of $\pi$ directly without first calculating any of the
earlier digits.  They understood that to design such an algorithm,
they would need an infinite series for $\pi$ whose denominators
contained powers of $2$.  They did not know of any such formula, and
made a computer search (PSLQ lattice reduction algorithm) for any
series of the desired form.  The search found a numerical identity
\[
\pi = \sum_{n=0}^\infty 
\left(
\frac{4}{8n+1} - \frac{2}{8n+4} - \frac{1}{8n+5} - \frac{1}{8n+6}
\right) 
\left(\frac{1}{16}\right)^n,
\]
which was then rigorously proved and used to implement their binary
digits algorithm.

% XX make the "denominators contained powers of 2" more precise.
% http://www.andrews.edu/~calkins/physics/Miracle.pdf

%\subsection{computer assisted discovery of conjectures}

\subsection{Birch and Swinnerton-Dyer conjecture}

I believe that the Birch and Swinnerton-Dyer conjecture is the deepest
conjecture ever to be formulated with the help of a computer~\cite{BSD}.  The
Clay Institute has offered a one million dollar prize to anyone who
resolves it.

Let $E$  be an elliptic curve defined by
an equation $y^2 = x^3 + a x + b$ 
over the field of rational
numbers.  Motivated by related quantities in Siegel's work on quadratic forms, Birch and Swinnerton-Dyer set out to estimate the quantity
\begin{equation}\label{eqn:np}
\prod {N_p/p},
\end{equation}
where $N_p$ is the number of rational points on $E$ modulo $p$, and
the product extends over primes $p\le P$~\cite{Bir}.  Performing
experiments on the EDSAC II computer at the Computer laboratory at Cambridge University during the years 1958--1962, they observed that as $P$ increases, the products (\ref{eqn:np})
grow asymptotically as 
\[
c(E) \log^r P,
\]
for some constant $c$, where $r$ is the Mordell-Weil rank of $E$; that
is, the maximum number of independent points of infinite order in the
group $E(\ring{Q})$ of rational points.  Following the suggestions of
Cassels and Davenport, they reformulated this numerical asymptotic law in
terms of the zeta function $L(E,s)$.  (Thanks to the work of Wiles in
the proof of Fermat's Last Theorem and subsequent extensions of that
work, it is known that $L(E,s)$ is an entire function of the complex
variable $s$.)  The Birch and Swinnerton-Dyer conjecture asserts that
the rank $r$ of an elliptic curve over $\ring{Q}$ is equal to the
order of the zero of $L(E,s)$ at $s=1$.

A major recent theorem establishes that the Birch and Swinnerton-Dyer
conjecture holds for a positive proportion of all elliptic curves over
$\ring{Q}$~\cite{BS:2010}.  This result, although truly spectacular, is
somewhat misleading in the sense that the elliptic curves of high rank rarely
occur but pose the greatest difficulties.





\subsection{Sato-Tate}

The Sato-Tate conjecture is another major conjecture about elliptic
curves that was discovered by computer.  If $E$ is an elliptic curve
with rational coefficients
\[
y^2 = x^3 + a x + b,
\]
then the number of solutions modulo a prime number $p$ (including the
point at infinity) has the form
\[
1 + p - 2\sqrt{p}\cos\theta_p.
\]
for some real number $0\le \theta_p\le \pi$.  In 1962, Sato,
Nagashima, and Namba started calculations  of $\theta_p$ on a Hitachi
HIPAC 103 computer to understand how these numbers are distributed as
$p$ varies for a fixed elliptic curve $E$~\cite{Sch}.  By the spring of 1963, the
evidence suggested  $\sin^2\theta$ as a good fit of the data.
That is, if $P(n)$ is is the set of the first $n$ primes, and
$f:[0,\pi]\to\ring{R}$ is any smooth test function, then for large
$n$,
\[
\frac{1}{n}\sum_{p\in P(n)} f(\theta_p) \quad\hbox{ tends to }\quad
\frac{2}{\pi}\int_0^\pi f(\theta)\,\sin^2\theta\,d\theta.
\]
The Sato-Tate conjecture (1963) predicts that this same distribution is
obtained, no matter the elliptic curve, provided the curve does not
have complex multiplication.  Tate, who arrived at the conjecture
independently, did so without computer calculations.

Serre interpreted Sato-Tate as a generalization of Dirchlet's
theorem on primes in arithmetic progression, and gave a proof strategy
of generalizing the analytic properties of $L$-functions used in
the proof of Dirichlet's theorem~\cite{Se68}  Indeed, complete proof of Sato-Tate
conjecture has now been found and is based on (extremely deep)
analytic properties of $L$-functions~\cite{Car:Bourbaki}.
The proof of the Sato-Tate conjecture and its generalizations has been
one of the most significant recent advances in number theory.

[XX Insert historical Sato-Tate graphic.]

% Sato-Tate:
% Sato's calc: graphic, http://www.math.ou.edu/~rschmidt/satotate/ST2.pdf
% publish this.

%% Sato's history of computation:
%% http://www.math.ou.edu/~rschmidt/satotate/page5.html

\subsection{packing  tetrahedra}


[XX insert graphic of Chen's packing]

\subsection{projective planes}

A finite projective plane of order $n>1$ is defined to be a set of
$n^2 + n + 1$ lines and $n^2 + n+ 1$ points with the following
properties:
\begin{enumerate}
\item Every line contains $n+1$ points;
\item Every point is on $n+1$ lines;
\item Every two distinct lines have exactly one point of intersection;
\item Every two distinct points lie on exactly one line.
\end{enumerate}

The definition is an abstraction of properties that evidently
hold for $\ring{P}^2(\ring{F}_q)$, the projective plane over a finite
field $\ring{F}_q$, with $q=n$, for any prime power $q$.  In
particular, a finite projective plane exists whenever $n>1$ is a prime
power.

[XX Insert graphic of Fano Plane]

The conjecture is that every finite projective plane
of order $n>1$ is a prime power.  The smallest integers $n>1$
that are {\it not} prime powers are
\[
6,~10,~12,~14,~15,~\dots
\]
The brute force approach to this conjecture is to eliminate each of
these possibilities in turn.  The case $n=6$ was settled in 1938.
Building on a number of theoretical advances~\cite{MST}, Lam eliminated the case
$n=10$ in 1989, in one of the most difficult computer proofs in
history~\cite{Lam89}.  This calculation was executed over
a period of years on multiple machines and eventually totaled about 2000
hours of Cray-1A time.  

Because of the possibilities of programming errors and soft errors
(see Section~\ref{sec:soft}), Lam was unwilling to call the result a
proof.  He writes, ``From personal experience, it is extremely easy to
make programming mistakes. We have taken many precautions, including
the use of two different programs to cross check selective sample
cases and the checking of internal consistency when isomorphism
testing is performed. Yet, I want to emphasize that this is only an
experimental result and it desperately needs an independent
verification, or better still, a theoretical
explanation''~\cite{LamS}.

Unlike the computer proof of the Four-Color theorem in 1976, the
projective plane proof has never received independent verification.
The original proof of the Four-Color Theorem also required about
$1200$ hours on an IBM 370-168, but that proof was eventually
simplfied~\cite{Robertson:1997:JCTB}, and then formally
verified~\cite{gonthier:2008:formal}.  Now the proof of the 4CT runs
in a few hours on an average computer.

% 4CT page 133-134 of MacKenzie, Mechanizing Proof.

Recent discussions at {\it Math Overflow}, which are more reliable
than tips on Twitter, suggest that the next case $n=12$ remains out of
computational reach~\cite{Horn}.

%% http://mathoverflow.net/questions/38632/projective-plane-of-order-12


\subsection{hyperbolic manifolds}
\subsection{strange attractors}


[XX Insert strange attractor graphic.]

\subsection{$4/3$}
Mandelbrot's conjectures in fractal geometry have resulted in two
Fields Medals.  Here he describes the discovery of the
$4/3$-conjecture made in~\cite{ManFN}.  

\bigskip

{

  \narrower\it 

  The notion that these conjectures might have been reached by pure
  thought -- with no picture -- is simply inconceivable.\dots

  I had my programmer draw a very big sample [Brownian] motion and
  proceeded to play with it. I was not trying to implement any
  preconceived idea, simply actively ``fishing'' for new things.\dots
  In order to achieve homogeneity, I decided to make the motion end
  where it had started. The resulting motion biting its own tail
  created a distinctive new shape I call Brownian cluster.  Next the
  same purely aesthetic consideration led to further processing. The
  continuing wish to eliminate extraneous complexity made me combine
  all the points that cannot be reached from infinity without crossing
  the Brownian cluster. Painting them in black sufficed, once again,
  to create something quite new, resembling an island.  Instantly, it
  became apparent that its boundary deserved to be investigated. Just
  as instantly, my long previous experience with the coastlines of
  actual islands on Earth came handy and made me suspect that the
  boundary of Brownian motion has a fractal dimension equal to $4/3$.
  (Mandelbrot \cite{Man})

}

\bigskip

This conjecture, which  Mandelbrot's trained eye spotted in an instant,
took 18 years to prove~\cite{LSW01}.

[XX Insert Brownian motion graphic.]

% Mandelbrot uses the phrase  ``visual inspection supported by computer experiments'', 
%http://users.math.yale.edu/users/mandelbrot/web_pdfs/EncycScienceAndTechnologyFractals.pdf
% use this phrase in a caption of Brownian motion image.
%
%
%Mandelbrot's 4/3 conjecture (1982), the Fractal Geometry of Nature.
%proved recently by Lawlor, Schramm, Werner.
%2001 proof.
% http://www.mrlonline.org/mrl/2001-008-004/2001-008-004-001.pdf

%% Mandelbrot quotation appears in the interview.
%% A theory of roughness.
%% http://www.edge.org/3rd_culture/mandelbrot04/mandelbrot04_index.html
%%



\subsection{visualizing geometry}

(MOVE)

Some say computers threaten to supplant genuine
understanding.  ``This invention will produce
forgetfulness in the minds of those who learn to use it.'' (Ph\ae
drus).  
% Plato (Phaedrus) Socrates quoting King Thamus of Egypt.
% quoted from 
% http://www.freeranger.com/chris/MM7.htm
It is the memoria technica or formula of unmatched power for the
organization of scientific knowledge.


\subsection{sphere eversion}

% http://en.wikipedia.org/wiki/Smale's_paradox

% http://torus.math.uiuc.edu/jms/Papers/isama/eversions.pdf Sullivan

%% Levy http://www.math.sunysb.edu/CDproject/OvUM/cool-links/www.geom.umn.edu/docs/outreach/oi/history.html

%%  Anthony Phillips, ``Turning a surface inside out'', Scientific American, May 1966, 112--120.

%% [Le] Silvio Levy. Making Waves: A Guide to the Ideas Behind "Outside In". AK Peters, Wellesley, MA, 1995.


Smale (1958) proved that it is possible to turn a sphere inside out
without introducing any creases.\footnote{I am fond of this example,
because The Scientific American
  article \cite{Phi66} about this theorem was my first exposure to ``real
  mathematics'' as a child.}  For a long time, this paradoxical result
defied the intuition of experts.  R. Bott, who had been Smale's
graduate advisor, refused to believe it at first.  Levy writes that
trying to visualize Smale's mathematical argument ``is akin to
describing what happens to the ingredients of a souffl\'e in minute
detail, down to the molecular chemistry, and expecting someone who has
never seen a souffl\'e to follow this `recipe' in preparing the
dish''~\cite{Le95}

[XX graphic of sphere eversion]

It is better to see and taste a souffl\'e first.  The computer videos
of this theorem are spectacular.  Watch them on YouTube!  As we watch
the sphere turn inside out, our intuition grows.  The computer
calculations behind the animations of the first video (the Optiverse)
start with a sphere, half inverted and half rightside out~\cite{SFL}.  From
halfway position, the path of steepest descent of an energy functional is
used to calculate the unfolding in both directions to the round spheres,
with one fully inverted.  The second video is based on Thurston's
``corrugations''~\cite{LMM}. As the name suggests, this sphere eversion has
undulating ruffles that dance like a jellyfish, but avoids sharp
creases.  I rest my case: through computers, understanding.

\subsection{minimal surface visualization} % (visualization)

Weber and Wolf \cite{WW11} report that the use of computer
visualization has become ``commonplace'' in minimal surface research,
``a conversation between visual aspects of the minimal surfaces and
advancing theory, each supporting the other.''  This started when
computer illustrations of the Costa surface (a particular minimal
surface) in the 1980s revealed dihedral symmetries of the surface that
were not seen directly from its defining equations.  The observation
of symmetry turned out to be the key to the proof that the Costa
surface is an embedding.  The symmetries further led to a conjecture
and then proof of the existence of other minimal surfaces of higher
genus with similar dihedral symmetries.  As Hoffman wrote about his discoveries, ``The
images produced along the way were the objects that we used to make
discoveries. They are an integral part of the process of doing
mathematics, not just a way to convey a discovery made without their
use'' \cite{Hoffman}.

[XX graphic of Costa surface]


\subsection{double bubble conjecture}

Closely related to minimal surfaces are surfaces of constant mean
curvature.  The mean curvature of a minimal surface is zero; surfaces
whose mean curvature is constant are a slight generalization.  They
arise as surfaces that are minimal subject to the constraint that
they enclose a region of fixed volume.  Soap bubble films are
surfaces of constant mean curvature.  

The isoperimetric inequality asserts that the sphere minimizes the
surface area among all surfaces that enclose a region of fixed volume.
The double bubble problem is the generalization of the isoperimetric
inequality to two enclosed volumes.  What is the surface minimizing way
to enclose and separate two regions of fixed volume?  In the
nineteenth century, Boys~\cite{Boy1890} and Plateau observed
experimentally that the answer should be two partial spherical bubbles
joined along a shared flat disk (Figure XX).  The size of the shared
disk is determined by the condition that angles should be $120^\circ$
where the three surfaces meet.  This is the {\it double bubble
  conjecture}.

The first case of double bubble conjecture to be established was that
of two equal volumes~\cite{HHS95}.  The proof was a combination of conventional
analysis and computer proof.  Conventional analysis (geometric measure
theory) established the existence of a minimizer and reduced the
possibilities to a small number of figures of revolution, and
computers were to analyze each of the cases, showing in each case by
interval analysis either that the case was not a local minimizer or
that its area was strictly larger than the double bubble.  Later
theorems proved the double bubble conjecture in the general unequal
volume case without the use of computers~\cite{HMRR}.

[XX graphic of double bubble]


\subsection{Kelvin problem}

[XX Insert Kelvin foam graphic.]



\subsection{kissing numbers} % (transcient use section)

[XX Insert 6 penny.]

In the plane, six pennies can be arranged in a hexagon so that they
all touch one more penny placed at the center of the hexagon.  Odlyzko
and Sloane, solved the corresponding problem in dimension $8$: at most
$240$ nonoverlapping congruent balls can be arranged so that they all
touch one more at the center.  Up to rotation, a unique arrangement of
$240$ exists.  A computer search turned up the polynomial
\[
,
\]
which has positive coefficients when expanded as a linear combination
of Gegenbauer polynomials.  With this special polynomial in hand, they
give a computer-free proof of their theorem based on the theory of
positive definite spherical functions.


\subsection{digression on $E_8$}

It is no coincidence that their calculation works in dimension $8$.
Wonderful things happen in eight dimensional space and again in $24$
dimensions (as string theorists are happy to explain).

Having mentioned the $240$ balls in eight dimensions, I cannot resist
mentioning some further computer proofs.  The centers of the $240$ balls
are vectors whose integral linear cominations generate 
a lattice in $\ring{R}^8$, known as the $E_8$ lattice.

There is a packing of congruent balls in eight dimensions that is
obtained by centering one ball at each vector in the $E_8$ lattice,
making the balls as large as possible without overlap.  Everyone
believes that this packing in eight dimensions is the densest
possible, but this fact defies proof.  Cohn and Kumar have a beautiful
computer assisted proof that the $E_8$ packing is the densest of all
{\it lattice} packings in $\ring{R}^8$ (and the corresponding result in
dimension $24$ for the Leech lattice).  That is, among all packings
whose centers form a lattice.  The proof is based on the Poisson
summation formula.  Pfender and Ziegler's account of this
computer-assisted proof won the Chauvenet Prize of the MAA for
writing~\cite{PZ}.

%% chauvenet. 2006.
%% http://mathdl.maa.org/mathDL/22/?pa=content&sa=viewDocument&nodeId=3065

[XX Insert graphic of $E_8$ Dynkin diagram.]

The $240$ vectors that generate the $E_8$ lattice are the {\it roots}
of a $240+8$ dimensonal Lie group (also called $E_8$); that is, a
differentiable manifold that has the analytic structure of a group.  All simple
Lie groups were classified in the nineteenth century.\footnote{I
  describe the families over $\ring{C}$.  Each complex Lie group has a
  finite number of further real forms.}  They fall into infinite
families named alphabetically, $A_n$, $B_n$, $C_n$, $D_n$, with $5$
more exceptional cases that do not fall into infinite familes $E_6$,
$E_7$ $E_8$, $F_4$, $G_2$.  The exceptional Lie group of highest
dimension is $E_8$.

The long-term {\it Atlas Project} aims to use computers to determine
all unitary representations of real reductive Lie groups~\cite{Atlas}.  The $19$-member team
(working in seamless harmony??)  focused on $E_8$ first, because
everyone respects $E_8$.  By 2007, a
computer had completed the character table of $E_8$.  Since there are
infinitely many irreducible characters and each character is an
analytic function on (an open dense subset of) the group, it is not
clear without much further explanation what it might even mean for a 
computer to output the character table as a $60$ gigabyte file.  What is
significant about this work is that it brings the computer to bear on
some abstract parts of mathematics that have been traditionally
largely beyond the reach of concrete computational description,
including infinite dimensional representations of Lie groups,
intersection cohomology and perverse sheaves.  Vogan's account of this
computational project was awarded the 2011 Conant Prize of the AMS~\cite{VE8}.

%Leeuwen's slides describe how a heavily recursive calculation requiring 160 GB
%of RAM was squeezed into 64 GB RAN. 

% XX Give NYT citation.
% Vogan, The character table for E8.
% E8 paper.
% http://atlas.math.umd.edu/software/
% cite (Marc Leuwen, computational aspects).
% http://atlas.math.umd.edu/talks/cracking-E8.pdf
% from 160 GB to 64 GB.
% http://young.sp2mi.univ-poitiers.fr/~marc/pdf/Zaragoza-E8.pdf

% 60 gb comes from http://atlas.math.umd.edu/talks/boston.pdf
% it also says 77 hours.






\subsection{further digression to $P$ versus $NP$}

While on the topic of computation, Kazhdan-Lusztig polynomials (which
lie at the core of the $E_8$ character table calculation), and
representation theory, I cannot resist a further digression into the
$P$ versus $NP$ problem.  This is the most fundamental unsolved
problem in mathematics and the most significant unsolved problem in
computer science.  Smale calls it computer science's ``gift to
mathematics.''  In approximate terms, the problem asserts that there
exists a class of problems that can be verified in polynomial time
once you are shown the solution, but that cannot be solved in
polynomial time without seeing the solution.

I regard the $P$ versus $NP$ problem as ``obviously'' independent of
the axioms of ZFC, and consequently do not put much faith in efforts
to prove it.  But if I were to place faith somewhere, it would be in
Mulmuley's program in {\it geometric complexity theory} that uses
geometric invariant theory and representation theory to understand
complexity classes.  According to the complexity taxonomists, there is
a large zoo of compexity classes.  Mulmuley's work treats a relative
of the $P$ versus $NP$ problem called the $GP$ versus $GNP$
problem.  % XX give the right names.

The idea goes something like this.  To prove that $GNP\ne GP$, it is
enough to demonstrate that some $GNP$ problem is not in $GP$.  Why
not pick a problem that has a rich mathematical structure?  Mulmuley
chooses the permanent problem: show that the computation of permanents
(a determinant in which all the signs $-1$ are flipped to $+1$, which
are $GNP$-hard to compute) cannot be reduced to the computation of
determinants of possibly larger matrices (which are computed in
polynomial time).  Both the permanent $P$ and determinant $D$ are
homogeneous polynomials in $n^2$ variables.  The group $GL(n^2)$ acts
on the vector space of all homogeneous polynomials.  The geometric
content of the $GNP\ne GP$ problem boils down to showing that the
closure $C_D$ of the orbit of $D$ under this action is not contained
in the closure $C_P$ of the orbit of $P$.  Mulmuley proposes that we
compare the irreducible constituents of the action of $GL$ on the
coordinate rings of $C_P$ and $C_D$.  An irreducible representation that
appears in the coordinate ring of one but not the other is a certificate that
the two complexity classes $GNP$ and $GP$ are different.
He has successfully pushed this strategy through under various limiting
hypotheses.

% CITE this paper by Regan, It looks clearer than Mulmuley.
% http://www.math.cas.cz/~pudlak/alg-geom/gctulm.pdf


\section{Mathematical Software}

(Give broad categories and one-line programs. Use ICMS proceedings)


\section{Formal Proof}

Proof assistants represent the best effort of logicians, computer scientists, and mathematicians
to obtain complete mathematical rigor by computer.

Gonthier, Types, Agda, HOL,


\subsection{Excluded Middle}

XX BOX.

The logic of HOL Light is intuitionistic until the axiom of choice is introduced.
By a result of Diononescu,
The law extensionality and choice imply the law of excluded middle and make
the logic classical (See BOX.) \cite{Bee}

Here is a proof of the law of excluded middle $\phi\lor \lnot\phi$.  By
the axiom of choice, we may pick an element $x_1$ that belongs to the nonempty set
of booleans
\[
P_1 = \{ x \mid (x = \text{false}) \lor ((x = \text{true}) \land \phi)\},
\]
and an element $x_2$ that belongs to the nonempty set of booleans
\[
P_2  = \{x \mid (x = \text{true}) \lor ((x = \text{false}) \land \phi)\}.
\]
The sets are evidently nonempty, because $\false\in P_1$ and $\true\in P_2$.
By construction, we have the relations
\begin{align*}
x_1\in P_1,&\qquad x_2\in P_2,\\
(x_1=\text{false})\lor (x_1 = \text{true}), &\qquad (x_2=\text{false})\lor (x_2 = \text{true}). 
\end{align*}
We may break the proof of the excluded middle into four cases, depending on the
truth values of $x_1$ and $x_2$.

{\bf Cases $(x_1,x_2)=(\true,\true),$ $(x_1,x_2)=(\true,\false)$}: 
By the definition of $P_1$, if $x_1 = \true$ then $\phi$, so $\phi\lor \lnot\phi$.
{\bf Case $(x_1,x_2)=(\false,\false)$}: By the definition of $P_2$,
if $x_2=\false$ then $\phi$, so also $\phi\lor\lnot\phi$.
{\bf Case $(x_1,x_2)=(\false,\true)$}:  If $\phi$, then  $P_1=P_2$, and the choices
$x_1$ and $x_2$ reduce to a single choice $x_1=x_2$, which contradicts $(x_1,x_2)=(\false,\true)$.  Hence $\phi$ implies $\false$; 
which by the definition of negation gives $\lnot\phi$, so also $\phi\lor\lnot\phi$.

XX END BOX.



\subsection{Proof Tactics in HOL Light}

In HOL Light, the predominant proof style is goal directed in an
interactive session with the proof assistant.  The user starts by
typing the statement to be proved.  This is the goal.  Then a command
is issued that reduces the goal to one or more simpler goals.  Further
commands reduce these simpler goals to simpler ones still.  This is
continued until nothing remains except goals that are trivial to
prove.  The proof is completed in this manner.

Each command that reduces a goal to one or more simpler goals it called a 
{\it tactic}.  There are about 100 different commands in the system related
to tactics, either the tactics themselves or operators on tactics (called tacticals) that modify the behavior of tactics.  However, the same few tactics tend to be used over and over again:  just twenty of these commands get 85\% of the usage.

Here is an example of a simple proof, starting with goals and
progressing with tactics.  Let us prove the trivial fact that an infinite set
is not empty.   We start with a statement of this goal, expressed in the syntax of HOL Light:
\[
\bq \forall~ S.~~ \text{INFINITE}~ S~ \Longrightarrow (\exists (x:A).~ x~IN~S)`
\]

The most commonly used tactic in the system is {\tt REWRITE\_TAC}.  It takes one or
more theorems that have the form of an equality $a=b$ or biconditional $a \Leftrightarrow b$, and replaces every occurence
of $a$ in the goal with $b$.  The theorem named {\tt INFINITE} has the form of
a biconditional and states that a set is infinite if and only if it is not finite.
The tactic
\[
\text{REWRITE\_TAC[INFINITE]}
\]
reduces the goal to
\[
\bq \forall~ S.~~ \lnot\text{FINITE}~ S~ \Longrightarrow (\exists x.~ x~\text{IN}~S)`
\]
A tactic that is frequently used at the beginning of a proof is {\tt STRIP\_TAC}.
The goal is generally presented in a bundled form, with each variable universally
quantified.  It is more convenient to remove the universal quantifiers and
move the antecedents of the goal to a separate list of assumptions.  We can
accomplish all this with a single command.
\[
\text{REPEAT STRIP\_TAC}
\]
{\tt REPEAT} is an example of a tactical, an operator acting on tactics.  This 
tactical causes the tactic to repeat as many times as possible. 
This is an example of a bookkeeping tactic;  without doing any significant theorem
proving, it breaks the assumption and conclusion apart:
\begin{align*}
  &0~[\bq\lnot\text{FINITE}~ S\bq]\\
  &`\exists~ x.~ x~ \text{IN}~ S`\\
\end{align*}
We will finish the proof by using two more theorems that are already in the system.
The first, {\tt FINITE\_EMPTY} asserts that the empty set is finite.  The second,
{\tt EMPTY\_EXISTS} asserts that a set is not equal to the empty set if and only if
it has a member:
\[
\text{EMPTY\_EXISTS:}\qquad |- \forall X.~\lnot(X = \{\}) \Leftrightarrow (\exists u.~u~\text{IN}~X)
\]
The current goal is evidently a logical consequence of these two theorems, but it is not convenient to use the rewrite tactic.  Instead we use the tactic {\tt ASM\_MESON\_TAC},
which completely finishes off a goal through first-order logical reasoning.  The command
\[
\text{ASM\_MESON\_TAC[FINITE\_EMPTY;EMPTY\_EXISTS]}
\]
completes the proof.  As we look over the tactics used in the proof,
we see that everything we did was a logical consequence of three
theorems that were already in the system.  We can now redo the proof
in a single step:
\[
\text{ASM\_MESON\_TAC[INFINITE;FINITE\_EMPTY;EMPTY\_EXISTS]}
\]
Another way to do the proof in a single line is the use the {\tt THEN} command,
which combines two tactics into one:
\[
\text{REWRITE\_TAC[INFINITE] THEN ASM\_MESON\_TAC[FINITE\_EMPTY;EMPTY\_EXISTS]}
\]
Here is a table of the most common comands related to tactics and their relative 
usage rates.  We have already introduced several of them.
\[
\begin{array}{llr}
\text{\bf name}  &~\text{\bf purpose} &\text{\bf usage rate}\\
\text{THEN\dots }   &~\text{combine two tactics into one}   & 37.2\%\\
\text{REWRITE \dots} &~\text{use $a=b$ to replace $a$ with $b$ throughout goal} & 14.5\%\\
\text{MP\_TAC} &~\text{introduce a known theorem to the goal context} &4.0\%\\
\text{SIMP\_TAC \dots}&~\text{rewriting with conditionals} & 3.1\%\\
\text{MATCH\_MP\_TAC} &~\text{reduce a goal $b$ to $a$, given a theorem $a\Longrightarrow b$}& 3.0\%\\
\text{STRIP\_TAC} &~\text{(bookkeeping) unpackage a bundled goal} & 2.9\%\\
\text{MESON\_TAC \dots}&~\text{apply first-order reasoning to solve the goal} & 2.6\%\\
\text{REPEAT} &~\text{repeat a tactic as many times as possible} & 2.5\%\\
\text{DISCH\_TAC \dots}&~\text{(bookkeeping) move hypothesis to the assumption list\!\!\!} & 2.3\%\\
\text{EXISTS\_TAC}&~\text{instantiate an existential goal $\exists x\dots$}& 2.3\%\\
\text{GEN\_TAC}&~\text{instantiate a universal goal $\forall x\dots$}& 1.4\%
\end{array}
\]
The $\dots$ indicate that the command has variants.  The table shows
that HOL Light is heavily used as a rewriting system.  Induction over
the natural numbers, which gets a great deal of attention in academic
publications and tutorials, accounts for a mere $0.1\%$ of the usage.

The meson tactic {\tt MESON\_TAC} and its variant {\tt
  ASM\_MESON\_TAC} are two of the most powerful tactics in the system.
  The meson decision procedure (an
acronym for Model Elimination, Subgoal Oriented) is due to Loveland
(1978) and can be briefly described as follows.  To prove that a set
of hypotheses $S$ implies a goal $g$, it is enough to show that the
union $S' = S \cup \{\lnot g\}$ is unsatisfiable.  There are standard
transformations (skolemization of formulas in prenex form) that strip
quantifiers from terms in $S'$.  Further transformations (splitting
conjuncts of the conjunctive normal form into separate entries of
$S'$) reduce $S'$ to a set of clauses (which by definition are
disjuncts of literals).  These standard transformations show that it
is enough to give an algorithm for the unsatisfiability of a finite
set of clauses in first order logic.

Following \cite{Ha09}, we describe the workings of the algorithm in propositional
logic.  Let  $C$ be a finite set of clauses and $L$ a set of literals.
It is based on a few simple logical rules.
\begin{enumerate}
\item If $L=\emptyset$ and if $c = \lnot P_1 \lor \cdots\lor \lnot
  P_n$ is a clause in $C$ where each $P_i$ is atomic, then $C$ is
  unsatisfiable provided the sets $L_i\cup C'$ are all unsatisfiable,
  where $L' =\{\lnot P_i\}$ and $C'= \cup (C\setminus \{c\})$.
\item If $P,\lnot P\in L$, with $P$ the element most recently added to $L$,
then $L\cup C$
is unsatisfiable.
\item If $P\in L$ is the most recently added element to $L$, and
  $c=\lnot P \lor Q_1 \lor \cdots\lor Q_n\in C$, then $L\cup C$ is
  unsatisfiable provided the following sets are unsatisfiable:
\begin{equation}\label{eqn:cup}
L' \cup C',\quad L'=\{Q_i\},\quad C' = (C\setminus \{c\}).
\end{equation}
\end{enumerate}
Starting with $L=\emptyset$ and $C=S'$, 
it is clear that these rules iteratively 
reduce the number of clauses in the sets $C$ and
hence must eventually terminate.  It can be proved that if the first rule
that applies is always applied, then as long as $C$ is nonempty, some rule
applies.



Returning to the first-order predicate logic, the algorithm proceeds
in the same way, but in the third reduction, the formula $c$ may have
the form $c=\lnot P' \lor \cdots$, where $P'$ can be {\it unified}
with $P$.\footnote{In the third step, it is enough to find some $c$
  and $P'$ that unifies with $P$, for which the sets (\ref{eqn:cup})
  are inconsistent.  A backtracking algorithm can run through the
  possibilities for $c$ and $P'$ until one is found that works.  The
  unification is a global constraint.}  I will not define unification
(see~\cite{Ha09}), but the concept is intuitive enough.  For
example, the atoms
\[
g(f(x),y) \text{\quad and \quad} g(u,h(z))
\]
are unequal, but unify to
\[
g(f(x),h(z)).
\]
Numerous optimizations are made in the implementation of meson to
reduce the number of possibilities~\cite{harrison:meson}.  
Further extensions of the method are required to make meson work with
equality and with higher-order logic.  
The details of the 
procedure are described in \cite[Sec.~3.15]{Ha09}.

Harrison devotes and entire chapter
of his book to issues of automated reasoning with equality.
Methods include paramodulation, adding axioms that characterize the equality
relation, and transforming the goal to eliminate equality.
% The standard approaches to including equality are as follows. XX. axioms. 
% paramodulation.

%% Optimizing Proof Search in Model Elimination, Harrison, 1996, cite.

\subsection{First-order theorem provers}

Meson is an example of a first-order theorem prover.   

The best known procedure for first-order proving is {\it resolution}.
The basic resolution inference rule is that from the two clauses:
\[
P \lor A_1 \lor \cdots \lor A_i,\qquad P' \lor B_1 \lor \cdots\lor B_j,
\]
where $P' = \lnot P$,
follows the clause
\begin{equation}\label{eqn:uni}
A_1\lor \cdots \lor A_i \lor B_1 \lor \cdots \lor B_j.
\end{equation}
(As usual, this is the rule in proposition logic; in first-order
predicate logic, instead of setting terms $P'$, $\lnot P$ equal,
we take their most general unification,  and
replace (\ref{eqn:uni}) with its unified form.)


XX BOX.
Here is a resolution proof of the proposition.  We eliminate equality by
introducing a new predicate $e$ whose intended interpretation is 
\[
e(X) \Leftrightarrow X \text{~is the empty set.}
\]
We also need a substitution axiom for the new predicate
\[
e(X) \land e(Y) \land \text{INFINITE}~X \Longrightarrow \text{INFINITE}~Y.
\]
The left column lists the axioms and the right column lists the resolvants.
The predicates {\tt INFINITE} and {\tt FINITE} are abbreviated to $i$ and $f$.
The constant for the infinite set is $c$. The propositions have been converted
to clauses.
\[
\begin{array}{rl}
&\text{\tt(EQUALITY AXIOM)}\quad e(\emptyset)\\
&\text{\tt(EMPTY EXISTS)}\quad e(X) \lor \lnot e(\emptyset) \lnot u(X)\in X\\
e(X) \lor u(X)\in X &\\
&\text{\tt(NEGATED GOAL)}\quad\lnot (t\in c)\\
e(c)&\\
&\text{\tt(EQUALITY AXIOM)}\quad\lnot e(X) \lor \lnot e(Y) \lor \lnot i(X) \lor i(Y) \\
 \lnot e(Y) \lor \lnot i(c) \lor  i(Y)&\\
&\text{\tt(ASSUMPTION)}\quad i(c) \\
\lnot e(Y) \lor i(Y)&\\
&\text{\tt (EQUALITY AXIOM)}\quad e(\emptyset) \\
i(\emptyset) &\\
&\text{\tt (INFINITE)}\quad \lnot i(X) \lor \lnot f(X)\\
\lnot f(\emptyset) &\\
&\text{\tt (FINITE EMPTY)}\quad f(\emptyset)\\
\perp
\end{array}
\]

XX BOX.
Here is a meson proof of the same proposition.  We preprocess as we did
with the resolution proof. At each step we keep track of the set $L$ of literals, the
set of axioms removed from $C$, and the set of global unifications, and the
steps at which the entry gets eliminated.
\[
\begin{array}{lllll}
& L & \text{removed} & \text{global unifications}\\
0&\emptyset & \emptyset & \emptyset & 1\\
1&e(\emptyset) & \text{\tt(EQUALITY AXIOM)} & \emptyset & 2,3 \\
2&e(\emptyset), e(X) & \text{\tt EQUALITY AXIOM, EMPTY EXISTS} & \\ 
3& & & \\
3& & & \\
3& & & \\
3& & & \\
3& & & \\
3& & & \\
3& & & \\
3& & & 
\end{array}
\]


Writing about resolution and unification, Huet and Paulin-Mohring
(2003) \cite{Coq} describe the situation in the early 1970s as a
``catastrophic state of the art.''  ``The standard mode of use was to
enter a conjecture and wait for the computer's memory to exhaust its
capacity.  Answers were obtained only in exceptionally trivial
cases.'' %\footnote{-tr. T. Hales}
%\footnote{Le mode standard d'utilisation \'etait de rentrer sa conjecture et
%d'attendre que la m\'emoire de l'ordinateur soit satur\'ee.  Seulement dans des cas
%exceptionnellement triviaux une r\'eponse \'etait obtenue.''
They go on to describe numerous developments (Knuth-Bendix, LISP,
rewriting technologies, LCF, ML, Martin-L\"of type theory, NuPrl,
Curry-Howard correspondence, dependent types, etc.) that led up to the
Coq proof assistant.  These developments led away from first-order
theorem proving with its ``thousands of unreadable logical
consequences'' to a highly structured approach to theorem proving in Coq.


Returning to our discussion of first-order theorem provers,
as we mentioned above, to prove a set $S$ of clauses
unsatisfiable, repeatedly resolve and unify clauses until an empty
clause is derived.  More generally, to prove a set $S$ of formulas
unsatisfible, a set of logical rules of inference $I$ other than
resolution may be used; the rules may be repeatedly applied until the
set becomes saturated; that is, nothing further can be derived from
the give rules; the empty clause in the saturated set attests
the unsatisifiability of the original set $S$. 
This is the process followed by a saturation theorem provers.

% Voronkov's lectures 1-5. on desktop, Thackmac.

First-order theorem proving has developed significantly over the years
into sophisticated software products.  They are no longer limited to
``exceptionally limited cases.''  There are now many different
software products that compete in an annual competition (CASC), to see
which can solve difficult first-order problems the fastest.  The LTB
(large theory batch) division of the competition includes problems
with thousands of axioms~\cite{PSST}.  Significantly, this is the same order of
magnitude as the total number of theorems in a proof assistant.  What
this means is that a first-order theorem provers have reached the
stage of development that they might be able to give fully automated
proofs of new theorems in a proof assistant, working from the full
library of previously proved theorems.

% Ref. PSST=CASC_ desktop Thackmac.

\subsection{Sledgehammer}

The Sledgehammer tactic is Paulson's implementation of this idea in
the Isabelle/HOL proof system.~\cite{Paar}  As the name `Sledgehammer' suggests,
the tactic is all-purpose and powerful, but demolishes all higher
mathematical structure, treating every goal as a massive unstructured
problem in first-order logic.  If $L$ is the set of all theorems in
the Isabelle/HOL library, and $g$ is a goal, it would be possible hand
off the problem $L\Longrightarrow g$ to a first-order theorem
prover.  However, success rates are dramatically improved, when the
theorems in $L$ are first assessed by heuristic rules for their likely
relevance for the goal $g$.  This process is called {\it relevance
  filtering}.  The heuristic relevance filtering rules are currently
very simple rules, such as a theorem is in $L$ is more likely to be
relevant for the proof of $g$, if $L$ and $g$ have some constants in
common.  Relevance filtering is used to reduce $L$ to an axiom set $L'$ of a
few hundred theorems that are deemed most likely to prove $g$.

The typed higher-order problem $L'\Longrightarrow g$ is then stripped
of type information, converted to a first-order, and fed
 in parallel into several first-order theorem provers.  
%According to B\"ohme and Nipkow, on average
%``Running each of the 3 provers for 5 [seconds] yields the same M-success rate
%(44\%) as running the most effective one [Vampire] for 120 [seconds].''
 ``B\"ohme and Nipkow \cite{Boehme-Nipkow-IJCAR10} have demonstrated that
running three different [first-order] theorem provers (E, SPASS and
Vampire) for five seconds solves as many problems as running the
best theorem prover (Vampire) for two full minutes.  It would be
better to utilise even more theorem provers'' 
\cite{Paar}.
% Paulson, paar. Desktop Thackmac.
When luck runs in your favor, one of the first-order theorem provers finds
a proof.

It was a technological challenge to build a mechanism to reconstruct
formal proof from a first-order proof.  For one thing, when type
information is stripped from the problem (which is done to improve
performance), soundness is lost.  ``In unpublished work by Urban,
MaLARea [a machine learning program for relevance ranking] easily
proved the full Sledgehammer test suite by identifying an
inconsistency in the translated lemma library; once MaLARea had found
the inconsistency in one proof, it easily found it in all the
others'' \cite{Paar},~\cite{UrM} 
% Paulson, paar.
% cite: MaLARea: a metasystem for automated reasoning in large theories, Urban
Even when the first-order proof is valid, Hurd and Paulson found that
in most cases, the first-order proofs were not explicit enough to permit a
reconstruction of
a formal proof.
%``Hurd [7] had noticed that
%the proofs delivered by automatic theorem provers (Gandalf, in his
%case) were not detailed and explicit enough. We made the same
%discovery [14] (in the case of SPASS), and despite considerable
%efforts, were only able to reconstruct a handful of
%proofs.'' % Paulson, paar.

Instead, once a first-order proof of the goal $g$ is found with axiom
set $L'$.  The theorem prover is repeatedly called on smaller and
smaller axiom sets $L''\subset L'$ until a small set of axioms is
identified that prove the goal $g$.  Ideally, this process reduces the
size of the axiom set from the hundreds in $L'$ to a set $L''$ of only
a few.  In other words, the first-order prover is used as nothing more
than a massive relevance filter, to identify a small set of theorems
in the Isabelle/HOL library that imply the goal $g$.  The manageably
sized set $L''$ is then passed to the metis tactic\footnote{Metis is a
  15K line program in SML that replaces the meson prover described
  above~\cite{Metis}.} in Isabelle/HOL, which independently constructs a proof
$L''\Longrightarrow g$.  

%T http://www.gilith.com/software/metis/notes.html  Metis.

B\"ohme and Nipkow \cite{Boehme-Nipkow-IJCAR10} have tested the success rates of the
sledgehammer tactic on a large test suite.  He took 1240 proof goals
that appear in several diverse theories of the Isabelle/HOL system and
ran sledgehammer on all of them. The results are truly
astounding. The success rate (of obtaining fully reconstructed proofs
by metis inside the proof assistant) when three different first-order
provers run for two-minutes each was 48\%.  The proofs of these same
goals by hand might represent years of human labor, now fully
automated through a single new tool.
% XX check years claim. 

% http://isabelle.in.tum.de/~nipkow/pubs/ijcar10.pdf Nipkow.

Sledgehammer has led to a new style of theorem proving, in which the
user is primarily responsible for stating the goals.  In the final
proof script, there is no explicit mention of sledgehammer.  Metis
proves the goals, with sledgehammer operating silently in the
background to feed metis with whatever theorems it needs.
For example, a typical proof script might contain  lines such as \cite{Paar}
\[
\begin{array}{ll}
\text{{\bf hence} ``$x \subseteq \text{space}~M$''}\\
\text{~~~{\bf by} (metis sets into space lambda system sets)}&
\end{array}
\]
The first line is the goal provided the user. The second line has been
automatically inserted into the proof script by the system, with
theorems {\tt sets, into, \dots} selected by Sledgehammer.


% Paulson (3-years paar, Desktop, Thackmac.)




\subsection{lemma overload.}

One annoyance of formal proof systems is that the number of theorems
is overwhelming.  What goes by the name of theorem includes
the usual big ticket items such as the Four-Color Theorem, 
fundamental theorem of calculus, prime number
theorem, and Brouwer fixed-point formula, but also includes thousands
of trivialities\footnote{The ampersand \& in the second formula is a token
that marks $0$ as a real number, as oppsed to a natural number. In future
examples, I suppress the idiosyncracies of the computer notation.} such as
\begin{align*}
\text{\tt INTER\_COMM} & &\forall s\ t.\ s \cap t = t \cap s\\
\text{\tt REAL\_LE\_SQUARE} & &\forall x.\ \ \text{\&}0 \le x * x
\end{align*}

Each triviality carries a name and must be quoted by name whenever
used.  At the last count the proof system HOL Light had about $14,000$
theorems.  Larger developments, such as Mizar, have about twice as
many theorems.  Quoting these facts by name rather than working at the
level of the primitive axioms of the system is a step in the right
direction, but far more automation is needed.

What relief is in sight?

First is search.  To find the exact name of the fundamental
theorem of arithmetic, I search for theorems whose name contains the
letter sequence FUND and whose statement contains the term $\bq\text{prime}\bq$:
\[
\text{search[name="FUND";\bq prime \bq]}.
\]
%When I need to normalize a vector $u\in\ring{R}^3$ to unit length, I
%search for theorems containing  $\bq\text{norm}~u=1\bq$ to
%see what results have already been contributed to the system.
To search for all theorems in the system about associative binary operators, I type
\[
\text{search[\bq f(x,f(y,z)) = f(f(x,y),z)\bq]}
\]

Second, powerful automated procedures are being developed that are
capable of solving a general classes of problems.  For example, the
{\it REAL\_RING} command of HOL Light fully automates the proof of a
large class of identities over the real numbers.

The {REAL\_RING} command is capable of proving any system of
equalities and inequalities that holds over an arbitrary integral
domain.  For example, I can give a one-line formal proof of an isogeny
$(x_1,y_1) \mapsto (x_2,y_2)$ of elliptic curves: if we have
\begin{align*}
y_1^2 &= 1 + a x_1^2 + b x_1^4,\\
x_2 y_1&=x_1,\\
y_2 y_1^2&=(1 - b_1^4),\\
y_1&\ne 0
\end{align*}
then $(x_2,y_2)$ satisfies
\[
y_2^2 = 1 + a' x_2^2 + b' x_2^4,
\]
where $a' = -2a$ and $b' = a^2 - 4b$.  The automated formal proof of this identity
takes $0.2$ seconds on my laptop, and the input of the statement is as economical
as what I have written here.  We expect computer algebra systems to be capable
of checking identities like this, but to my amazement,
I found it {\it easier} to check this isogeny in HOL Light than
to check it in {\it Mathematica}.

The algorithm {\tt REAL\_RING} works in the following manner.  A universally
quantified system of equalities and inequalities holds over all integral
domains if and only if it holds over all fields.  
By putting the formula
in conjunctive normal form, it is enough to prove a finite collection of
identities of the form:
\begin{equation}\label{eqn:q}
(p_1(x)=0) \lor \cdot (p_n(x)=0) \lor (q_1(x)\ne 0) \lor (q_k(x)\ne 0).
\end{equation}
An element in a field is zero, if and only if it is not a unit.  Thus we may
rewrite each equality $p_i(x)=0$ as an equivalent inequality $1-p_i(x)z_i\ne 0$.
Thus, without loss of generality, we may assume that $n=0$; so that all disjuncts
are inequalities.  The formula (\ref{eqn:q}) is logically equivalent to
\[
(q_1(x) =0) \land \cdots \land (q_k(x) = 0) \Longrightarrow \text{false}.
\]
In other words, it is enough to prove that 
the ideal $I=(q_1,\ldots,q_n)$ has no common zero in any field.  For this,
we may use Gr\"obner bases  to prove that $1\in I$, to certify that
there is no common zero.  



%% REAL_RING `a' = &2 * a /\ b' = a*a - &4 * b /\ x2 * y1 = x1 /\ y2 * y1 pow 2 = &1 - b * x1 pow 4 /\ y1 pow 2 = &1 + a * x1 pow 2 + b * x1 pow 4 /\ ~(y1 = &0) ==> y2 pow 2 = &1 - a' * x2 pow 2 + b' * x2 pow 4`;;

Users can contribute their own procedures.  For example, Harrison has
formally proved all the basic rules of differential calculus: the
chain rule, product rule, quotient rule, and so forth.  I wrote a
little procedure that combines these rules. The function 
takes two arguments: the function to be differentiated (with a distinguished variable
marked with a $\lambda$) and  the point at which the deriviate is to be evaluated.
By 
entering
\[
\text{differentiate}~~  (\bq\lambda t.\ 1/(\cos(a  t^2)\bq)
~~(\bq b+1\bq)
\]
% let differentiate f x = Calc_derivative.differentiate f x `(:real)`;;
% differentiate `\t. &1/(cos (a * t pow 2))` `b+ &1`;;
%   |- derived_form (~(cos (a * (b + &1) pow 2) = &0))      (\t. &1 / cos (a * t pow 2))      (--((a * &2 * (b + &1) pow 1) * --sin (a * (b + &1) pow 2)) /       cos (a * (b + &1) pow 2) pow 2)      (b + &1)  (:real)
a theorem is generated automatically that states that the derivative of $1/\cos(at^2)$ 
with respect to $t$, evaluated at $(b+1)$ is
\[
  -((a\,\,  2\,\,  (b + 1) ^1) ( -\sin (a  (b + 1)^2))) /
      \cos (a  (b + 1) ^2 ) ^2,
\]
the derivative in unsimplified form.
The generated theorem includes, as it must, the
assumption $\cos(a (b+1)^2)\ne 0$ needed to justify the conclusion.  This
is in sharp contrast with the default behavior of computer algebra
systems that neglect the justifying assumptions.  

Ultimately this calculation
of a derivative in the proof assistant (which takes only $10^{-5}$ second)
is based on thousands of proofs in the system,
constructing the natural numbers from the axiom of infinity, constructing the
real numbers (by a variant of the usual Cauchy sequence construction), complex
analysis (to define the cosine as the real part of the complex exponential $e^{i x}$),
infinite sums to define the exponential,
$\delta$-$\epsilon$ treatment of limits, and so forth.  The proof assistant takes
no shortcuts whatsover.  Every minor fact is justified.

A graduate student Solovyev has contributed a procedure that generates proofs
of solutions to linear programming problems. XX finish.

\subsection{overloaded lemmas}

% How to make ad hoc proof automation less ad hoc. Gonthier et al.

Theorems retain various advantages over automated proof procedures.  
As mentioned above, the list of all theorems can be easily searched by name or by
matching subterm.  We can search for the fundamental theorem of arithmetic, but we
have no way to search for the procedure {\it REAL\_RING}.  We learn its existence and
usage by reading the user manual.  If a user contributes a new proof procedure, we
have no systematic way to find out about it.

Every theorem is self-documenting; the statement of the theorem and
its supporting definitions give me all that I need to use it. As much
as mathematicians take pride in understanding the proofs of theorems
they cite, this is not a logical necessity.  In classical logic, this
property of theorems is called proof irrelevance.  By contrast, the
meaning of a proof procedure is ultimately the body of computer code
that implements it.  If we are lucky, the code is clean and
well-documented.  When out of luck, we experiment and guess from their
names what the procedures
{\it sledgehammer}, {\it blast}, and {\it meson} can do.

Finally, theorems are composable in ways that proof procedures are not.  They
can be used in simplification, rewrites, and model elimination (meson).

In a move toward making proof assistants more useable, Gonthier et al.
have developed {\it overloaded lemmas}.  These are objects that have
the look and feel of theorems (in fact they are theorems), but behave
like an automated proof procedure, giving the best of both worlds.
A simple example is the overloaded lemma
\[
\text{for all natural numbers}~m~n.~m+n  = \text{\it computed-answer.}
\]
When we instantiate the variables $m$ and $n$ to $1$ and $2$ we get
the theorem $1+2=3$, and when we instantiate the variables to $2$ and
$3$ we get the theorem $2+3=5$.  The point is that instantiation
triggers a procedure that computes the sum $m+n$ and fills the open
slot {\it computed-answer} with the result.  They show that by playing
games with dependent types,
instantiation of variables can trigger arbitrarily complex procedures
that fill the open slots of a theorem.



\subsection{modular formalization of Finite Group Theory}

Gonthier's recent work can be found at
\cite{gonSSRE}, \cite{gonPFSF}, \cite{gonPMS}, \cite{gonMF},
\cite{gonHoc}, \cite{gonISSR}, \cite{gonC}.

The Feit-Thompson theorem, or odd-order theorem as it is sometimes
called, is one of the most significant theorems of the twentieth
century.  (For his work, Thompson was awarded the three highest forms
forms of recognition in the mathematical world: the Fields Medal, the
Abel Prize, and the Wolf Prize.)  The Feit-Thompson theorem states
that every finite simple group has even order, except for cyclic
groups of prime order.  Although the statement is very simple, the
proof is extremely technical and runs about 250 pages. It relies on
numerous other theorems in group and character theory.  The
Feit-Thompson theorem was the key ingredient in the classification of
all finite simple groups, a monumental undertaking that engulfed an
entire generation of group theorists.

% Peterfalvi, Thomas (2000), Character theory for the odd order theorem, London Mathematical Society Lecture Note Series, 272, Cambridge University Press, ISBN 978-0-521-64660-4, MR1747393

% Bender, Helmut; Glauberman, George (1994), Local analysis for the odd order theorem, London Mathematical Society Lecture Note Series, 188, Cambridge University Press, ISBN 978-0-521-45716-3, MR1311244

From a strictly mathematical point of view, the most significant
current development in the world of formal proofs is Gonthier's team
project to formalize the Feit-Thompson theorem.\footnote{As of January
  2011, the project was about half complete, having completed the
  formalization of \cite{BG94} but not yet \cite{P00}.}  Why does this
formalization project have particular historical significance?  

(1) It works out the formal architecture of abstract algebraic
structures.  (He does a beautiful job of it too, going much further
than earlier formalization projects.)  Of course,  the
formal architecture of abstract algebraic structures has already been
worked out by a long line of mathematicians, including Noether, van
der Waerden, and Bourbaki.  Nevertheless, the reimplementation of
abstract algebra by computer in the framework of
%the Calculus of Inductive Constructions 
Coq's dependent type system represents the single largest
transformation of the subject since category theory.  I can imagine a
future generation of textbooks in abstract algebra for math majors
that are based on this work.

%  Earlier formalization projects in abstract algebra did not go much beyond Sylow's first theorem.

 Significantly, it shows how to get multiple abstract structures
to work coherently together in a formal setting.  As Gonthier
correctly observes, when it comes to the formal development of
abstract algebra, ``the problem is not much in capturing the semantics
of each individual construct but rather in having all the concepts
working together well.'' % p14. Mod. Form.

(2) People in the world of formal proofs tend to be drawn into
formalizing results that have elegant self-contained proofs and to
follow the published proofs as closely as possible.  Some even feel a
duty to remain faithful to the published text.  The theorems tend to
be ones that have been worked over by mathematicians for a century or
so.  By contrast, Gonthier is drawn to major theorems with a
forbidding allure, and uses the power of computation to tame them.

The definition of a finite group in Coq is similar to the textbook definition,
expressed in types and structures.
\bigskip
{

\obeylines\tt
Structure~finGroupType~Type~:= FinGroupType \{
~~~element~:>~finType;
~~~~~~~1~:~element;
~~~~~~\hskip0.8mm ${}^{-1}$~:~element $\to$ element;
~~~~~~~*~:~element $\to$ element $\to$ element;
~~~unitP~:~$\forall\,x,~1*x = x$;
~~~~invP~:~$\forall\,x,~x^{-1} * x = 1$;
~~~~mulP~:~$\forall\,x_1~x_2~x_3,~ x_1 * (x_2 * x_3) = (x_1 * x_2) * x_3$
\}.

}

\smallskip

It declares a finite type called {\tt element} that is the group carrier or domain.
The rest of the structure specifies a left-unit element $1$, 
a left-inverse ${}^{-1}$ and an associative binary operation $( * )$. 
It is an exercise to check that the axioms imply that the left-unit is a two-sided
unit and that the left-inverse is a two-sided inverse.




\bigskip

Different systems can be commended in different ways: HOL Light for its small trustworthy kernel, Coq for its powerful type system, Mizar for its extensive libraries, and Isabelle/HOL for its useability.



\subsection{Flyspeck}
\section{Trust in Computers}

We all have first-hand experience of the bugs and glitches of
software.  We exchange stories when computers run amok.  Science
recently reported the story of a textbook ``The Making of a Fly'' that
was on sale at Amazon for more than 23 million dollars~\cite{Sci11}.  The
skyrocketing price was triggered by an automated bidding war between
two sellers, who let their algorithm run unsupervised.  The author,
Berkeley professor Peter Lawrence, said he hoped that the price would
reach ``a billion.''

%% cite Science, Vol 332, 6 May 2011. "The \$23 milllion textbook"

An overpriced textbook on the fly is harmless, except for students who
have it as a required text.  But what about the Flash Crash on Wall
Street that brought a 600 point plunge in the Dow Jones in just 5
minutes at 2:41 pm on May 6, 2010?  According to the New York Times \cite{NYT2010}, the flash
crash started when a mutual fund used a computer algorithm ``to sell
\$4.1 billion in futures contracts.''  The algorithm was designed to
sell ``without regard to price or time.\dots [A]s the computers of the
high-frequency traders traded [futures] contracts back and forth, a
`hot potato' effect was created.''  When computerized traders backed
away from the unstable markets, share prices of major companies
fluctuated even more wildly. ``Over 20,000 trades across more than 300
securities were executed at prices more than 60\% away from their
values just moments before'' \cite{SEC2010} Throughout the crash,
computers followed algorithms to a T, to the havoc
of the global economy. 

% http://en.wikipedia.org/wiki/2010_Flash_Crash
% 2:41-2:46pm time from the NYT graphic.
%T http://www.nytimes.com/2010/10/02/business/02flash.html
%T http://sec.gov/news/studies/2010/marketevents-report.pdf, ``Findings Regarding the Market Events of May 6, 2010'' (SEC etc.) REPORT OF THE STAFFS OF THE CFTC AND SEC TO THE JOINT ADVISORY COMMITTEE ON EMERGING REGULATORY ISSUES , Sept 10, 2010.
 

\subsection{In HOL Light we trust}

To what extent can we trust theorems certified by HOL Light?  There
are three various aspects to this question.  Is the underlying logic
of the system consistent?  Are there any programming errors in the
implementation of the system?  Can a devious user find ways to create
theorems in ways that circumvent the logic of the system?  Are the
underlying compilers, operating system, and hardware reliable?

Formal methods represent the best cumulative effort of
logicians, computer scientists and mathematicians over the decades and
even over the centuries to create a trustworthy foundation for the
practice of mathematics, and by extension, the practics of science and
engineering.  

\subsection{self-verification}

John Harrison, in a gem of a paper, repeats the classical question
{\it ``Quis custodiet ipsos custodes''} -- who guards the
guards~\cite{HaSelf}?  How do we prove the correctness of the prover
itself?  In that paper, he proves the consistency of the HOL Light
logic and the correctness of its implementation in computer code.  He
makes this verification in HOL Light itself!  To skirt G\"odel's
theorem, which implies that HOL Light -- if consistent -- cannot prove
its own consistency, he gives two versions of his proof.  The first
uses HOL Light to verify a weakened version of HOL Light that does not
have the axiom of infinity.  The second uses a HOL Light with a
strengthened axiom of infinity to verify standard HOL Light.

Recently, Mark Adams has developed his own implementation of HOL
(called HOL Zero).  His system has the ability to mechanically import
proofs that were developed in any proof assistant in the HOL
family~\cite{Adams}.  He imported Harrison's verification of HOL
Light, to obtain a HOL Zero verification of HOL Light.  You see where
this is going.  As mechanical translation capabilities are developed
for proof assistants, it becomes possible for different proof
assistants to share consistency proofs, similar to the way that
different axiomatic systems give relative consistency proofs of one
another.  We are headed in the direction of knowing that if the logic
or implementation of HOL Light has an error, then all other major
proof assistants must fall with it.

\subsection{hacking HOL}

Of course, every formal verifcation project is a verification of an
abstract model of the computer code, the computer language, and its
semantics.  In practice, there are gaps between the abstract
model and reality.

  
This leaves open the possibility that a hacker might find ways to
create an unauthorized theorem; that is a theorem generated by some
means other than the rules of inference of HOL Logic?  Indeed, there
are openings that a hacker can exploit.\footnote{To name two examples,
  (1) strings are mutable in Objective CAML, which allows theorems to
  maliciously altered.  (2) Objective CAML has {\it object magic},
  which is a way to defeat the type system.  These vulnerabilities
  would be detected during translation of the proof from HOL Light to
  HOL Zero.  I am not aware of any vulnerabilities that could pass
  undetected from one proof assistant to another.}  Adams maintains a
webpage of known vulnerabilities in his system and offers a cash
bounty to anyone who uncovers a new vulnerability.

These documented vulnerabilities need to be kept in perspective.  They
lie at the paranoid fringe of the most reliable software products ever
designed. Proof assistants are used verify the correctness of chips
and microcode, compilers, safety-critical software such as aircraft
guidance systems, security protocols, and mathematical theorems that
defeat the usual refereeing process.  

Penrose takes the view that nothing short of absolute certainty in
mathematics gives an adequate basis for science.  Poincar\'e was less
exacting, only requiring the imprecision of mathematical analysis to be
dwarfed by experimental error.  As Harrison puts it, 
``a foundational death spiral adds little value''~\cite{harrison-pm}.


% cite Harrison's Principia slides.

XX Trust via type system, closed off by modules, implemented in a small
kernel of 400 lines.


\subsection{Soft Errors}\label{sec:soft}

A soft error in a computer is a transcient error that cannot be
attributed to permanent hardware defects nor to bugs in software.  
Hard errors -- errors that can be attributed to a lasting hardware failure --
also occur, but at rates that are ten times smaller than
soft errors~\cite{MW04}.
%% cite: Ritesh Mastipuram and Edwin C Wee, Cypress Semiconductor -- EDN, September 30, 2004 (on hard drive).
Soft errors come from many sources. A
typical soft error is caused by cosmic rays, or rather by the shower
of energetic neutrons they produce through interactions in the earth's
atmosphere.  A nucleus of an atom in the hardware can capture one of
these energetic neutrons and throw off an alpha particle, which
strikes a memory circuit and changes the value stored in memory.  To
the end user, a soft error appears as a gremlin, a seemingly
inexplicable random error that disappears when the computer is rebooted and
the program runs again.

As an example, we will calculate the expected
number of soft errors in one of the mathematical calculations of
section XX.  The Atlas Project calculation of
the $E_8$ character table was a $77$ hour calculation that required
$64$ gigabytes RAM~\cite{AtlasSlides}.  Soft errors rates are generally measured in units
of failures-in-time (FIT). One FIT is defined as one error per $10^9$
hours of operation.
%The soft error rate of a memory device is typically in the range 1000
%to 5000 FIT per Mbit \cite{White Paper}.
% http://www.tezzaron.com/about/papers/soft_errors_1_1_secure.pdf
If we assume a soft error rate of $10^3$ FIT per Mbit, (which is a
typical rate for a modern memory device operating at sea
level\footnote{The soft error rate is remarkably sensitive to
  elevation; a calculation in Denver produces about three times more
  soft errors than the same calculation on identical hardware in Boston.}
\cite{WP}),
% 3 x in Mastipuram and Wee, EDN article.
 then we would expect there to be about $39$ soft
errors in memory during the calculation:
\[
\frac{10^3 \text{~FIT}}{1\text{~Mbit}} \cdot 64 \text{~GB} \cdot 77\text{~hours} =
\frac{10^3 \text{~errors~}}{10^9\text{~hours~}\text{Mbit}} \cdot
({64\cdot 8\cdot 10^3 \text{~Mbit}}) \cdot 77\text{~hours~} 
\approx 39.4 \text{~errors}.
\]
This example shows that soft errors can be a realistic concern in
mathematical calculations.

In software that has been thoroughly debugged, soft errors become the
most significant source of error in computation.  Although there are
numerous ways to cut soft errors with methods such as error-correcting
codes, hardware redesign carries an economic cost.  In fact, soft errors are on
the rise through miniaturization: a smaller circuit generally has a lower
capacitance and responds to less energetic alpha particles than a larger
circuit.

Soft errors are depressing news in the ultra-reliable world of proof
assistants.  Alpha particles rain on perfect and imperfect software alike.
In fact, because the number of soft errors is proportional to the length
of a calculation, (as JH pointed out to me) by being slow and methodical,
the probability of a soft error during a calculation inside
a proof assistant can be much higher than the probability when done outside.
For example, the calculation of a double precision 
floating point approximation to $\sqrt{2}$ with interval arithmetic
takes about XX flops (or floating-point operations).  
\begin{equation}
1.4 < \sqrt{2} < 1.42 XX.
\end{equation}
The formal proof of these inequalities in a proof assistant (HOL
Light) requires about $10^n$ XX times longer (and thus incurs $10^n$
times more soft errors) than the interval arithmetic calculation.
Soft errors especially become a issue in a formal proof project that
makes intensive floating-point calculations.\footnote{This includes my
  Flyspeck project, which will include the formalization of a
  floating-point calculation that takes $19$ hours to run outside the
  proof assistant.}

Soft errors and susceptibility to hacking
have come to be more than a nuisance to me.  They alter my
philosopical views of the foundations of mathematics.  I am a
computational formalist, that is, a formalist who admits physical
limits to the reliability of any verification process, whether by hand
or machine.  These limits taint even the simplest theorems, such as
our ability to verify that $1+1=2$ is a consequence of a set of
axioms.  One rogue alpha particle brings all my schemes of
perfection to nought.  %No matter how hard I try to reach formal perfection in my mathematical proofs, the stray alpha particle meddles.  
The rigour of
mathematics and the reliability of technology are mutually dependent;
math to provide ever more accurate models of science, and technology
to provide ever more reliable execution of mathematial proofs.


\subsection{Artificial mathematician}

Turing's great theoretical achievements were to delineate what a
computer can do in the concept of a universal Turing machine, to
establish limits to what a computer can do in his solution to the {\it
  Entscheidungsproblem}, and yet to advocate nonetheless that
computers might imitate all intelligent activity. It remains a
challenging research program (as well as a stunnng metynomy): to show
that one limited branch of mathematics, computation, might stand for
all mathematical activity.

In the century since Turing's birth, the computer has become so
ubiquitous and the idea of computer as brain so commonplace that it
bears repeating that we must still think very long and hard about how
to construct a computer that can imitate a living, thinking
mathematician.  As a first step toward imitation, we need a computer
that understands the language of mathematics.

%at a time when the
%meaning of the word computer was just starting to shift from a dreary
%human occupation to a mechanical device.

\bigskip

Ganesalingam's thesis \cite{Gan09}, \cite{Gan10} is the most
significant linguistic study of the language of mathematics to date.
Ganesalingam was awarded the 2011 Beth Prize for the best dissertation
in Logic, Language, or Information.  Although this research is still
at an early stage, it strongly suggests that the mechanical
translation that the mechanical translation of mathematical prose into
formal computer syntax that faithfully represents the semantics is a
realistic hope for the not-to-distant future.

% http://www.cl.cam.ac.uk/news/2011/07/mohan-ganesalingam-awarded-beth-prize/

% http://people.pwf.cam.ac.uk/mg262/GanesalingamMdis.pdf The Language of Mathematics.

The linguistic problems surrounding the language of mathematics differ
in various ways  from those of say standard English.  A
mathematical paper introduces new definitions and new notations as it
progesses, whereas in English the meanings of terms is generally
consider as fixed from the outset.  Mathematical writing freely mixes
English with symbolic expressions.  At the same time, mathematics is
self-contained in a way that English can never be; to understand
English is to understand the world.  By contrast, in principle, the
meanings of all notation and terms in a carefully written mathematical
paper are determined by Zermelo Fraenkel set theory (or whatever your
favorite foundational system might be).

His analysis of notational syntax is general enough to treat quite
general ``mixfix'' operations.  He analyzes subscripted infix
operators (such as a semidirect product $H\rtimes_\alpha N$),
multi-symboled operators (such as the three-symboled $[~:~]$ operator
for the degree $[K:k]$ of a field-extension), prefixed words
($R$-module), text within formulas $\{(a,b) \mid a \text{~is a factor
  of~} b\}$, unusual script placement ${}^LG$, chained relations
$a<b<c$, ellipses $1+2+\cdots+n$, contracted forms $x,y\in\ring{N}$,
and exposed formulas (such as ``for all $x>0$, \dots'' to mean ``for
all $x$,~if $x>0$, then \dots'').

The thesis treats what is called the formal mode of the language of
mathematics, that is the language divested all the side-remarks and
speculations that are frequently interspersed with the serious
mathematical content.  The syntax is treated as a context-free
grammar, and the semantics are analyzed with a variant of Discourse
Representation Theory.  In my limited understanding of Discourse
Representation Theory, gleaned from Ganesalingam's writings, it is
something very similar to first-order logic; but different in one
significant aspect: it provides a theory of pronoun references; or put
more formally, a theory of what may be the ``legitimate antecedent for
anaphor.''

A major issue in Ganesalingam's thesis is the resolution of ambiguity.
For example, in the statement
\begin{equation}\label{eqn:P}
P \text{\it ~is prime}
\end{equation}
does the term `prime' mean prime number, prime ideal, or prime
manifold?  His solution is to attach type information to terms (in the
sense of types as discussed above).  The reading of (\ref{eqn:P})
depends on the type of $x$, a number, a subset of a ring, or a
manifold.  In this analysis, resolution of ambiguity becomes a task of
a type inference engine.  

Because of the need for type information, Ganesalingam raises
questions about the suitability of Zermelo-Fraenkel set theory as the
ultimate semantics of mathematics.  A number of formal-proof
researchers have been arguing in favor of typed foundational systems
for many years.  There is in fact remarkable convergence between
Ganesalingam's linguistic analysis and say Gonthier's structural
analysis of abstract algebra in Coq. For example, in both camps we find
ellipses (aka  big operators), mixfix operators, type inference, missing
argument inference mechanisms, etc.


\subsection{Machine Learning}

\section{Further Reading}

To everyone who has any interest at all in proofs by computer, I
highly recommend MacKenzie's book \cite{Mac}.  It written by a
sociologist with a fine sensitivity to issues that matter to
mathematicians.  The author received the Robert K. Merton Award of the
American Sociological Association in 2003 for this book.

A fascinating account of the history of higher-order
logic appears in \cite{Gor}.  A few years ago, a special issue of
the Notices of the AMS presented a general introduction to formal
proofs~\cite{Hales:2008:formal},~\cite{Harrison:2008:formal},~\cite{gonthier:2008:formal},~\cite{Wiedijk:2008:formal}.  The book~\cite{wiedijk:17} takes
the reader through the proof of the irrationality of $\sqrt2$ in 
seventeen different proof assistants.  

I have relied on the descriptions of algorithms in
Harrison's book during the preparation of this article~\cite{Ha09}.


\bigskip
Blah blah blah


\raggedright
\bibliographystyle{amsalpha} % was plain %plainnat
\bibliography{/Users/thomashales/Desktop/googlecode/flyspeck/latex/bibliography/all}


\bigskip
\noindent
\svninfo



\end{document}

